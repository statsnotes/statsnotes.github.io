---
title: "Statistics for Social Scientists"
subtitle: "Lesson 2.4: Variance of the OLS Estimator"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Lesson 2.4: Variance of the OLS Estimator"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

::: callout-note
## Lesson Overview

In the last lesson, we discussed the properties of the OLS estimator. In this lesson, we discuss the variance and standard error of the OLS estimator in order to conduct hypothesis testing.

This lesson covers the following topics:

-   What homoscedasticity and heteroscedasticity are.
-   How standard errors are calculated under the assumption of homoscedasticity.
-   How the presence of heteroscedasticity affects our standard errors, and how we can adjust for this with robust standard errors.
:::

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 2.4.1: Gauss-Markov and Homoscedasticity

We have proved that OLS is unbiased under 4 conditions in the last lesson, and asymptotically consistent under one weakened condition.

However, if we recall from [2.1.3](https://statsnotes.github.io/metrics/1.html#variance-and-efficiency-of-estimators), unbiasedness is not the only thing we care about in an estimator. We also care about the estimator's variance.

Luckily, the Gauss-Markov Theorem does not stop at unbiasedness. With an additional condition (homoscedasticity), we can determine that OLS is not only unbiased, but the linear estimator with the least variance (and thus, the most efficient linear unbiased estimator).

::: callout-tip
## Definition: Gauss-Markov Theorem

The Gauss-Markov Theorem states that under 5 conditions, the OLS estimator is the Best Linear Unbiased Estimator (BLUE) for $\beta_1$, being the unbiased linear estimator with the best efficiency.

-   **SLR.1/MLR.1 (Linearity in Parameters)**: The parameters of the model are linear.
-   **SLR.2/MLR.2 (Random Sampling)**: All observations in our sample are randomly sampled from the population.
-   **SLR.3/MLR.3 (Sample Variation in** $x$**/ No Perfect Multicollinearity)**: $Var(x) â‰  0$, and no perfect correlation between explanatory variables.
-   **SLR.4/MLR.4 (Zero Conditional Mean)**. The error term $u$ has an expectation of 0, given any value of the explanatory variables.
-   **SLR.5/MLR.5 (Homoscedasticity)**: The error term has the same variance given any value of $x$.
:::

<br />

But what does this new condition of homoscedasticity mean?

Mathematically, homoscedasticity is defined as:

$$
Var(u|x_1, \dots, x_k) = \sigma^2 \text{ for all } x
$$

-   Of course, for simple linear regression, there is only one $x$, so the condition is $Var(u|x) = \sigma^2$.

First of all, what even is the variance $Var(u)$ representing? It is the variance of the "errors" of our error term $u_i$, which is also a random variable. The figure below displays this, with the green lines representing the distribution of $u_i$.

![](images/clipboard-3513031173.png){fig-align="center" width="90%"}

In the figure above, you can see the variance (spread) of the error term $u_i$'s distribution is consistent, no matter the value of $x$. This means that homoscedasticity is met.

When $Var(u)$ is not constant (and changes with the value of $x$), we have **heteroscedasticity**.

The best way to identify if this assumption is met is to look at a plot of the residuals (errors). If the variance in the residuals is constnant, we have homoscedasticity. If not, we have heteroscedasticity.

![](images/clipboard-1713529842.png){fig-align="center" width="100%"}

In the figure above, we can see on the left chart, the variance of the residuals is clearly smaller when $x$ is lower, and the variance of the residuals is larger when $x$ is higher. That is a clear violation of the homoscedasticity assumption.

-   NOTE: heteroscedasticity (failure to meet SLR.5 homoscedasticity) does not bias OLS estimates. It only determines if OLS has the lowest variance of unbiased linear estimators.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 2.4.2: Variance of OLS Estimates in Simple Linear Regression

Assuming Homoscedasticity is met, we know $Var(u|x) = \sigma^2$.

-   When homoscedasticity is not met, nothing in this section applies. See [2.4.6](https://statsnotes.github.io/metrics/4.html#robust-standard-errors-for-bivariate-regression) for more information on how to do estimates when this is violated.

What is variance? From [1.1.5](https://statsnotes.github.io/theory/1.html#variance-and-standard-deviation), we know that the formula for variance is:

$$
E(x - \mu)^2
$$

We know from SLR.4 Zero-Conditional Mean assumption that $E(u|x) = 0$. Thus, we can use that to calculate the variance $Var(u|x)$ using the variance formula:

$$
\begin{split}
Var(u|x) = \sigma^2 & = E[ \ ((u|x) - E(u|x))^2 \ ] \\
& = E((u|x) - 0)^2 \\
& = E[(u|x)^2] \\
& = E(u^2 |x)
\end{split}
$$

And since by homoscedasticity, we know variance does not depend on $x$, so $Var(u|x) = Var(u)$. Thus, we also know that:

$$
E(u^2|x) = E(u^2) = \sigma^2
$$

<br />

Remember when we were proving unbiasedness of OLS in [2.3.3](https://statsnotes.github.io/metrics/3.html#proof-of-ols-unbiasedness-in-simple-linear-regression), we got to this stage:

$$
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^n w_i u_i
$$

-   Where $w_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}$, which is a function of random variable $x$.
-   We could also write $w_i$ as $\frac{x_i - \bar{x}}{SST_x}$ (where $SST_x$ is total sum of squares for $x$).

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, $\sum w_i u_i$ is the variance in $\hat\beta_1$.

$$
\begin{split}
Var(\hat\beta_1|x) & = Var\left( \sum\limits_{i=1}^n w_i u_i \bigg| x\right) \\
& = \sum\limits_{i=1}^n Var(w_i u_i | x) \\
& = \sum\limits_{i=1}^n w_i^2 Var(u_i | x)
\end{split}
$$

And given SLR.2 Random Sampling (see [2.3.2](https://statsnotes.github.io/metrics/3.html#unbiasedness-of-ols-under-the-gauss-markov-theorem)), we know $Var(u_i | x)$ is also equal to $Var(u_i|x_i)$. Thus:

$$
Var(\hat\beta_1|x) = \sum\limits_{i=1}^n w_i^2 Var(u_i | x_i)
$$

And using SLR.5 homoscedasticity, we know $Var(u|x) = \sigma^2$ and is constant, thus:

$$
\begin{split}
Var(\hat\beta_1 | x) & = \sum\limits_{i=1}^n w_i^2 Var(u_i | x_i) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma^2 \\
& = \sigma^2 \sum\limits_{i=1}^n w_i^2
\end{split}
$$

Remember, $w_i$ is its own function of $x$, where $w_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}$, or can be written as $\frac{x_i - \bar{x}}{SST_x}$ (see [2.3.3](https://statsnotes.github.io/metrics/3.html#proof-of-ols-unbiasedness-in-simple-linear-regression) for derivation).

We have $\sum w_i^2$ in our final equation, and we can do some quick algebra to rearrange it (note, if you are not familiar with SST, see [2.2.7](https://statsnotes.github.io/metrics/2.html#r-squared-and-goodness-of-fit)):

$$
\begin{split}
\sum\limits_{i=1}^n w_i^2 & = \sum\limits_{i=1}^n \frac{(x_i - \bar{x})^2}{(SST_x)^2} \\
& = \frac{\sum_{i=1}^n(x_i - \bar x)^2}{(SST_x)^2} \\
& = \frac{SST_x}{(SST_x)^2} \\
& = \frac{1}{SST_x}
\end{split}
$$Thus, we can plug that in to get our final variance of $\hat\beta_1$ formula:

$$
\begin{split}
Var(\hat\beta|x) & = \sigma^2 \sum\limits_{i=1}^n w_i^2 \\
& = \sigma^2 \frac{1}{SST_x} \\
& = \frac{\sigma^2}{SST_x}
\end{split}
$$

Thus, that is the variance of our OLS estimator $\hat\beta_1$, and also the variance of the sampling distribution of $\hat\beta_1$.

-   This is only the case if SLR.5 homoscedasticity assumption holds. It is not valid if the assumption is not met. See [2.4.6](https://statsnotes.github.io/metrics/4.html#robust-standard-errors-for-bivariate-regression) for more information on how to do estimates when this is violated.
-   One might notice we calculated $Var(\hat\beta_1|x)$, not $Var(\hat\beta_1)$. However, this does not matter, since our final formula does not depend on the value of $x$ (from homoscedasticity, we know $\sigma^2$ is independent of $x$, and also $SST_x$ is constant no matter the specific value of $x$ as it includes all values of $x$).

There is one issue: we know that $\sigma^2 = E(u^2)$ (see earlier in the section). However, we do not actually know the value of $E(u^2)$! Remember, that is the error term $u$ - while we only know the residual term $\hat u$.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 2.4.3: Standard Errors in Simple Linear Regression

We know, as shown in the last section, that the variance of the sampling distribution is:

$$
Var(\hat\beta) = \frac{\sigma^2}{SST_x}
$$

If we want the standard deviation of the sampling distribution, we simply take the square root:

$$
sd(\hat\beta_1) = \frac{\sigma}{\sqrt{SST_x}}
$$

There is one issue: we know that $\sigma^2 = E(u^2)$ (see previous section for proof). However, we do not actually know the value of $E(u^2)$! Remember, that is the error term $u$ - while we only know the residual term $\hat u$.

So, what we can do is well, simply replace $u$ with its estimate, $\hat u$.

-   Recall that $u_i = y_i - \beta_0 - \beta_1 x_i$
-   And $\hat u_i = y_i - \hat\beta_0 - \hat\beta_1 x_i$.

So naturally, instead of $\sigma^2 = E(u^2)$, we could estimate it with $E(\hat u^2)$. Mathematically:

$$
\begin{split}
\hat\sigma^2 = E(\hat u^2) & = \frac{1}{n} \sum\limits_{i=1}^n \hat u_i^2 \\
& = SSR/n
\end{split}
$$

-   Where $SSR$ is the square sum of residuals (see [2.2.7](https://statsnotes.github.io/metrics/2.html#r-squared-and-goodness-of-fit) for more details).

However, there is an issue with this estimate of $\sigma^2$. It is biased - the expected value is actually slightly less than $\sigma^2$.

We will not prove this mathematically, but this bias is because OLS imposes two conditions on its estimation process (that we discussed in [2.3.1](https://statsnotes.github.io/metrics/3.html#residuals-and-properties-of-ols-estimator)):

$$
\begin{split}
& \sum\limits_{i=1}^n \hat u_i = 0 \\
& \sum\limits_{i=1}^n x_i \hat u_i = 0
\end{split}
$$

-   The actual error term $u_i$ (not the OLS residuals $\hat u_i$) do not have these restrictions.

We can adjust the estimator to be more accurate by including a degrees of freedom adjustment. So, instead of $SSR/n$, we can do $SSR/(n-2)$. Thus, our estimator for $\sigma^2$ is:

$$
\hat\sigma^2 = \frac{SSR}{n-2} = \frac{\sum_{i=1}^n \hat u_i^2}{n-2}
$$

With that estimate of $\sigma^2$, we can plug it back into our formula for the standard deviation of $\hat\beta_1$.

-   We call this standard deviation the standard error (as discussed in [1.2.3](https://statsnotes.github.io/theory/2.html#uncertainty-in-estimates-and-sampling-distributions))

$$
\widehat{se}(\hat\beta_1) = \frac{\hat\sigma}{\sqrt{SST_x}}
$$

::: callout-tip
## Definition: Standard Errors for Simple Linear Regression

The standard error for the coefficient $\hat\beta_1$ from an OLS estimator in a simple linear regression is:

$$
\widehat{se}(\hat\beta_1) = \frac{\hat\sigma}{\sqrt{SST_x}}
$$

Where $\hat\sigma$ is defined as:

$$
\hat\sigma = \sqrt{\frac{SSR}{n-2}} = \sqrt{\frac{\sum_{i=1}^n \hat u_i^2}{n-2}}
$$

And where $SST_x$ is defined as:

$$
SST_x = \sum_{i=1}^n(x_i - \bar x)^2
$$
:::

Using these standard errors, we can conduct confidence intervals and hypothesis testing as shown in [1.4.4](https://statsnotes.github.io/intro/4.html#standard-errors-and-confidence-intervals) and [1.4.5](https://statsnotes.github.io/intro/4.html#hypothesis-testing).

When homoscedasticity is not met, this standard error is invalid. See [2.4.6](https://statsnotes.github.io/metrics/4.html#robust-standard-errors-for-bivariate-regression) for more information on how to do estimates when this is violated.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 2.4.4: Variance in Multiple Linear Regression

Now, we will do the same exercise, but for multiple linear regression.

Assuming homoscedasticity is met, we know $Var(u|x_1, \dots, x_k) = \sigma^2$.

-   When homoscedasticity is not met, nothing in this section applies. See [2.4.7](https://statsnotes.github.io/metrics/4.html#robust-standard-errors-for-multiple-regression) for more information on how to do estimates when this is violated.

Let us find the variance of OLS estimates (we will use $\hat\beta_1$ for simplicity, but this applies to any other coefficient $\hat\beta_1 , \dots, \hat\beta_k$. In proving unbiasedness of OLS in [2.3.4](https://statsnotes.github.io/metrics/3.html#proof-of-ols-unbiasedness-in-multiple-linear-regression), we got to this stage:

$$
\hat\beta_1 = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
$$

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, the second part is the variance in $\hat\beta_1$.

Let us define $w_i$ as following, as a function of $x_1, \dots, x_k$:

$$
w_i = \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
$$

This allows us to write $\hat\beta_1$ as:

$$
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^nw_i u_i
$$

<br />

Thus, we can proceed in the same way as the simple linear regression case:

$$
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) & = Var\left( \sum\limits_{i=1}^nw_i u_i \biggr|x_1, \dots ,x_k \right) \\ & = \sum\limits_{i=1}^n Var(w_i u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \ Var(u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma^2 \\
& = \sigma^2 \sum\limits_{i=1}^n w_i^2
\end{split}
$$

Now, plugging back in $w_i$, we get:

$$
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) & = \sigma^2 \sum\limits_{i=1}^n w_i^2 \\
& = \sigma^2 \sum_{i=1}^n \left( \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \right) \\
& = \sigma^2 \frac{\sum_{i=1}^n \widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \\
& = \frac{\sigma^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
$$

And assuming homoscedasticity (where $Var(\hat\beta_1)$ does not depend on $x_1, \dots, x_k$, we thus know:

$$
Var(\hat\beta_1) = \frac{\sigma^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
$$

-   When homoscedasticity is not met, this is not valid. See [2.4.7](https://statsnotes.github.io/metrics/4.html#robust-standard-errors-for-multiple-regression) for more information on how to do estimates when this is violated.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 2.4.5: Standard Errors and Multicollinearity in Multiple Regression

However, just like we mentioned in [2.4.3](https://statsnotes.github.io/metrics/4.html#standard-errors-in-simple-linear-regression), the real value of $\sigma^2$ of a regression not calculable. This is an issue because our variance formula has $\sigma^2$ in the numerator.

Just like in simple linear regression (see [2.4.3](https://statsnotes.github.io/metrics/4.html#standard-errors-in-simple-linear-regression)), we can estimate $\sigma^2$ by using the residual term $\hat u_i$ with a degrees of freedom adjustment.

$$
\hat\sigma^2 = \frac{SSR}{n-k-1} = \frac{\sum_{i=1}^n \hat u_i^2}{n-k-1}
$$

-   Where $n$ is the number of observations in our sample data.
-   Where $k$ is the number of explanatory variables in our model.

Thus, with this estimate, we can calculate the **standard errors** (square root of variance) of our estimates of coefficients $\hat\beta_j$.

::: callout-tip
## Definition: Standard Errors for Multiple Linear Regression

The standard error for the coefficient $\hat\beta_j$ from an OLS estimator in multiple linear regression is:

$$
\widehat{se}(\hat\beta_j) = \frac{\hat\sigma}{\sqrt{\sum_{i=1}^n \widetilde{r_{1i}}^2}}
$$

-   We will never calculate this by hand, we will use a statistical software to do this.

Where $\hat\sigma$ is defined as:

$$
\hat\sigma = \sqrt{\frac{SSR}{n-k-1}} = \sqrt{\frac{\sum_{i=1}^n \hat u_i^2}{n-k-1}}
$$
:::

Using these standard errors, we can conduct confidence intervals and hypothesis testing as shown in [1.5.5](https://statsnotes.github.io/intro/5.html#inference-and-hypothesis-testing).

When homoscedasticity is not met, this is not valid. See [2.4.7](https://statsnotes.github.io/metrics/4.html#robust-standard-errors-for-multiple-regression) for more information on how to do estimates when this is violated.

<br />

### Multicollinearity

Notice how the denominator contains $\sum\widetilde{r_{1i}}^2$. That means, the smaller $\sum\widetilde{r_{1i}}^2$ is, the larger the variance is.

-   $\sum\widetilde{r_{1i}}^2$ is smaller when our explanatory variable of interest $x_1$ is highly correlated with another explanatory variable, since $\widetilde{r_{1i}}^2$ represents the part of $x_1$ that is uncorrelated with other explanatory variables.
-   This means that if we have highly correlated explanatory variables, our variance will in our OLS estimates will be high.

This is something important to think about when choosing explanatory variables. We want to include all confounding variables, but highly correlated confounders will make our variance very high for our estimates.

-   One way of dealing with this is dimensional reduction techniques, which we will discuss in Part IV of the guide dealing with multivariate analysis.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 2.4.6: Robust Standard Errors for Bivariate Regression

MLR.5 Homoscedasticity states that $Var(u|x_1, \dots x_k) = \sigma^2$. Or in other words, no matter the value of $x_1, \dots, x_k$, the variance of the error term is constant at $\sigma^2$.

When MLR.5 Homoscedasticity is violated, the OLS estimator is still unbiased, however, it is no longer the unbiased linear estimator with the least variance.

More importantly for us however, is that heteroscedasticity invalidates the variance and standard error formulas we have calculated earlier.

<br />

Let us calculate these standard errors under heteroscedasticity. For simplicity, consider the bivariate regression:

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

However, without MLR.5 Homoscedasticity, variance $Var(u|x)$ is no longer constant at $\sigma^2$. Instead, $Var(u|x)$ varies depending on the value of $x$. Let us define the variance as:

$$
Var(u_i|x_i) = \sigma_i^2
$$

-   Where $\sigma^2_1$ is the error term variance for $x_1$, and $\sigma_2^2$ is the error term variance for $x_2$, and so on $\sigma_3^2, \dots, \sigma_n^2$.

Then, recalling from the start of [2.4.2](https://statsnotes.github.io/metrics/4.html#variance-of-ols-estimates-in-simple-linear-regression), we have the following formula for the simple linear regression $\hat\beta_1$ estimate:

$$
\hat\beta_1 = \beta_1 + \sum_{i=1}^nw_i u_i
$$

-   Where $w_i = \frac{x_i - \bar x}{\sum(x_i - \bar x)^2}$, which is a function of random variable $x$.
-   We could also write $w_i$ as $\frac{x_i - \bar x}{SST_x}$.

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, the second part is the variance in $\hat\beta_1$. Thus:

$$
\begin{split}
Var(\hat\beta_1|x) & = Var \left( \sum\limits_{i=1}^n w_i u_i \biggr| x \right) \\
& = \sum\limits_{i=1}^n Var(w_i u_i |x) \\
& = \sum\limits_{i=1}^n w_i^2 \ Var (u_i | x) \\
& = \sum\limits_{i=1}^nw_i^2 \ Var(u_i|x_i) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma_i^2
\end{split}
$$

Now, plugging back $w_i$ in, we get:

$$
\begin{split}
Var(\hat\beta_1|x) & = \sum\limits_{i=1}^n w_i^2 \sigma_i^2 \\
& = \sum\limits_{i=1}^n \left(\frac{x_i - \bar x}{SST_x} \right)^2 \sigma^2_i \\
& = \frac{\sum_{i=1}^n (x_i - \bar x)^2 \sigma^2_i}{SST_x^2}
\end{split}
$$

Of course, just like with homoscedasticity, we do not know $\sigma_i^2$, and have to estimate it with $\hat u$. We do not need a degrees of freedom adjustment in this case. Thus, our estimate of variance is:

$$
\widehat{Var}(\hat\beta_1|x) = \frac{\sum_{i=1}^n (x_i - \bar x)^2 \hat u^2_i}{SST_x^2}
$$

And thus, the standard error (square root of variance) of our estimate $\hat\beta_1$ under homoscedasticity is the square root of our estimate of variance.

::: callout-tip
## Definition: Robust Standard Error for Bivariate Regression

The heteroscedasticity-robust standard error for $\hat\beta_1$ in bivariate regression is:

$$
\widehat{se}(\hat\beta_1|x) = \sqrt{\frac{\sum_{i=1}^n (x_i - \bar x)^2 \hat u^2_i}{SST_x^2}}
$$

-   Where SST is the total sum of squares $SST = \sum(y_i - \bar y)^2$.
:::

Using these standard errors, we can conduct confidence intervals and hypothesis testing as shown in [1.4.4](https://statsnotes.github.io/intro/4.html#standard-errors-and-confidence-intervals) and [1.4.5](https://statsnotes.github.io/intro/4.html#hypothesis-testing).

Nowadays, it is typical to use heteroscedasticity-robust standard errors as default, and only using normal standard errors if we can prove homoscedasticity is met.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 2.4.7: Robust Standard Errors for Multiple Regression

Let us do the same as the last section, but for multiple regression.

Without MLR.5 Homoscedasticity, variance $Var(u|x)$ is no longer constant at $\sigma^2$. Instead, $Var(u|x)$ varies depending on the value of $x$. Let us define the variance as:

$$
Var(u_i|x_{1i}, \dots, x_{ki}) = \sigma_i^2
$$

Let us find the variance of OLS estimates (we will use $\hat\beta_1$ for simplicity, but this applies to any other coefficient $\hat\beta_1 , \dots, \hat\beta_k$. In proving unbiasedness of OLS in [2.3.4](https://statsnotes.github.io/metrics/3.html#proof-of-ols-unbiasedness-in-multiple-linear-regression), we got to this stage:

$$
\hat\beta_1 = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
$$

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, the second part is the variance in $\hat\beta_1$.

Let us define $w_i$ as following, as a function of $x_1, \dots, x_k$:

$$
w_i = \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
$$

This allows us to write $\hat\beta_1$ as:

$$
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^nw_i u_i
$$

<br />

Thus, we can proceed in the same way as the simple linear regression case in the last section.

$$
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) & = Var\left( \sum\limits_{i=1}^nw_i u_i \biggr|x_1, \dots ,x_k \right) \\ & = \sum\limits_{i=1}^n Var(w_i u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \ Var(u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma^2_i
\end{split}
$$

Now, plugging back in $w_i$, we get:

$$
\begin{split}
Var(\hat\beta_1|x) & = \sum\limits_{i=1}^n w_i^2 \sigma_i^2 \\
& = \sum\limits_{i=1}^n \left(\frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \right)^2 \sigma^2_i \\
& = \frac{\sum_{i=1}^n \widetilde{r_{1i}}^2 \sigma^2_i}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
\end{split}
$$

Of course, just like before, we do not know $\sigma_i^2$, and have to estimate it with $\hat u$. Thus, our estimate of variance is:

$$
\widehat{Var}(\hat\beta_1) = \frac{\sum_{i=1}^n \widetilde{r_{1i}}^2 \hat u_i^2}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
$$

And thus, the standard error (square root of variance) of our estimate $\hat\beta_1$ under homoscedasticity is the square root of our estimate of variance.

-   This can be generalised to any other coefficient $\hat\beta_1, \dots, \hat\beta_k$.

::: callout-tip
## Definition: Robust Standard Error for Multiple Regression

The heteroscedasticity-robust standard error for $\hat\beta_1$ in multiple regression is:

$$
\widehat{se}(\hat\beta_j) = \sqrt{\frac{\sum_{i=1}^n \widetilde{r_{ji}}^2 \hat u^2_i}{\sum_{i=1}^n\widetilde{r_{ji}}^2}}
$$
:::

Using these standard errors, we can conduct confidence intervals and hypothesis testing as shown in [1.5.5](https://statsnotes.github.io/intro/5.html#inference-and-hypothesis-testing).

Nowadays, it is typical to use heteroscedasticity-robust standard errors as default, and only using normal standard errors if we can prove homoscedasticity is met.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)
