<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistics for Social Scientists</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="2_files/libs/clipboard/clipboard.min.js"></script>
<script src="2_files/libs/quarto-html/quarto.js"></script>
<script src="2_files/libs/quarto-html/popper.min.js"></script>
<script src="2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="2_files/libs/quarto-html/anchor.min.js"></script>
<link href="2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="2_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Statistics for Social Scientists</h1>
            <p class="subtitle lead">Lesson 2.2: The Ordinary Least Squares Estimator</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Lesson 2.2: The Ordinary Least Squares Estimator</h2>
   
  <ul class="collapse">
  <li><a href="#sum-of-squared-errors" id="toc-sum-of-squared-errors" class="nav-link active" data-scroll-target="#sum-of-squared-errors">2.1.1: Sum of Squared Errors</a></li>
  <li><a href="#estimation-for-simple-linear-regression" id="toc-estimation-for-simple-linear-regression" class="nav-link" data-scroll-target="#estimation-for-simple-linear-regression">2.2.2: Estimation for Simple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#useful-properties-of-summation" id="toc-useful-properties-of-summation" class="nav-link" data-scroll-target="#useful-properties-of-summation">Useful Properties of Summation</a></li>
  </ul></li>
  <li><a href="#multiple-linear-regression-with-linear-algebra" id="toc-multiple-linear-regression-with-linear-algebra" class="nav-link" data-scroll-target="#multiple-linear-regression-with-linear-algebra">2.2.3: Multiple Linear Regression with Linear Algebra</a></li>
  <li><a href="#mathematics-of-the-ordinary-least-squares-estimator" id="toc-mathematics-of-the-ordinary-least-squares-estimator" class="nav-link" data-scroll-target="#mathematics-of-the-ordinary-least-squares-estimator">2.2.4: Mathematics of the Ordinary Least Squares Estimator</a></li>
  <li><a href="#regression-anatomy-and-controlling-for-confounders" id="toc-regression-anatomy-and-controlling-for-confounders" class="nav-link" data-scroll-target="#regression-anatomy-and-controlling-for-confounders">2.5.5: Regression Anatomy and Controlling for Confounders</a></li>
  <li><a href="#regression-anatomy-formula-for-ols-estimation" id="toc-regression-anatomy-formula-for-ols-estimation" class="nav-link" data-scroll-target="#regression-anatomy-formula-for-ols-estimation">2.2.6: Regression Anatomy Formula for OLS Estimation</a></li>
  <li><a href="#r-squared-and-goodness-of-fit" id="toc-r-squared-and-goodness-of-fit" class="nav-link" data-scroll-target="#r-squared-and-goodness-of-fit">2.2.7: R-Squared and Goodness of Fit</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the last part of the course, lessons <a href="https://statsnotes.github.io/intro/4.html">1.4</a> and <a href="https://statsnotes.github.io/intro/5.html">1.5</a>, we discussed the linear regression model. However, we did not go into details on how the coefficients of the model are estimated. This lesson introduces the most popular method of estimation for the linear-regression model: the ordinary least squares estimator.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>This lesson requires a strong understanding of the simple and multiple linear regression models introduced in lessons <a href="https://statsnotes.github.io/intro/4.html">1.4</a> and <a href="https://statsnotes.github.io/intro/5.html">1.5</a>. Please review these sections before beginning.</p>
</div>
</div>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
<section id="sum-of-squared-errors" class="level1">
<h1>2.1.1: Sum of Squared Errors</h1>
<p>We have a simple and multiple linear regression model in the following forms.</p>
<p><span class="math display">
\begin{split}
y_i &amp; = \beta_0 + \beta_1 x_i + u_i \\
y_i &amp; = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i
\end{split}
</span></p>
<p>How do we estimate the parameters <span class="math inline">\overrightarrow\beta</span> to create our best-fit lines/planes? One way we can do this is to find the best-fit line that minimises the sum of squared errors (SSE). This method is called the Ordinary Least Squares Estimator.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Sum of Squared Errors
</div>
</div>
<div class="callout-body-container callout-body">
<p>The sum of squared errors (SSE) is for simple linear regression is as follows:</p>
<p><span class="math display">
\begin{split}
SSE &amp; = \sum\limits_{i=1}^n (y_i - \hat y_i)^2 \\
&amp; = \sum\limits_{i=1}^n (y_i - (\hat \beta_0+ \hat\beta_1 x_i)) \\
&amp; = \sum\limits_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1 x_i)^2
\end{split}
</span></p>
<ul>
<li>The sum of squared errors is exactly as it sounds. Find the error, the distance between the actual <span class="math inline">y_i</span> and predicted <span class="math inline">\hat y</span>, which is <span class="math inline">y_i - \hat y</span>, then square that error <span class="math inline">(y_i - \hat y_i)^2</span>, then sum up for all observations <span class="math inline">i</span> in the data.</li>
<li>We get the second equation by substituting in the fitted values model, where <span class="math inline">\hat{y} = \hat\beta_0 + \hat\beta_1x_i</span>.</li>
</ul>
<p>For multiple regression, the intuition is the same:</p>
<p><span class="math display">
\begin{split}
SSE &amp; = \sum\limits_{i=1}^n (y_i - \hat y_i)^2 \\
&amp; = \sum\limits_{i=1}^n (y_i - (\hat \beta_0+ \hat\beta_1 x_{1i} + \dots + \hat\beta_kx_{ki})) \\
&amp; = \sum\limits_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1 x_{1i} - \dots - \hat\beta_kx_{ki})^2
\end{split}
</span></p>
</div>
</div>
<p>More inuitively, the errors of a best-fit line are highlighted in red. We will square each error, then sum all the errors up, to get the sum of squared errors for that best-fit line:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-846785636.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%"></p>
</figure>
</div>
<p>Why do we want to square the errors?</p>
<ul>
<li>This is because we do not care about the direction of errors - only the size of the errors.</li>
<li>For example, in the figure above, the error <em>d1</em> is positive, while <em>d2</em> is negative. If we sum them together, those almost cancel out, giving us an error of near zero. However, we do not want them to be cancelled out - we care about the sizes of the errors.</li>
<li>Thus, by squaring the errors, we make all errors positive, thus only focusing on the size of the errors, not their positive/negative direction.</li>
</ul>
<p>A common question is why we square the errors, and don’t use absolute values of the errors. There are a few reasons this is the case.</p>
<ol type="1">
<li>As we will see in the next section, minimising functions relies on finding the derivative of the function. An absolute value function is not differentiable at its vertex, making it difficult to minimise (as we are trying to minimise the errors).</li>
<li>The least-squares method has several desirable properties for inference that we will cover mostly in <a href="https://statsnotes.github.io/metrics/3.html">lesson 2.3</a>.</li>
</ol>
<p>There is a estimator that uses the absolute values of the errors: the Least Absolute Deviations Estimator. However, it does not have all the nice property of the OLS estimator, so we will not worry about it.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="estimation-for-simple-linear-regression" class="level1">
<h1>2.2.2: Estimation for Simple Linear Regression</h1>
<p>The Ordinary Least Squares (OLS) Estimator estimates the coefficients <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> by finding the values of <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span> that result in the line with the smallest sum of squared errors (as discussed in the last section).</p>
<p>We can describe the goal of OLS in a more mathematical way:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Ordinary Least Squares (OLS) Estimator
</div>
</div>
<div class="callout-body-container callout-body">
<p>The goal of the Ordinary Least Squares (OLS) Estimator is to find the values of <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> that make the following statement true:</p>
<p><span class="math display">
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1) &amp; = \min\limits_{\hat{\beta}_0, \hat{\beta}_1} \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\
&amp; =\min\limits_{\hat{\beta_0}, \hat{\beta}_1} S(\hat{\beta}_0, \hat{\beta}_1)
\end{split}
</span></p>
<p>Where function <span class="math inline">S</span> is the sum of squared errors.</p>
</div>
</div>
<p>How do we minimise <span class="math inline">S</span> (the function of the sum of squared errors)? From calculus, we know that a minimum/maximum of a function is where the derivative of the function is equal to 0.</p>
<p>Thus, let us find the partial derivative of the function <span class="math inline">S</span> in respect to both <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>, and set them equal to 0. These will be called the <strong>first-order conditions</strong>.</p>
<p><br></p>
<section id="first-order-conditions" class="level3">
<h3 class="anchored" data-anchor-id="first-order-conditions">First Order Conditions</h3>
<p>First, let us find the partial derivative of <span class="math inline">S</span> in respect to <span class="math inline">\hat\beta_0</span>:</p>
<p><span class="math display">
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} = \frac{\partial }{\partial \hat{\beta}_0} \left[ \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \right]
</span></p>
<p>First, ignore the summation. The partial derivative of the internal section, using chain rule, is the following:</p>
<p><span class="math display">
\frac{\partial}{\partial \hat{\beta}_0} \left[ (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \right] = -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
</span></p>
<p>But how do we deal with the summation? We know that there is the sum rule of derivatives <span class="math inline">[f(x) + g(x)]' = f'(x) + g'(x)</span>. Thus, we know we just sum up the individual derivatives to get the derivative of the sum:</p>
<p><span class="math display">
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} &amp; = \sum\limits_{-i=1}^n \left[ -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
&amp; = -2 \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
</span></p>
<p>To find the value of <span class="math inline">\hat\beta_0</span> that minimises <span class="math inline">S</span>, we set the derivative equal to 0. We can ignore the -2, since if the summation is equal to 0, the whole derivative will equal 0. Thus, the first order condition is:</p>
<p><span class="math display">
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
</span></p>
<p><br></p>
<p>Now, let us do the same for <span class="math inline">\hat\beta_1</span>. Using the same steps as before</p>
<p><span class="math display">
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_1} &amp; = \sum\limits_{i=1}^n \left[ -2x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
&amp; = -2 \sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
</span></p>
<p>The first order condition for <span class="math inline">\hat\beta_1</span> will be (again, ignoring the -2 for the same reason as before):</p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: First Order Conditions of OLS
</div>
</div>
<div class="callout-body-container callout-body">
<p>Thus, the first order conditions of OLS are:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
</span></p>
</div>
</div>
<p><br></p>
</section>
<section id="solving-the-system-of-equations" class="level3">
<h3 class="anchored" data-anchor-id="solving-the-system-of-equations">Solving the System of Equations</h3>
<p>We now have our two first-order conditions. Now, we have a 2-equation system of equations, with 2 variables.</p>
<ul>
<li>We can solve this through substitution - in the first equation, solve for <span class="math inline">\hat\beta_0</span> in terms of <span class="math inline">\hat\beta_1</span>.</li>
<li>Then, plug in <span class="math inline">\hat\beta_0</span> in terms of <span class="math inline">\hat\beta_1</span> into the second equation, thus making that a one-variable equation. We can solve that equation for <span class="math inline">\hat\beta_1</span>, then find <span class="math inline">\hat\beta_0</span>.</li>
</ul>
<p><br></p>
<p>First, let us solve the first equation for <span class="math inline">\hat\beta_0</span> in terms of <span class="math inline">\hat\beta_1</span>:</p>
<p><span class="math display">
\begin{split}
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) &amp; =  0 \\
\sum\limits_{i=1}^n y_i - n \hat{\beta}_0 - \hat{\beta}_1 \sum\limits_{i=1}^n x_i &amp; = 0 \\
-n\hat{\beta}_0 &amp;= -\sum\limits_{i=1}^n y_i + \hat{\beta}_1\sum\limits_{i=1}^nx_i \\
\hat\beta_0 &amp; = \frac{1}{-n} \left( -\sum\limits_{i=1}^n y_i + \hat{\beta}_1\sum\limits_{i=1}^n x_i \right) \\
\hat{\beta}_0 &amp; = \frac{1}{n} \sum\limits_{i=1}^n y_i - \frac{1}{n}\hat{\beta}_1 \sum\limits_{i=1}^n x_i \\
\hat\beta_0&amp; = \bar{y} - \hat{\beta}_1 \bar{x}
\end{split}
</span></p>
<p>Now, let us substitute our calculated <span class="math inline">\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}</span> into the <span class="math inline">\hat{\beta}_1</span> condition and solve for <span class="math inline">\hat{\beta}_1</span>:</p>
<p><span class="math display">
\begin{split}
0 &amp; =\sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)  \\
&amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - [\bar{y} - \hat{\beta}_1\bar{x}] - \hat{\beta}_1x_i) \right] \\
&amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y} - \hat{\beta}_1 (x_i - \bar{x})) \right] \\
&amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y}) - x_i \hat{\beta}_1(x_i - \bar{x}) \right] \\
&amp; = \sum\limits_{i=1}^n x_i (y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^nx_i (x_i - \bar{x})
\end{split}
</span></p>
</section>
<section id="useful-properties-of-summation" class="level2">
<h2 class="anchored" data-anchor-id="useful-properties-of-summation">Useful Properties of Summation</h2>
<p>Before we finish, here are a few key properties of summation</p>
<p><strong>Property 1:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
</span></p>
<ul>
<li>This is because we can expand the left to <span class="math inline">\sum x_i - \sum \bar x</span>.</li>
<li>Then, we know <span class="math inline">\sum x_i = \sum \bar x</span> (by the formula for mean), so <span class="math inline">\sum x_i - \sum \bar x = 0</span>.</li>
</ul>
<p><strong>Property 2:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y})
</span></p>
<ul>
<li>This is because on the right side can expand to <span class="math inline">\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]</span>.</li>
<li>Then, split into <span class="math inline">\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)</span>.</li>
<li>We know that by property 1 (which applies to any variable), <span class="math inline">\sum (y_i - \bar y) = 0</span>. Thus, the right side disappears, and we are left with <span class="math inline">\sum x_i (y_i - \bar y)</span>.</li>
</ul>
<p><strong>Property 3:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
</span></p>
<ul>
<li>Start by expanding right side to <span class="math inline">\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]</span></li>
<li>Which splits into <span class="math inline">\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)</span></li>
<li>By the first property, we know <span class="math inline">\sum x_i - \bar x = 0</span>, so we are only left with <span class="math inline">\sum x_i (x_i - \bar x)</span></li>
</ul>
<p>Knowing these properties of summation, we can transform what we had before:</p>
<p><span class="math display">
\begin{split}
0 &amp; = \sum\limits_{i=1}^n x_i (y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^nx_i (x_i - \bar{x}) \\
0 &amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 \\
\hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 &amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) \\
\hat{\beta}_1 &amp; = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
\end{split}
</span></p>
<p>Note that the numerator is equivalent to the formula of covariance <span class="math inline">Cov(x,y)</span>, and the denominator is equal to the variance <span class="math inline">Var(x)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: OLS Estimate of Coefficient
</div>
</div>
<div class="callout-body-container callout-body">
<p>Thus, the OLS estimate <span class="math inline">\hat\beta_1</span> (slope) of the linear regression model is:</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2} = \frac{Cov(x, y)}{Var(x)}
</span></p>
<p>This is the expected change in <span class="math inline">y</span> given a one unit increase in <span class="math inline">x</span>.</p>
<ul>
<li>Remember, this is the <strong>relationship</strong> between <span class="math inline">x</span> and <span class="math inline">y</span>, <u><strong>not</strong> the causal effect</u>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>Of course, we still need to find <span class="math inline">\hat\beta_0</span> (the slope). We found that <span class="math inline">\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}</span> earlier, so we just plug our solution of <span class="math inline">\hat\beta_1</span> in.</p>
<p>And now, we have our estimates <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>, and thus we now have a best-fit line and an estimate of the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>.</p>
<p>Note: in <a href="https://statsnotes.github.io/metrics/3.html">lesson 1.6</a>, we will discuss if the OLS estimator is a good estimator or not. For now, we just care about the mechanics of the estimator.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="multiple-linear-regression-with-linear-algebra" class="level1">
<h1>2.2.3: Multiple Linear Regression with Linear Algebra</h1>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section contains Linear Algebra. It is not essential that you understand this section, however, it can be helpful to develop intuition.</p>
</div>
</div>
<p>In <a href="https://statsnotes.github.io/intro/5.html">lesson 1.5</a>, we introduced the multiple linear regression model as the following:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + \dots + \beta_k x_{ki} + u_i
</span></p>
<p>However, it can be very useful to also see the multiple linear regression model in terms of linear algebra (vectors and matrices).</p>
<ul>
<li>The main reason for this is because this will make the estimation process far easier.</li>
</ul>
<p><br></p>
<p>The <span class="math inline">i</span>’th observation can be re-written in vector form as following:</p>
<p><span class="math display">
y_i = x_i'\beta + u_i, \text{ where }\beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{bmatrix} \text{ and }x_i = \begin{bmatrix}1 \\x_{1i} \\\vdots \\x_{ki}\end{bmatrix}
</span></p>
<ul>
<li>The <span class="math inline">x_i'</span> in the equation is the transpose of <span class="math inline">x_i</span>, to make matrix multiplication possible.</li>
<li>The first element of the <span class="math inline">x_i</span> matrix is 1, since <span class="math inline">1 \times \beta_0</span> gives us the first parameter (intercept) in the linear model.</li>
<li>Thus, when multiplying out, we get the same equation as the original multiple linear regression.</li>
</ul>
<p><br></p>
<p>Note how we have the subscript <span class="math inline">i</span> representing each individual observation. With a vector, we can expand out these subscripts.</p>
<ul>
<li>For example, instead of <span class="math inline">y_i</span>, we could have a vector with <span class="math inline">y_1, y_2, \dots, y_n</span> (assuming we have <span class="math inline">n</span> observations).</li>
<li>Same for <span class="math inline">x'_i</span>, which can be expanded into a vector of <span class="math inline">x_1', x_2', \dots x_n'</span>, and for the error term <span class="math inline">u_i</span>, which can be expanded into a vector of <span class="math inline">u_1, u_2, \dots, u_n</span>.</li>
</ul>
<p>Using this logic, we can obtain the following, with the <span class="math inline">x_i'</span> and <span class="math inline">\beta</span> being vectors within a vector:</p>
<p><span class="math display">
\begin{split}
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} &amp; = \begin{pmatrix}x_1'\beta + u_1 \\ x_2'\beta + u_2 \\ \vdots \\ x_n'\beta + u_n\end{pmatrix} \\
&amp; \\
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix}&amp; = \begin{pmatrix}x_1'\beta \\ x_2'\beta \\ \vdots \\ x_n'\beta\end{pmatrix} + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
\end{split}
</span></p>
<p>Since <span class="math inline">\beta</span> vector appears as a common factor for all observations <span class="math inline">i=1,...,n</span>, we can factor it out and have an equation:</p>
<p><span class="math display">
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} = \begin{pmatrix}x_1' \\ x_2' \\ \vdots \\ x_n'\end{pmatrix} \space \beta + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
</span></p>
<p><br></p>
<p>We can expand the <span class="math inline">x_1',...,x_n'</span> vector into a matrix. Remember that each <span class="math inline">x_1',...,x_n'</span> is already a vector of different explanatory variables. So, we get the following result:</p>
<p><span class="math display">
y = X \beta + u, \text{ where } X = \begin{bmatrix}1 &amp; x_{21} &amp; \dots &amp; x_{k1} \\1 &amp; x_{22} &amp; \dots &amp; x_{k2} \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\1 &amp; x_{2n} &amp; \dots &amp; x_{kn}\end{bmatrix}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression with Linear Algebra
</div>
</div>
<div class="callout-body-container callout-body">
<p>The multiple linear regression can be expressed in linear algebra as:</p>
<p><span class="math display">
y = X \beta + u
</span></p>
<p>Where vector <span class="math inline">y</span> is equal to:</p>
<p><span class="math display">
y = \begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix}
</span></p>
<p>Where matrix <span class="math inline">X</span> is equal to:</p>
<p><span class="math display">
X = \begin{bmatrix}1 &amp; x_{21} &amp; \dots &amp; x_{k1} \\1 &amp; x_{22} &amp; \dots &amp; x_{k2} \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\1 &amp; x_{2n} &amp; \dots &amp; x_{kn}\end{bmatrix}
</span></p>
<ul>
<li>Where the notation for elements of <span class="math inline">X</span> is <span class="math inline">x_{ki}</span>, with <span class="math inline">i</span> being the unit of observation <span class="math inline">i = 1, \dots n</span>, and <span class="math inline">k</span> being the explanatory variables index.</li>
<li>The first column of <span class="math inline">X</span> is a vector of 1, which exists because these 1’s are multiplied with <span class="math inline">\beta_0</span> in our model (see below).</li>
</ul>
<p>Where vector <span class="math inline">\beta</span> is equal to:</p>
<p><span class="math display">
\beta = \begin{pmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{pmatrix}
</span></p>
</div>
</div>
<p>The point of expressing the model in linear algebra is that it makes the estimation process far easier, as we will see in the next section.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="mathematics-of-the-ordinary-least-squares-estimator" class="level1">
<h1>2.2.4: Mathematics of the Ordinary Least Squares Estimator</h1>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section contains Linear Algebra. It is not essential that you understand this section, however, it can be helpful to develop intuition.</p>
</div>
</div>
<p>As we remember from <a href="https://statsnotes.github.io/theory/4.html#mathematics-of-the-ordinary-least-squares-estimator">1.4.4</a>, the goal of Ordinary Least Squares Estimation is to minimise the sum of squared errors. The sum of squared errors in multiple regression is:</p>
<p><span class="math display">
\begin{split}
SSE &amp; = \sum\limits_{i=1}^n (y_i - \hat y_i)^2\\
&amp; = \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} -  ... - \hat\beta_kx_{ki})^2
\end{split}
</span></p>
<p>Similar to our simple linear regression (but with additional variables), our minimisation condition is:</p>
<p><span class="math display">
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...) &amp; = \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} ...)^2 \\
&amp; = \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} S(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)
\end{split}
</span></p>
<p><br></p>
<p>Taking the partial derivatives of each parameter like in simple linear regression (see <a href="https://statsnotes.github.io/theory/4.html#mathematics-of-the-ordinary-least-squares-estimator">1.4.4</a>), we get first order conditions:</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\&amp; \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
&amp; \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>This system of equations includes <span class="math inline">k+1</span> variables and <span class="math inline">k+1</span> equations, which is way too difficult to solve.</p>
<p><br></p>
<p>Instead, we can use linear algebra. Let us define our estimation vector <span class="math inline">\hat{\beta}</span> as the value of <span class="math inline">\hat\beta</span> that minimises the sum of squared errors:</p>
<p><span class="math display">
\hat{\beta} = \min\limits_{b} (y - Xb)' (y - Xb) = \min\limits_b S(b)
</span></p>
<ul>
<li><span class="math inline">(y - Xb)</span> is our error, since <span class="math inline">\hat y = Xb</span>,</li>
</ul>
<p>We can expand <span class="math inline">S(b)</span> as follows:</p>
<p><span class="math display">
\begin{split}
S(b) &amp; = y'y - b'X'y - y'Xb + b'X'Xb \\
&amp; = y'y - 2b'X'y + b'X'Xb
\end{split}
</span></p>
<p>Taking the partial derivative in respect to <span class="math inline">b</span>:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = \begin{pmatrix}\frac{\partial S(b)}{\partial b_1} \\\vdots \\\frac{\partial S(b)}{\partial b_k}\end{pmatrix}
</span></p>
<p>Differentiating with the vector <span class="math inline">b</span> yields:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = -2X'y + 2X'Xb
</span></p>
<p>Evaluted at <span class="math inline">\hat{\beta}</span>, the derivatives should equal zero (since first order condition of finding minimums):</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} \biggr|_{\hat{\beta}} = -2X'y + 2X'X \hat{\beta} = 0
</span></p>
<p>When assuming <span class="math inline">X'X</span> is invertable, we can isolate <span class="math inline">\hat{\beta}</span> to find the solution to OLS:</p>
<p><span class="math display">
\begin{split}
-2X'y + 2X'X \hat{\beta} &amp; = 0 \\
2X'X\hat\beta &amp; = 2X'y \\
\hat\beta &amp; = (2X'X)^{-1} 2 X'y \\
\hat\beta &amp; = (X'X)^{-1}X'y
\end{split}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: OLS Estimate of <span class="math inline">\hat\beta</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Ordinary Least Squares Estimate of vector <span class="math inline">\hat\beta</span> for multiple linear regression is:</p>
<p><span class="math display">
\hat{\beta} = (X'X)^{-1} X'y
</span></p>
</div>
</div>
<p>Once we have estimates of <span class="math inline">\hat{\beta}</span>, we can plug them into our linear model to obtain fitted values:</p>
<p><span class="math display">
\begin{split}
\hat{y} &amp; = X\hat{\beta} \\
&amp; = X(X'X)^{-1} X'y
\end{split}
</span></p>
<p>Note: in <a href="https://statsnotes.github.io/metrics/3.html">lesson 2.3</a>, we will discuss if the OLS estimator is a good estimator or not. For now, we just care about the mechanics of the estimator.</p>
<p><br></p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="regression-anatomy-and-controlling-for-confounders" class="level1">
<h1>2.5.5: Regression Anatomy and Controlling for Confounders</h1>
<p>We talked about how multiple linear regression allows us to control for confounders in <a href="https://statsnotes.github.io/intro/5.html">lesson 1.5</a>. But what does that mean? How does it affect our interpretations of coefficients?</p>
<p>The <strong>Regression Anatomy</strong> Theory, also called the <strong>Frisch–Waugh–Lovell (FWL)</strong> theorem, illustrates this concept. Take our standard multiple linear regression:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i
</span></p>
<p><br></p>
<p>Let us say we are interested in <span class="math inline">x_1</span> (this can be generalised to any explanatory variable <span class="math inline">x_j</span>). Let us make <span class="math inline">x_1</span> the outcome variable of a regression with explanatory variables <span class="math inline">x_2, ..., x_k</span>:</p>
<p><span class="math display">
x_{1i} = \gamma_0 + \gamma_1 x_{2i} + ... + \gamma_{k-1}x_{ki} + \widetilde{r_{1i}}
</span></p>
<ul>
<li>Where <span class="math inline">\gamma_0, ..., \gamma_{k-1}</span> are coefficients.</li>
<li>Where <span class="math inline">\widetilde{r_{1i}}</span> is the error term.</li>
</ul>
<p>The error term is <span class="math inline">\widetilde{r_{1i}}</span>, which represents the part of <span class="math inline">x_{1i}</span> that are uncorrelated to <span class="math inline">x_2, ..., x_k</span>.</p>
<ul>
<li>In other words, <span class="math inline">\widetilde{r_{1i}}</span> is the part of <span class="math inline">x_1</span> that cannot be explained by any other explanatory variable <span class="math inline">x_2, ..., x_k</span>. (uncorrelated with them)</li>
</ul>
<p><br></p>
<p>Now, take the regression of with outcome variable <span class="math inline">y</span>, with all explanatory variables <u>except</u> <span class="math inline">x_1</span>:</p>
<p><span class="math display">
y_i = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i}
</span></p>
<ul>
<li>Where <span class="math inline">\delta_0, ..., \delta_{k-1}</span> are coefficients.</li>
<li>Where <span class="math inline">\widetilde {y_i}</span> is the error term.</li>
</ul>
<p>The error term is <span class="math inline">\widetilde{y_i}</span>, which is the part of <span class="math inline">y_i</span> that cannot be explained by <span class="math inline">x_2, ..., x_k</span> (uncorrelated with them).</p>
<p><br></p>
<p>Since <span class="math inline">\widetilde{y_i}</span> is not explained by <span class="math inline">x_2, ..., x_k</span>, variable <span class="math inline">x_1</span> must be the one explaining <span class="math inline">\widetilde{y_i}</span>.</p>
<ul>
<li>But, it is not the whole of <span class="math inline">x_1</span> explaining <span class="math inline">\tilde{y_i}</span>.</li>
<li>This is since <span class="math inline">x_1</span> may also correlated with <span class="math inline">x_2, ..., x_k</span>, and the correlated parts of <span class="math inline">x_1</span> with <span class="math inline">x_2, ..., x_k</span> are already picked up in the regression by the coefficients of <span class="math inline">x_2, ..., x_k</span>.</li>
</ul>
<p>Thus, <span class="math inline">\widetilde{y_i}</span> must be explained by the part of <span class="math inline">x_1</span> that is uncorrelated and not explained by <span class="math inline">x_2, ..., x_k</span>, which we derived earlier as <span class="math inline">\widetilde{r_{1i}}</span>.</p>
<p><br></p>
<p>Thus, we can create another regression with explanatory variable <span class="math inline">\widetilde{x_{1i}}</span> and outcome variable <span class="math inline">\widetilde{y_i}</span>.</p>
<p><span class="math display">
\widetilde{y_i} = \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i
</span></p>
<p>We can plug <span class="math inline">\widetilde{y_i}</span> back into our regression of <span class="math inline">y_i</span> with explanatory variables <span class="math inline">x_2 ..., x_k</span>, and re-arrange:</p>
<p><span class="math display">
\begin{split}
y_i &amp; = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i} \\
y_i &amp; = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i \\
y_i  &amp; = (\delta_0 + \alpha_0) + \alpha_1 \widetilde{r_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i
\end{split}
</span></p>
<p>As we can see, this new regression mirrors the original standard multiple linear regression:</p>
<p><span class="math display">
\begin{split}
y_i  &amp; = (\delta_0 + \alpha_0) + \alpha_1 \widetilde{r_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i \\
y_i &amp; = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + u_i
\end{split}
</span></p>
<ul>
<li>The <span class="math inline">\beta_0</span> in the original is analogous to the <span class="math inline">(\delta_0 + \alpha 0)</span>.</li>
<li>The <span class="math inline">\beta_1 x_{1i}</span> in the original is analogous to <span class="math inline">\alpha_1 \widetilde{r_{1i}}</span>.</li>
<li>The <span class="math inline">\beta_2 x_{2i} + \dots + \beta_k x_{ki}</span> is analogous to <span class="math inline">\delta_1 x_{2i} + ... + \delta_{k-1} x_{ki}</span>.</li>
<li>The <span class="math inline">u_i</span> is in both regressions.</li>
</ul>
<p><br></p>
<p>Importantly we know the <span class="math inline">\beta_1 x_{1i}</span> in the original is analogous to <span class="math inline">\alpha_1 \widetilde{r_{1i}}</span>. Thus, <u>the estimate of <span class="math inline">\alpha_1</span> will be the same as <span class="math inline">\beta_1</span> in the original regression</u>.</p>
<ul>
<li>The coefficient <span class="math inline">\alpha_1</span> (which is equal to <span class="math inline">\beta_1</span>) explains the expected change in <span class="math inline">y</span>, given an increase in the part of <span class="math inline">x_1</span> uncorrelated with <span class="math inline">x_2, ..., x_k</span>.</li>
<li>So essentially, <u>we have <strong>partialed out</strong> the effect of the other explanatory variables, and only focus on the effect on <span class="math inline">y</span> of the uncorrelated part of <span class="math inline">x_1</span> (which is <span class="math inline">\widetilde{r_{1i}}</span>)</u></li>
</ul>
<p>Or in other words, OLS in multiple linear regression estimates the effect of <span class="math inline">\widetilde{r_{1i}}</span> on <span class="math inline">y</span>.</p>
<ul>
<li>We can apply this to any explanatory variable <span class="math inline">x_1, \dots, x_k</span>. The uncorrelated parts of any explanatory variable <span class="math inline">x_j</span> are labelled <span class="math inline">\widetilde{r_{ji}}</span>.</li>
</ul>
<p><br></p>
<p>Using all this info, we can interpret the meaning of any coefficient <span class="math inline">\hat\beta_j</span> on the relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Coefficients in OLS
</div>
</div>
<div class="callout-body-container callout-body">
<p>The interpretation of <span class="math inline">\hat\beta_j</span> coefficient, multiplied to variable <span class="math inline">x_j</span> is:</p>
<ul>
<li>When <span class="math inline">x_j</span> increases by one unit, there is an expected <span class="math inline">\hat\beta_j</span> unit change in <span class="math inline">y</span>, holding all other explanatory variables constant.</li>
</ul>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="regression-anatomy-formula-for-ols-estimation" class="level1">
<h1>2.2.6: Regression Anatomy Formula for OLS Estimation</h1>
<p>In the last section, we discussed the idea of regression anatomy, and how including confounding variables changes the estimates.</p>
<ul>
<li>That instead of finding the effect of <span class="math inline">x_j</span> entirely on <span class="math inline">y</span>, we partial out the effect of other explanatory variables, and only find the effect of the uncorrelated part of <span class="math inline">x_j</span> (labelled <span class="math inline">\widetilde {r_{ji}}</span>) on <span class="math inline">y</span>.</li>
</ul>
<p>We have already discussed the mathematical solution of OLS in relation to linear algebra (see <a href="https://statsnotes.github.io/theory/6.html#mathematics-of-the-ordinary-least-squares-estimator">1.6.4</a>). However, we can also express the estimation solution of <span class="math inline">\hat\beta_j</span> in relation to the regression anatomy formula.</p>
<p><br></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><strong>Property 1:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
</span></p>
<ul>
<li>This is because we can expand the left to <span class="math inline">\sum x_i - \sum \bar x</span>.</li>
<li>Then, we know <span class="math inline">\sum x_i = \sum \bar x</span> (by the formula for mean), so <span class="math inline">\sum x_i - \sum \bar x = 0</span>.</li>
</ul>
<p><strong>Property 2:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y})
</span></p>
<ul>
<li>This is because on the right side can expand to <span class="math inline">\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]</span>.</li>
<li>Then, split into <span class="math inline">\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)</span>.</li>
<li>We know that by property 1 (which applies to any variable), <span class="math inline">\sum (y_i - \bar y) = 0</span>. Thus, the right side disappears, and we are left with <span class="math inline">\sum x_i (y_i - \bar y)</span>.</li>
</ul>
<p><strong>Property 3:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
</span></p>
<ul>
<li>Start by expanding right side to <span class="math inline">\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]</span></li>
<li>Which splits into <span class="math inline">\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)</span></li>
<li>By the first property, we know <span class="math inline">\sum x_i - \bar x = 0</span>, so we are only left with <span class="math inline">\sum x_i (x_i - \bar x)</span></li>
</ul>
</div>
</div>
<p><br></p>
<p>Knowing these properties of summation, let us begin.</p>
<p>Let us start off with the OLS estimator for simple linear regression, which calculates the <span class="math inline">\hat\beta_1</span>, the relationship between <span class="math inline">x</span> and <span class="math inline">y</span> (which we derived in <a href="https://statsnotes.github.io/theory/4.html#mathematics-of-the-ordinary-least-squares-estimator">1.4.4</a>):</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<p>Let us look at the numerator. Let us expand the numerator:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
= &amp; \sum\limits_{i=1}^n [(x_i - \bar x)y_i - (x_i - \bar x) \bar y] \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \sum\limits_{i=1}^n (x_i - \bar x) \bar y \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
\end{split}
</span></p>
<p>We know that <span class="math inline">\sum (x_i - \bar x) = 0</span> from the above properties. Thus, we can further simplify to:</p>
<p><span class="math display">
\begin{split}
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y(0) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i
\end{split}
</span></p>
<p>Thus, putting the numerator back in, we now we have the equation:</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})y_i}{\sum_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<p><br></p>
<p>We know that in multiple linear regression, <span class="math inline">\hat\beta_j</span> is not the full relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>. Instead, it is the relationship of the part of <span class="math inline">x_j</span> that is uncorrelated with all other explanatory variables, and <span class="math inline">y</span>.</p>
<ul>
<li>So in other words, it is the relationship of <span class="math inline">\widetilde{r_{ji}}</span> on <span class="math inline">y</span>.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reminder: Regression Anatomy
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember, in the last section, we defined <span class="math inline">\widetilde{r_{ji}}</span> as the error term in a regression of <span class="math inline">x_j</span> on all other explanatory variables:</p>
<p><span class="math display">
x_{ji} = \gamma_0 + \gamma_1 x_{1i} + ... + \gamma_{k-1}x_{ki} + \widetilde{r_{ji}}
</span></p>
<p>Thus, <span class="math inline">\widetilde{r_{ji}}</span> is the part of <span class="math inline">x_j</span> uncorrelated with any other explanatory variable.</p>
</div>
</div>
<p>So, since multiple linear regression is the relationship of <span class="math inline">\widetilde{r_{ji}}</span> on <span class="math inline">y</span>, instead of <span class="math inline">x</span> on <span class="math inline">y</span>, let us replace the <span class="math inline">x</span>’s in our formula with <span class="math inline">\widetilde{r_{ji}}</span>:</p>
<p><span class="math display">
\hat{\beta}_j = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))^2}
</span></p>
<p>We can actually simplify this more with a property of regression - remember, that the error term of a regression <span class="math inline">u</span>, should be such that <span class="math inline">E(u)=0</span>.</p>
<p>We know that <span class="math inline">\widetilde{r_{ji}}</span> is also the error term of a regression, so, <span class="math inline">E(\widetilde{r_{ji}}) = 0</span> as well. Plugging that into our equation, we can get the regression anatomy formula for OLS.</p>
<p><span class="math display">
\begin{split}
\hat{\beta}_j &amp; = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))^2} \\
\\
&amp; = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - 0)y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - 0)^2} \\
\\
&amp; = \frac{\sum_{i=1}^n \widetilde{r_{ji}} \ y_i}{\sum_{i=1}^n \widetilde {r_{ji}}^2}
\end{split}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Regression Anatomy Formula
</div>
</div>
<div class="callout-body-container callout-body">
<p>The OLS estimate of coefficient <span class="math inline">\beta_j</span> can be written in terms of regression anatomy as follows:</p>
<p><span class="math display">
\hat{\beta}_j = \frac{\sum_{i=1}^n \widetilde{r_{ji}} \ y_i}{\sum_{i=1}^n \widetilde {r_{ji}}^2}
</span></p>
</div>
</div>
<p>Note: in <a href="https://statsnotes.github.io/metrics/3.html">lesson 2.3</a>, we will discuss if the OLS estimator is a good estimator or not. For now, we just care about the mechanics of the estimator.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="r-squared-and-goodness-of-fit" class="level1">
<h1>2.2.7: R-Squared and Goodness of Fit</h1>
<p>For each observation, we know that the actual <span class="math inline">y_i</span> value is the predicted <span class="math inline">\hat y_i</span> plus the residual term <span class="math inline">\hat u_i</span>. Thus:</p>
<p><span class="math display">
y_i = \hat y_i + \hat u_i
</span></p>
<p>Now, let us define these three concepts: the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR):</p>
<p><span class="math display">
\begin{split}
&amp; SST = \sum\limits_{i=1}^n (y_i - \bar y)^2 \\
&amp; SSE = \sum\limits_{i=1}^n (\hat y_i - \bar y)^2 \\
&amp; SSR = \sum\limits_{i=1}^n (\hat u_i)^2
\end{split}
</span></p>
<ul>
<li>The SST explains the total amount of variation in <span class="math inline">y</span></li>
<li>The SSE is the amount of variation in <span class="math inline">y</span> explained by our model</li>
<li>The SSR is the amount of variation in <span class="math inline">y</span> not explained by our model</li>
</ul>
<p>Let us look at the total sum of squares (SST). We can manipulate it as follows:</p>
<p><span class="math display">
\begin{split}
SST &amp; = \sum\limits_{i=1}^n (y_i - \bar y)^2 \\
&amp; = \sum\limits_{i=1}^n(y_i - \hat y_i+ \hat y_i - \bar y)^2 \\
&amp; = \sum\limits_{i=1}^n((y_i - \hat y_i)+ \hat y_i - \bar y)^2 \\
&amp; = \sum\limits_{i=1}^n[\hat u_i + \hat y_i - \bar y]^2 \\
&amp; = \sum\limits_{i=1}^n[\hat u_i^2 + \hat u_i \hat y_i - \hat u_i \bar y + \hat y_i \hat u_i + \hat y_i^2 - \hat y_i \bar y-\bar y \hat u_i -\bar y \hat  y_i+\hat y^2_i] \\
&amp; = \sum\limits_{i=1}^n[ \hat u_i^2 + 2 \hat u_i \hat y_i+ \hat y_i^2 - 2 \hat u_i \bar y - 2 \hat y_i \bar y + \bar y ^2]
\end{split}
</span></p>
<p>By a property of linear regression, <span class="math inline">\sum \hat y_i \hat u_i = 0</span>. Knowing this, we can further simplify to:</p>
<p><span class="math display">
\begin{split}
SST &amp; = \sum\limits_{i=1}^n[ \hat u_i^2 + \hat y_i^2 - 2 \hat u_i \bar y - 2 \hat y_i \bar y + \bar y ^2] \\
&amp; = \sum\limits_{i=1}^n[\hat u_i^2 + (\hat y_i - \bar y)^2]\\
&amp; = \sum\limits_{i=1}^n \hat u_i^2 + \sum\limits_{i=1}^n(\hat y_i - \bar y)^2 \\
&amp; = SSE + SSR
\end{split}
</span></p>
<p>This makes sense: After all, SSE is the squared errors explained by the model, and SSR is the residual (non-explained) parts of the model, so together, they should be equal to the total sum of squares.</p>
<p><br></p>
<p>Using these properties, we can create a statistic which explains how well our model explains the variation in <span class="math inline">y</span>. This statistic is called <span class="math inline">R^2</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: R-Squared
</div>
</div>
<div class="callout-body-container callout-body">
<p>The R-squared metric is a metric describing how good of a fit our model is. Mathematically:</p>
<p><span class="math display">
R^2 = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}
</span></p>
<p>What does this <span class="math inline">R^2</span> value mean?</p>
<ul>
<li>Well SSE is the amount of variation in <span class="math inline">y</span> explained by our model, and SST is the total amount of variation in <span class="math inline">y</span>.</li>
<li>Thus, <span class="math inline">R^2</span> is the proportion of variation in <span class="math inline">y</span> explained by our model.</li>
</ul>
</div>
</div>
<p><span class="math inline">R^2</span> is always between 0 and 1:</p>
<ul>
<li>This is because it is a proportion, so and <span class="math inline">0 ≤ SSE ≤ SST</span>, so this must be true.</li>
<li>Values closer to 1 mean our model explains the variance in <span class="math inline">y</span> more</li>
<li>Values closer to 0 mean our model explains less of the variance in <span class="math inline">y</span>.</li>
</ul>
<p><span class="math inline">R^2</span> is also equal to the correlation coefficient between <span class="math inline">y_i</span> and <span class="math inline">\hat y_i</span>. In simple linear regression, <span class="math inline">R^2</span> is also equal to the square of the correlation coefficient <span class="math inline">r</span> between <span class="math inline">x</span> and <span class="math inline">y</span>.</p>
<p><br></p>
<p>There is an additional property with <span class="math inline">R^2</span> in multiple linear regression: <u><span class="math inline">R^2</span> never falls when another explanatory is added to the regression.</u></p>
<ul>
<li>So essentially, any explanatory variable added to the regression will always boost R-squared.</li>
</ul>
<p>This shows the downside of R-squared as a metric - if we just include random variables, by chance, these variables will explain the variation in <span class="math inline">y</span>.</p>
<ul>
<li>Thus, focusing on R-squared can result in models with silly or nonsensical explanatory variables.</li>
</ul>
<p>We will discuss the importance of careful model selection in the later parts of the course, when we move to more applied statistics for social science purposes.</p>
<p><br></p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>