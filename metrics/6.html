<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistics for Social Scientists</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="6_files/libs/clipboard/clipboard.min.js"></script>
<script src="6_files/libs/quarto-html/quarto.js"></script>
<script src="6_files/libs/quarto-html/popper.min.js"></script>
<script src="6_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="6_files/libs/quarto-html/anchor.min.js"></script>
<link href="6_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="6_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="6_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="6_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="6_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Statistics for Social Scientists</h1>
            <p class="subtitle lead">Lesson 2.6: The Maximum Likelihood Estimator</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Lesson 2.6: The Maximum Likelihood Estimator</h2>
   
  <ul class="collapse">
  <li><a href="#intuition-of-the-maximum-likelihood-estimator" id="toc-intuition-of-the-maximum-likelihood-estimator" class="nav-link active" data-scroll-target="#intuition-of-the-maximum-likelihood-estimator">2.6.1: Intuition of the Maximum Likelihood Estimator</a></li>
  <li><a href="#simple-example-of-maximum-likelihood-estimation" id="toc-simple-example-of-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#simple-example-of-maximum-likelihood-estimation">2.6.2: Simple Example of Maximum Likelihood Estimation</a></li>
  <li><a href="#gradient-descent-methods-of-estimation" id="toc-gradient-descent-methods-of-estimation" class="nav-link" data-scroll-target="#gradient-descent-methods-of-estimation">2.6.3: Gradient Descent Methods of Estimation</a></li>
  <li><a href="#properties-of-the-maximum-likelihood-estimate" id="toc-properties-of-the-maximum-likelihood-estimate" class="nav-link" data-scroll-target="#properties-of-the-maximum-likelihood-estimate">2.6.4: Properties of the Maximum Likelihood Estimate</a></li>
  <li><a href="#likelihood-ratios" id="toc-likelihood-ratios" class="nav-link" data-scroll-target="#likelihood-ratios">2.6.5: Likelihood Ratios</a></li>
  <li><a href="#information-criteria-statistics" id="toc-information-criteria-statistics" class="nav-link" data-scroll-target="#information-criteria-statistics">2.6.6: Information Criteria Statistics</a></li>
  <li><a href="#ols-as-a-maximum-likelihood-estimator" id="toc-ols-as-a-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#ols-as-a-maximum-likelihood-estimator">2.6.7: OLS as a Maximum Likelihood Estimator</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p>This lesson introduces and discusses a new estimator: the Maximum Likelihood Estimator. We first introduce the intuition of MLE, provide a simple example of MLE, and talk about how computers can use gradient descent algorithms to conduct MLE. Then, we discuss a few additional topics related to MLE and regressions.</p>
<p>This lesson covers the following topics:</p>
<ul>
<li>Introduces the intuition of maximum likelihood estimation, and what likelihood functions are.</li>
<li>Discusses the estimation process of maximum likelihood, including how gradient descent algorithms are often used by computers.</li>
<li>Discuss some statistics relating to likelihood functions, that can help us determine which models are better.</li>
<li>Show how OLS is a special case of maximum likelihood estimation under certain circumstances.</li>
</ul>
</div>
</div>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
<section id="intuition-of-the-maximum-likelihood-estimator" class="level1">
<h1>2.6.1: Intuition of the Maximum Likelihood Estimator</h1>
<p>In Part I, lessons <a href="https://statsnotes.github.io/intro/8.html">1.8</a>, <a href="https://statsnotes.github.io/intro/9.html">1.9</a>, and <a href="https://statsnotes.github.io/intro/10.html">1.10</a>, we covered non-linear extensions to the linear model, including logistic, negative binomial, and poisson regression.</p>
<p>All of these models involve a <strong>link-function</strong>, which takes the linear model, and transforms it into a non-linear output.</p>
<p>Let us define any link functions as function <span class="math inline">G</span>. Our models all take the form:</p>
<p><span class="math display">
y = G(\beta_0 + \beta_1 x_1 + \dots + \beta_k x_k) + u
</span></p>
<p>We could take the same estimation approach as OLS, and minimise the sum of squared errors:</p>
<p><span class="math display">
\min\limits_b \sum\limits_{i=1}^n (y_i - \hat y)^2 = \min\limits_b \sum\limits_{i=1}^n (y_i - G(b_0 + b_1 x_1 + \dots + b_kx_k)^2
</span></p>
<p>However, this estimate would not be efficient due to <strong>heteroscedasticity</strong>. This is because the assumption of constant variance is unrealistic in binary/categorical outcomes.</p>
<p>We can use an alternative estimator in this case: the <strong>Maximum Likelihood Estimator</strong></p>
<ul>
<li>The maximum likelihood estimator can also be used on other models where there is no “error term” or algebraic way to solve for the solution.</li>
</ul>
<p><br></p>
<p>But what is maximum likelihood estimation? The answer is it is kind of complex. We will first introduce the intuition behind the estimator, before delving into the mathematics of estimation.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Maximum Likelihood Estimation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Maximum Likelihood Estimator, intuitively speaking, attempts to find the values of the parameters of the model which maximise the probability of the observed data under the fitted model.</p>
<p>In other words - every real parameter value <span class="math inline">\theta</span> results in a probability of getting a certain data set.</p>
<ul>
<li>For example, in a linear regression, if the true population parameter <span class="math inline">\beta_1 = 1</span>, then we would expect the relationship between <span class="math inline">x</span> and <span class="math inline">y</span> to conform to a line with a slope of 1. That is the most likely observed data we would get.</li>
</ul>
<p>Maximum Likelihood attempts to find the value of <span class="math inline">\theta</span> that gives the highest likelihood of observing our sample data. That value of <span class="math inline">\theta</span> which maximise the likelihood is the estimate <span class="math inline">\hat\theta</span>.</p>
</div>
</div>
<p><br></p>
<p>So the goal is to find the parameter values that maximise the likelihood of the data observed. But what is the likelihood of the data observed?</p>
<p>Let us be randomly sampling from a population of random variable <span class="math inline">y</span>, with the population described by some probability density function <span class="math inline">f(y; \theta)</span> (see <a href="https://statsnotes.github.io/intro/1.html#probability-mass-and-density-functions">1.1.3</a>)</p>
<ul>
<li>This probability density function is determined by some parameter <span class="math inline">\theta</span> (for example, maybe mean, or variance, or multiple parameters), and the variable in question <span class="math inline">y</span>.</li>
<li>The probability density function outputs the likelihood of randomly selecting that specific <span class="math inline">y</span>, given the parameter <span class="math inline">\theta</span>.</li>
</ul>
<p>Thus, the probability of selecting point <span class="math inline">y_1</span> in our data is <span class="math inline">f(y_1, \theta)</span>, the probability of selecting point <span class="math inline">y_2</span> in our data is <span class="math inline">f(y_2, \theta)</span>, and so on.</p>
<p>We know by the rules of probability, that the probability of two independent events is their independent probabilities multiplied. Thus, the probability of selecting points in our sample <span class="math inline">(y_1, \dots, y_n)</span> is the probability of selecting each point multiplied:</p>
<p><span class="math display">
f(y_1, \dots, y_n; \theta) = f(y_1; \theta) \times f(y_2; \theta) \times \dots \times f(y_n; \theta)
</span></p>
<p>This is the likelihood of observing the <span class="math inline">y</span> values in our data. Thus, our likelihood function <span class="math inline">L</span> is defined as:</p>
<p><span class="math display">
\begin{split}
L(\theta; y_1, \dots, y_n) &amp; = f(y_1; \theta) f(y_2; \theta) \dots f(y_n; \theta) \\
&amp; = \prod\limits_{i=1}^nf(y_i; \theta)
\end{split}
</span></p>
<ul>
<li>Where the likelihood function <span class="math inline">L</span> depends on our population parameter <span class="math inline">\theta</span>, and the observed data <span class="math inline">y_1, \dots, y_n</span>.</li>
</ul>
<p><br></p>
<p>We want to find some <span class="math inline">\hat\theta</span> that <strong>maximises</strong> this likelihood function:</p>
<p><span class="math display">
\hat\theta = \max\limits_{\theta}L(\theta; y_1, \dots, y_n)
</span></p>
<p>However, this is typically quite difficult. However, product notation is very difficult to work with in terms of calculus (which is how we typically optimise functions).</p>
<p>There is a solution to this problem - we can use the log-likelihood function, which is just the log of the likelihood function. This is because the value of <span class="math inline">\theta</span> which maximises the log-likelihood function, also maximises the likelihood function:</p>
<p><span class="math display">
\log L(\theta; y_i, \dots, y_n) = \log \prod_{i=1}^n f(y_i; \theta)
</span></p>
<p>But why is the log-likelihood function useful - it still has product notation in it? Well actually, with properties of logarithms, we can convert the log of a product notation, into a summation notation (which is much easier to work with mathematically).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Converting Between Product and Summation Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can convert between the log of a product notation to the log of a summation notation as follows (because of the rules of logarithms):</p>
<p><span class="math display">
\begin{split}
\log \left( \prod\limits_{i=1}^n x_i\right) &amp; = \log(x_1 \times x_2 \times \dots \times x_n) \\
&amp; = \log(x_1) + \log(x_2) + \dots + \log(x_n) \\
&amp; = \sum\limits_{i=1}^n\log (x_i)
\end{split}
</span></p>
</div>
</div>
<p>Thus, we can rewrite our log-likelihood function as following:</p>
<p><span class="math display">
Log \  L(\theta; y_1, \dots, y_n) = \sum\limits_{i=1}^n f(y_i; \theta)
</span></p>
<p>This is typically much more easier to optimise mathematically.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="simple-example-of-maximum-likelihood-estimation" class="level1">
<h1>2.6.2: Simple Example of Maximum Likelihood Estimation</h1>
<p>To illustrate maximum likelihood estimation, let us start with the simplest example of a logistic regression - a logistic regression with only one intercept term and no coefficients/explanatory variables.</p>
<p><span class="math display">
\log \left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0
</span></p>
<p>And thus (if you do not understand, see lesson <a href="https://statsnotes.github.io/intro/8.html">1.8</a> on logistic regression):</p>
<p><span class="math display">
\pi_i = Pr(y_i = 1) = \frac{e^{\beta_0}}{1 + e^{\beta_0}} = \pi
</span></p>
<ul>
<li>Since there is no explanatory variables, all units <span class="math inline">i</span> will have the same probability <span class="math inline">\pi</span> to be in category <span class="math inline">y = 1</span>.</li>
</ul>
<p><br></p>
<p>We have observations <span class="math inline">y_1, \dots, y_n</span> from a Bernoulli distribution. The probability density function of a Bernoulli distribution is the following:</p>
<p><span class="math display">
f(y; \theta) = Pr(y = 1) = \pi^y(1 - \pi)^{1-y}
</span></p>
<ul>
<li>Where <span class="math inline">y</span> can be either <span class="math inline">y=0</span> or <span class="math inline">y=1</span>.</li>
</ul>
<p>We know by the rules of probability, that the probability of two independent events is their independent probabilities multiplied. Thus, the probability of selecting points in our sample <span class="math inline">(y_1, \dots, y_n)</span> is the probability of selecting each point multiplied:</p>
<p><span class="math display">
f(y_1, \dots y_n| \pi) = f(y_1; \pi) \times \dots \times f(y_n; \pi) = \prod\limits_{i=1}^nf(y_i; \pi)
</span></p>
<p>Thus, we can create our likelihood function <span class="math inline">L</span>, which is the probability of the observed data, given some value of <span class="math inline">\pi</span>:</p>
<p><span class="math display">
\begin{split}
L(\pi; y_i) &amp; = \prod\limits_{i=1}^n f(y_i;\pi)\\
&amp; =  \prod\limits_{i=1}^n \pi^{y_i}(1- \pi)^{1 - y_i} \\
&amp; = \pi^m (1-\pi)^{n-m}
\end{split}
</span></p>
<ul>
<li>Where <span class="math inline">m</span> is the number of observations with <span class="math inline">y_i = 1</span>, and <span class="math inline">n-m</span> is the number of observations with <span class="math inline">y_i = 0</span>.</li>
</ul>
<p><br></p>
<p>Now, we want to find the value of <span class="math inline">\pi</span> that will maximise our likelihood function. This will be easier with the log likelihood function:</p>
<p><span class="math display">
\log L(\pi; y_i) = \sum\limits_{1=1}^n[y_i \log \pi + (1 - y_i \log (1 - \pi)]
</span></p>
<p>To maximise this, we will simply use the first derivative, and set it equal to 0 (first order conditions):</p>
<p><span class="math display">
\begin{split}
\frac{\partial\log L(\pi; y_i)}{\partial \pi} &amp; = \frac{\partial}{\partial \pi} \sum\limits_{1=1}^ n[y_i \log \pi + (1 - y_i) \log (1 - \pi)] \\
&amp; = \sum\limits_{1=1}^ n \left( \frac{y_i}{\pi} - \frac{1 - y_i}{1 - \pi} \right)
\end{split}
</span></p>
<ul>
<li>If you are confused about this derivation, consult derivative properties in appendix A.</li>
</ul>
<p>Now, set equal to 0 and solve for <span class="math inline">\pi</span>:</p>
<p><span class="math display">
\begin{split}
0 &amp; = \sum\limits_{1=1}^ n \left( \frac{y_i}{\pi} - \frac{1 - y_i}{1 - \pi} \right) \\
0 &amp; = \frac{m}{\pi} - \frac{n-m}{1 - \pi} \\
\pi &amp; = \frac{m}{n}
\end{split}
</span></p>
<p>Here, our estimate <span class="math inline">\hat\pi</span> is just <span class="math inline">m/n</span>, which makes sense:</p>
<ul>
<li><span class="math inline">m</span> is the number of observations with <span class="math inline">y_i = 1</span>, and <span class="math inline">n</span> is the total number of observations.</li>
<li><span class="math inline">\pi</span> is the probability that an observation is <span class="math inline">y_i = 1</span>.</li>
<li>Thus, the probability of an observation being <span class="math inline">y_i = 1</span> should naturally be <span class="math inline">m/n</span>.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="gradient-descent-methods-of-estimation" class="level1">
<h1>2.6.3: Gradient Descent Methods of Estimation</h1>
<p>We just covered a very simple version of maximum likelihood estimation, applied to a simple logistic model with no coefficients or explanatory variables.</p>
<p>However, what happens when we have a more complex model?</p>
<ul>
<li>The answer is it is generally very difficult, if not impossible, to solve by hand.</li>
<li>Often, there isn’t even a mathematical closed-form solution that we can solve for.</li>
</ul>
<p>Instead, we use a machine learning method called <strong>gradient descent</strong> to approximate the <span class="math inline">\theta</span> that maximises the likelihood function.</p>
<p><br></p>
<p><strong>Gradient Descent</strong> is an algorithmic way of optimsation, when mathematical solutions are not practical. The algorithm takes this form:</p>
<ol type="1">
<li>We randomly choose a value of <span class="math inline">\theta</span> to start, and calculate the likelihood <span class="math inline">L</span> at that <span class="math inline">\theta</span>.</li>
<li>Then, we slightly go above our original <span class="math inline">\theta</span>, and calculate the likelihood <span class="math inline">L</span>. We also go slightly below our original <span class="math inline">\theta</span>, and calculate the likelihood <span class="math inline">L</span>.</li>
<li>Now that we have tested both directions from our original <span class="math inline">\theta</span>, choose the direction (above or below) that had the higher likelihood <span class="math inline">L</span> increase.</li>
<li>Once choosing the direction, shift in that direction to the new <span class="math inline">\theta</span> value. Once again, look above and below this <span class="math inline">\theta</span> value, and see moving in which direction increases <span class="math inline">L</span> the most.</li>
<li>Keep doing the same process of moving in the direction that raises <span class="math inline">L</span> the most, looking around, determining which direction raises <span class="math inline">L</span> more, and moving in that direction.</li>
<li>Stop moving when you are at some point <span class="math inline">\theta</span> where both directions decreases <span class="math inline">L</span>. We are at the “top of the mountain”, the maximum.</li>
</ol>
<p><br></p>
<p>You might note that this algorithm is not flawless - for example, what about local maximums?</p>
<ul>
<li>After all, a local maximum will also be a point where both sides decreases <span class="math inline">L</span>. However, it will not be the global maximum that maximises <span class="math inline">L</span>.</li>
</ul>
<p>Luckily for us, this is not an issue with any type of logistic, poisson, or negative binomial model. This is because all these models are single-peaked with only a global maximum and no other extreme points at stationary points (see Appendix A for mathematical help).</p>
<p>The figure below shows the typical likelihood function of a logsitic regression with an intercept and one explanatory variable. Note how it is only single-peaked.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3919234954.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p><br></p>
<p>However, in some more advanced models, we will have this local maximum issue. There is a solution to this.</p>
<ul>
<li>We can essentially run many many different gradient descent estimates, starting at different points.</li>
<li>The highest <span class="math inline">L</span> maximum out of all of these gradient descents will be considered the global maximum.</li>
<li>Theoretically, if we run enough gradient descents, we will find the global maximum for sure.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="properties-of-the-maximum-likelihood-estimate" class="level1">
<h1>2.6.4: Properties of the Maximum Likelihood Estimate</h1>
<p>The maximum likelihood estimator has a few very useful properties for us.</p>
<p>First, the maximum likelihood estimator is <strong>asymptotically consistent</strong>. That means under large sample sizes <span class="math inline">n \rightarrow ∞</span>, the estimator will converge around the true parameter value:</p>
<p><span class="math display">
\text{plim} \left( \hat\theta^{MLE} \right) = \theta
</span></p>
<p>Second, the maximum likelihood estimator is <strong>asymptotically normal</strong>. That means under large sample sizes <span class="math inline">n \rightarrow ∞</span>, the sampling distribution will be normally distributed.</p>
<ul>
<li>This allows us to conduct statistical inference and hypothesis tests.</li>
</ul>
<p>Finally, the maximum likelihood estimator has the <strong>smallest asymptotic variance</strong> in the general class of estimators.</p>
<ul>
<li>It is more efficient that OLS in these discrete-choice models where homoscedasticity is present.</li>
<li>Thus, it is the most asymptotically efficient estimator.</li>
</ul>
<p>Thus, the Maximum Likelihood Estimator is an ideal estimator for the logistic, poisson, and negative binomial regressions.</p>
<p><br></p>
<p>Also, a final property of maximum likelihood estimation is that under the gauss-markov conditions, the maximum likelihood estimator is equivalent to the ordinary least squares estimator.</p>
<ul>
<li>We will explore this in section <a href="https://statsnotes.github.io/metrics/6.html#ols-as-a-maximum-likelihood-estimator">2.6.7</a>.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="likelihood-ratios" class="level1">
<h1>2.6.5: Likelihood Ratios</h1>
<p>In regressions, we sometimes want to test more than one coefficient at a time.</p>
<ul>
<li>In linear regression, we use the F-test to do this. F-tests compare the <span class="math inline">R^2</span> of each model.</li>
<li>In logistic, poisson, and negative binomial regression, we use the likelihood ratio tests (that we briefly went over in lesson <a href="https://statsnotes.github.io/intro/8.html">1.8</a>).</li>
</ul>
<p>However, we did not go into detail on what the likelihood ratio test is doing.</p>
<p><br></p>
<p>The intuition of testing multiple coefficients is the same between linear and non-linear regression models:</p>
<ul>
<li>We want to see if the alternative model with extra coefficients is a significantly better fit than the null model missing the extra coefficients.</li>
</ul>
<p>In linear regression, <span class="math inline">R^2</span> is the natural choice of determining how well a model is fit.</p>
<ul>
<li><span class="math inline">R^2</span> is after all, the amount of variation in <span class="math inline">y</span> explained by the explanatory variables.</li>
<li>Thus, the F-test compares <span class="math inline">R^2</span> values between two models, and sees if there is a statistically significant improvement in <span class="math inline">R^2</span> in the alternative model.</li>
</ul>
<p>However, in logistic (and other non-linear regressions), the <span class="math inline">R^2</span> idea does not really make sense.</p>
<ul>
<li>The variation in <span class="math inline">y</span> isn’t a very useful idea when <span class="math inline">y</span> is binary and comes from a Bernoulli distribution.</li>
</ul>
<p><br></p>
<p>Thus, we need an alternative way to measure the fit of a model. Well, we already know a great way to determine the fit - the likelihood function. After all, the likelihood function is how likely we are to observe the data we have, given our parameters and our model.</p>
<p>Larger values of likelihood indicate our model better explains <span class="math inline">y</span>. Thus, we can use the likelihood function outputs as an indication of fit, just like we did <span class="math inline">R^2</span> in linear regression.</p>
<p><br></p>
<p>More specifically, we want to consider the difference between two model’s likelihoods. This is the likelihood ratio test statistic <span class="math inline">L^2</span>:</p>
<p><span class="math display">
L^2 = 2\log \left( \frac{L_1}{L_0}\right) = 2 \log (L_2) - 2 \log (L_1)
</span></p>
<ul>
<li>Where <span class="math inline">L_1</span> is the likelihood of the alternative hypothesis, and <span class="math inline">L_0</span> is the likelihood of the null model.</li>
<li>The final part of the equation is derived due to logarithmic rules.</li>
</ul>
<p><br></p>
<p>Once we have our test statistic <span class="math inline">L^2</span>, we consult a <span class="math inline">\chi^2</span> distribution, with degrees of freedom equal to the number of extra coefficients in the alternative hypothesis.</p>
<ul>
<li>A small p-value means that our alternative model with more coefficients is statistically significantly better fit than the null model.</li>
<li>A large p-value means we cannot reject that the null hypothesis is just as good as our bigger model with more coefficients.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="information-criteria-statistics" class="level1">
<h1>2.6.6: Information Criteria Statistics</h1>
<p>We have talked about using the likelihood ratio test in comparing different models.</p>
<p>However, there is an issue with the likelihood ratio test - it is a nested model test:</p>
<ul>
<li>The null model’s coefficients must also be included in the alternative model, just that the alternative model has a few extra coefficients.</li>
</ul>
<p>But, what if we wanted to compare two models that had different coefficients, and that were not nested?</p>
<p><br></p>
<p>First, we have to decide what we want in a comparison of models.</p>
<ul>
<li>Obviously, we want a model that fits the data well (which in this case, means a high likelihood).</li>
<li>But, we also do not want an overly complex model. Why? Well, say we have one model with 3 explanatory variables, and another with 20 explanatory variables, but both have the same likelihood. Which one is better? We would rather the more efficient model with 3 explanatory variables (not to mention, this will reduce our variance in our estimates).</li>
</ul>
<p><br></p>
<p>So we want a statistic that balances the fit of the model, and the complexity of the model. These statistics are often called <strong>Information Criteria (IC) Statistics</strong>.</p>
<p>The most commonly used one is called <strong>Akaike’s Information Criterion</strong>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Akaike’s Information Criterion (AIC)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Akaike’s Information Criterion (almost always called AIC) is an information criteria statistic that allows us to compare different models with different explanatory variables, and determine which model is better.</p>
<p>The formula for AIC of a model is as follows:</p>
<p><span class="math display">
AIC = -2 \log L + 2k
</span></p>
<ul>
<li>Where <span class="math inline">L</span> is the likelihood of the model given by the likelihood function.</li>
<li>Where <span class="math inline">k</span> is the number of coefficients/parameters in the model.</li>
</ul>
<p>The lower the AIC, the better the model is.</p>
</div>
</div>
<p>There are also other information criterion statistics, but they are not as commonly used as AIC.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="ols-as-a-maximum-likelihood-estimator" class="level1">
<h1>2.6.7: OLS as a Maximum Likelihood Estimator</h1>
<p>The Maximum Likelihood Estimator produces the same results as the OLS when a few properties are met:</p>
<ul>
<li>First, the error term <span class="math inline">u</span> must be normally distributed, such that <span class="math inline">n \sim \mathcal N(0, \sigma^2)</span>.</li>
<li>Second, that we assume homoscedasticity: <span class="math inline">Var(u|x) = \sigma^2</span>.</li>
</ul>
<p><br></p>
<section id="proof-of-ols-and-mle-equivalency" class="level3">
<h3 class="anchored" data-anchor-id="proof-of-ols-and-mle-equivalency">Proof of OLS and MLE Equivalency</h3>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>This proof’s content is advanced. You do not need to know this.</p>
</div>
</div>
<p>A normal distribution has the following probability density function (as discussed in <a href="https://statsnotes.github.io/intro/1.html#the-normal-distribution">1.1.6</a>):</p>
<p><span class="math display">
f(y) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{\left( \frac{(x- \mu)^2}{2 -\sigma^2}\right)}
</span></p>
<p>Assuming our error term is normally distributed, we know that the conditional probability density function of our linear model <span class="math inline">y = \beta_0 + \beta_1 x + u_i</span> is as follows:</p>
<p><span class="math display">
f(y_i|x_i;\beta_0, \beta_1, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}}
e^{ \left( -\frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2 \right)}</span></p>
<p>By the independence of <span class="math inline">y_1, \dots, y_n</span>, the likelihood function is:</p>
<p><span class="math display">
\begin{split}
L &amp; = \prod_{i=1}^nf(y_i|x_i; \beta_0, \beta_1, \sigma^2) \\
&amp; =  \frac{1}{\sqrt{2 \pi \sigma^2}}
e^{ \left( -\frac{1}{2 \sigma^2}\sum_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2 \right)}
\end{split}
</span></p>
<p>The log-likelihood function is thus:</p>
<p><span class="math display">
\log L = -\frac{n}{2} \log (2 \pi) - \frac{n}{2} \log (\sigma^2) -\frac{1}{2 \sigma^2}\sum_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2
</span></p>
<p>We can maximise the log-likelihood function (and thus the likelihood function) by finding the first order conditions:</p>
<p><span class="math display">
\begin{split}
\frac{\partial \log L}{\partial\beta_0} : &amp; \ -\frac{1}{\sigma^2} \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) \times -1 =0 \\
\frac{\partial \log L}{\partial\beta_1} : &amp; \ -\frac{1}{\sigma^2} \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) \times -x_i =0 \\
\frac{\partial \log L}{\partial\sigma^2} : &amp; \ -\frac{n}{2\sigma^2}  + \frac{1}{2\sigma^4} \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) =0 \\
\end{split}
</span></p>
<p>We can focus on the first two conditions, since we are interested in the intercept <span class="math inline">\beta_0</span> and coefficient <span class="math inline">\beta_1</span> estimates. We can ignore the initial <span class="math inline">\frac{1}{\sigma^2}</span>, since if the summation equals zero, the whole partial derivative will also equal zero.</p>
<p>Thus, our first order conditions for maximum likelihood estimation are:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i)x_i = 0
\end{split}
</span></p>
<p>These conditions are identical to our OLS conditions. Thus, the maximum likelihood estimator is equivalent to OLS given normality of the error term and homoscedasticity.</p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>