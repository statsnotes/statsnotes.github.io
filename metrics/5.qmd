---
title: "Statistics for Social Scientists"
subtitle: "Lesson 2.5: Method of Moments Estimator"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Lesson 2.5: Method of Moments Estimator"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.6.6: OLS as a Method of Moments Estimator

We have mostly focused on one specific estimator: the Ordinary Least Squares estimator. However, in this section, we will focus on the Method of Moments Estimator, and how actually, OLS is a special case of the Method of Moments estimator.

::: callout-tip
## Definition: Method of Moments Estimator

The Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population **moments** of interest - which are the population parameters written in terms of expected values.

Then, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter.

-   For example, to estimate the population mean, the Method of Moments uses the sample mean.

We should have **at least** as many population moments, as we have estimands.
:::

<br />

This is a little hard to grasp. However, a simple example of the Method of Moments estimator can help us understand this.

Let us say that we have some random variable $y$, with a true population mean $\mu$. We want to estimate $\mu$, but we only have a sample of the population

First, let us define the moment of interest $\mu$ in terms of expectation:

$$
\mu = E(y)
$$

The method of moments says we should use the sample equivalent of the population parameter. The sample equivalent of $\mu$ (the true mean of the population), is of course, the sample mean $\bar y$.

With the method of moments estimator, our "moment" estimate of the true mean $\mu$ of $y$ would be:

$$
\hat\mu = \bar y = \frac{1}{n}\sum\limits_{i=1}^ny_i
$$

<br />

Another example of a Method of Moments estimator is if we have some random variable $y$, and we want to estimate the population mean $\mu$ and variance $\sigma^2$.

Here, we have two population moments $\mu$ and $\sigma^2$, that can be expressed in expected value:

$$
\begin{split}
& \mu = E(y) \\
& \sigma^2 = E((y - \mu)^2)
\end{split}
$$

The method of moments estimator uses the sample equivalents of the population parameters to estimate it. Thus, our "moment" estimates are:

$$
\begin{split}
& \hat\mu = \bar y \\
& \hat\sigma^2 = S.Var(y)
\end{split}
$$

-   Where $S.Var(y)$ is the variance of the sample $y$.

<br />

[OLS can also be considered a Method of Moments estimator]{.underline}. We will have moments for each parameter $\beta_0$ and $\beta_1$ we are trying to estimate.

::: callout-tip
## Definition: Bivariate Regression as a Method of Moments Estimator

Consider the bivariate regression model:

$$
y = \beta_0 + \beta_1x + u
$$

The OLS estimator can be derived as a method of moments estimator, with 2 moments, one for each parameter ($\beta_0, \beta_1$):

$$
\begin{split}
& E(y-\beta_0 -\beta_1x) = 0 \\
& E(x(y - \beta_0 - \beta_1 x)) = 0
\end{split}
$$

Since we know $u = y - \beta_0 - \beta_1 x$, we can rewrite the two moments as:

$$
\begin{split}
& E(u) = 0 \\
& E(xu) = 0
\end{split}
$$

The estimates of these moments would use the sample equivalents: $\hat\beta_0$ and $\hat\beta_1$.

$$
\begin{split}
& E(y-\hat\beta_0 -\hat\beta_1x) = 0 \\
& E(x(y - \hat\beta_0 - \hat\beta_1 x)) = 0
\end{split}
$$
:::

<br />

Let us prove that OLS is a special case of the method of moments estimator. Remember our OLS minimisation conditions (from [1.4.4](https://statsnotes.github.io/theory/4.html#mathematics-of-the-ordinary-least-squares-estimator))

$$
\begin{split}
& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
& \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
$$

Since by definition, average/expectation is $E(x) = \frac{1}{n} \sum x_i$, we can rewrite the OLS minimisation conditions as:

$$
\begin{split}
& n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
& n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
$$

And since anything multiplied to a zero turns into zero, we can ignore the $n$ in the first order condition, and only focus on the expected value part. Thus, our conditions are:

$$
\begin{split}
& E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
& E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
$$

Which as we can see, are the exact same minimisation conditions as the method of moments estimator. Thus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients.

<br />

Note how the two moment conditions of the Method of Moments estimator can be written as:

$$
\begin{split}
& E(u) = 0 \\
& E(xu) = 0
\end{split}
$$

These moments also imply that $Cov(x,u) = 0$, which was proved in [1.6.1](https://statsnotes.github.io/theory/6.html#residuals-and-properties-of-ols-estimator). This is the assumption of exogeneity, as discussed in [1.6.5](https://statsnotes.github.io/theory/6.html#endogeneity-and-violation-of-gauss-markov).

Thus, this means that if we do not meet exogeneity, and have endogeneity present, we are estimating our parameters with an invalid moment condition - since the moments are not true of endogeneity is present.

-   This is another reason why violating endogeneity produces biased estimates - since endogeneity means the moment conditions are invalid.

We will come back to the method of moments estimator in lesson 1.10 on instrumental variables, which will discuss how we can fix this invalid moment condition when endogeneity is present.

<br />
