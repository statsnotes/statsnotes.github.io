<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistics for Social Scientists</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="4_files/libs/clipboard/clipboard.min.js"></script>
<script src="4_files/libs/quarto-html/quarto.js"></script>
<script src="4_files/libs/quarto-html/popper.min.js"></script>
<script src="4_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="4_files/libs/quarto-html/anchor.min.js"></script>
<link href="4_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="4_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="4_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="4_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="4_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Statistics for Social Scientists</h1>
            <p class="subtitle lead">Lesson 2.4: Variance of the OLS Estimator</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Lesson 2.4: Variance of the OLS Estimator</h2>
   
  <ul class="collapse">
  <li><a href="#gauss-markov-and-homoscedasticity" id="toc-gauss-markov-and-homoscedasticity" class="nav-link active" data-scroll-target="#gauss-markov-and-homoscedasticity">2.4.1: Gauss-Markov and Homoscedasticity</a></li>
  <li><a href="#variance-of-ols-estimates-in-simple-linear-regression" id="toc-variance-of-ols-estimates-in-simple-linear-regression" class="nav-link" data-scroll-target="#variance-of-ols-estimates-in-simple-linear-regression">2.4.2: Variance of OLS Estimates in Simple Linear Regression</a></li>
  <li><a href="#standard-errors-in-simple-linear-regression" id="toc-standard-errors-in-simple-linear-regression" class="nav-link" data-scroll-target="#standard-errors-in-simple-linear-regression">2.4.3: Standard Errors in Simple Linear Regression</a></li>
  <li><a href="#variance-in-multiple-linear-regression" id="toc-variance-in-multiple-linear-regression" class="nav-link" data-scroll-target="#variance-in-multiple-linear-regression">2.4.4: Variance in Multiple Linear Regression</a></li>
  <li><a href="#standard-errors-and-multicollinearity-in-multiple-regression" id="toc-standard-errors-and-multicollinearity-in-multiple-regression" class="nav-link" data-scroll-target="#standard-errors-and-multicollinearity-in-multiple-regression">2.4.5: Standard Errors and Multicollinearity in Multiple Regression</a></li>
  <li><a href="#robust-standard-errors-for-bivariate-regression" id="toc-robust-standard-errors-for-bivariate-regression" class="nav-link" data-scroll-target="#robust-standard-errors-for-bivariate-regression">2.4.6: Robust Standard Errors for Bivariate Regression</a></li>
  <li><a href="#robust-standard-errors-for-multiple-regression" id="toc-robust-standard-errors-for-multiple-regression" class="nav-link" data-scroll-target="#robust-standard-errors-for-multiple-regression">2.4.7: Robust Standard Errors for Multiple Regression</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the last lesson, we discussed the properties of the OLS estimator. In this lesson, we discuss the variance and standard error of the OLS estimator in order to conduct hypothesis testing.</p>
<p>This lesson covers the following topics:</p>
<ul>
<li>How standard errors are calculated under the assumption of homoscedasticity.</li>
<li>How the presence of heteroscedasticity affects our standard errors, and how we can adjust for this with robust standard errors.</li>
</ul>
</div>
</div>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
<section id="gauss-markov-and-homoscedasticity" class="level1">
<h1>2.4.1: Gauss-Markov and Homoscedasticity</h1>
<p>We have proved that OLS is unbiased under 4 conditions in the last lesson, and asymptotically consistent under one weakened condition.</p>
<p>However, if we recall from <a href="https://statsnotes.github.io/metrics/1.html#variance-and-efficiency-of-estimators">2.1.3</a>, unbiasedness is not the only thing we care about in an estimator. We also care about the estimator’s variance.</p>
<p>Luckily, the Gauss-Markov Theorem does not stop at unbiasedness. With an additional condition (homoscedasticity), we can determine that OLS is not only unbiased, but the linear estimator with the least variance (and thus, the most efficient linear unbiased estimator).</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Gauss-Markov Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Gauss-Markov Theorem states that under 5 conditions, the OLS estimator is the Best Linear Unbiased Estimator (BLUE) for <span class="math inline">\beta_1</span>, being the unbiased linear estimator with the best efficiency.</p>
<ul>
<li><strong>SLR.1/MLR.1 (Linearity in Parameters)</strong>: The parameters of the model are linear.</li>
<li><strong>SLR.2/MLR.2 (Random Sampling)</strong>: All observations in our sample are randomly sampled from the population.</li>
<li><strong>SLR.3/MLR.3 (Sample Variation in</strong> <span class="math inline">x</span><strong>/ No Perfect Multicollinearity)</strong>: <span class="math inline">Var(x) ≠ 0</span>, and no perfect correlation between explanatory variables.</li>
<li><strong>SLR.4/MLR.4 (Zero Conditional Mean)</strong>. The error term <span class="math inline">u</span> has an expectation of 0, given any value of the explanatory variables.</li>
<li><strong>SLR.5/MLR.5 (Homoscedasticity)</strong>: The error term has the same variance given any value of <span class="math inline">x</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>But what does this new condition of homoscedasticity mean?</p>
<p>Mathematically, homoscedasticity is defined as:</p>
<p><span class="math display">
Var(u|x_1, \dots, x_k) = \sigma^2 \text{ for all } x
</span></p>
<ul>
<li>Of course, for simple linear regression, there is only one <span class="math inline">x</span>, so the condition is <span class="math inline">Var(u|x) = \sigma^2</span>.</li>
</ul>
<p>First of all, what even is the variance <span class="math inline">Var(u)</span> representing? It is the variance of the “errors” of our error term <span class="math inline">u_i</span>, which is also a random variable. The figure below displays this, with the green lines representing the distribution of <span class="math inline">u_i</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3513031173.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
<p>In the figure above, you can see the variance (spread) of the error term <span class="math inline">u_i</span>’s distribution is consistent, no matter the value of <span class="math inline">x</span>. This means that homoscedasticity is met.</p>
<p>When <span class="math inline">Var(u)</span> is not constant (and changes with the value of <span class="math inline">x</span>), we have <strong>heteroscedasticity</strong>.</p>
<p>The best way to identify if this assumption is met is to look at a plot of the residuals (errors). If the variance in the residuals is constnant, we have homoscedasticity. If not, we have heteroscedasticity.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1713529842.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
<p>In the figure above, we can see on the left chart, the variance of the residuals is clearly smaller when <span class="math inline">x</span> is lower, and the variance of the residuals is larger when <span class="math inline">x</span> is higher. That is a clear violation of the homoscedasticity assumption.</p>
<ul>
<li>NOTE: heteroscedasticity (failure to meet SLR.5 homoscedasticity) does not bias OLS estimates. It only determines if OLS has the lowest variance of unbiased linear estimators.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="variance-of-ols-estimates-in-simple-linear-regression" class="level1">
<h1>2.4.2: Variance of OLS Estimates in Simple Linear Regression</h1>
<p>Assuming Homoscedasticity is met, we know <span class="math inline">Var(u|x) = \sigma^2</span>.</p>
<ul>
<li>When homoscedasticity is not met, nothing in this section applies. See <a href="https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-bivariate-regression">1.7.7</a> for more information on how to do estimates when this is violated.</li>
</ul>
<p>What is variance? From <a href="https://statsnotes.github.io/theory/1.html#variance-and-standard-deviation">1.1.5</a>, we know that the formula for variance is:</p>
<p><span class="math display">
E(x - \mu)^2
</span></p>
<p>We know from SLR.4 Zero-Conditional Mean assumption that <span class="math inline">E(u|x) = 0</span>. Thus, we can use that to calculate the variance <span class="math inline">Var(u|x)</span> using the variance formula:</p>
<p><span class="math display">
\begin{split}
Var(u|x) = \sigma^2 &amp; = E[ \ ((u|x) - E(u|x))^2 \ ] \\
&amp; = E((u|x) - 0)^2 \\
&amp; = E[(u|x)^2] \\
&amp; = E(u^2 |x)
\end{split}
</span></p>
<p>And since by homoscedasticity, we know variance does not depend on <span class="math inline">x</span>, so <span class="math inline">Var(u|x) = Var(u)</span>. Thus, we also know that:</p>
<p><span class="math display">
E(u^2|x) = E(u^2) = \sigma^2
</span></p>
<p><br></p>
<p>Remember when we were proving unbiasedness of OLS in <a href="https://statsnotes.github.io/theory/6.html#proof-of-ols-unbiasedness-in-simple-linear-regression">1.6.3</a>, we got to this stage:</p>
<p><span class="math display">
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^n w_i u_i
</span></p>
<ul>
<li>Where <span class="math inline">w_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}</span>, which is a function of random variable <span class="math inline">x</span>.</li>
<li>We could also write <span class="math inline">w_i</span> as <span class="math inline">\frac{x_i - \bar{x}}{SST_x}</span> (where <span class="math inline">SST_x</span> is total sum of squares for <span class="math inline">x</span>).</li>
</ul>
<p>We know that <span class="math inline">\beta_1</span> is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in <span class="math inline">\hat\beta_1</span>. Thus, <span class="math inline">\sum w_i u_i</span> is the variance in <span class="math inline">\hat\beta_1</span>.</p>
<p><span class="math display">
\begin{split}
Var(\hat\beta_1|x) &amp; = Var\left( \sum\limits_{i=1}^n w_i u_i \bigg| x\right) \\
&amp; = \sum\limits_{i=1}^n Var(w_i u_i | x) \\
&amp; = \sum\limits_{i=1}^n w_i^2 Var(u_i | x)
\end{split}
</span></p>
<p>And given SLR.2 Random Sampling (see <a href="https://statsnotes.github.io/theory/6.html#unbiasedness-of-ols-under-the-gauss-markov-theorem">1.6.2</a>), we know <span class="math inline">Var(u_i | x)</span> is also equal to <span class="math inline">Var(u_i|x_i)</span>. Thus:</p>
<p><span class="math display">
Var(\hat\beta_1|x) = \sum\limits_{i=1}^n w_i^2 Var(u_i | x_i)
</span></p>
<p>And using SLR.5 homoscedasticity, we know <span class="math inline">Var(u|x) = \sigma^2</span> and is constant, thus:</p>
<p><span class="math display">
\begin{split}
Var(\hat\beta_1 | x) &amp; = \sum\limits_{i=1}^n w_i^2 Var(u_i | x_i) \\
&amp; = \sum\limits_{i=1}^n w_i^2 \sigma^2 \\
&amp; = \sigma^2 \sum\limits_{i=1}^n w_i^2
\end{split}
</span></p>
<p>Remember, <span class="math inline">w_i</span> is its own function of <span class="math inline">x</span>, where <span class="math inline">w_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}</span>, or can be written as <span class="math inline">\frac{x_i - \bar{x}}{SST_x}</span> (see <a href="https://statsnotes.github.io/theory/6.html#proof-of-ols-unbiasedness-in-simple-linear-regression">1.6.3</a> for derivation).</p>
<p>We have <span class="math inline">\sum w_i^2</span> in our final equation, and we can do some quick algebra to rearrange it (note, if you are not familiar with SST, see <a href="https://statsnotes.github.io/theory/4.html#r-squared-and-goodness-of-fit">1.4.6</a>):</p>
<p><span class="math display">
\begin{split}
\sum\limits_{i=1}^n w_i^2 &amp; = \sum\limits_{i=1}^n \frac{(x_i - \bar{x})^2}{(SST_x)^2} \\
&amp; = \frac{\sum_{i=1}^n(x_i - \bar x)^2}{(SST_x)^2} \\
&amp; = \frac{SST_x}{(SST_x)^2} \\
&amp; = \frac{1}{SST_x}
\end{split}
</span>Thus, we can plug that in to get our final variance of <span class="math inline">\hat\beta_1</span> formula:</p>
<p><span class="math display">
\begin{split}
Var(\hat\beta|x) &amp; = \sigma^2 \sum\limits_{i=1}^n w_i^2 \\
&amp; = \sigma^2 \frac{1}{SST_x} \\
&amp; = \frac{\sigma^2}{SST_x}
\end{split}
</span></p>
<p>Thus, that is the variance of our OLS estimator <span class="math inline">\hat\beta_1</span>, and also the variance of the sampling distribution of <span class="math inline">\hat\beta_1</span>.</p>
<ul>
<li>This is only the case if SLR.5 homoscedasticity assumption holds. It is not valid if the assumption is not met. See <a href="https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-bivariate-regression">1.7.7</a> for more information on how to do estimates when this is violated.</li>
<li>One might notice we calculated <span class="math inline">Var(\hat\beta_1|x)</span>, not <span class="math inline">Var(\hat\beta_1)</span>. However, this does not matter, since our final formula does not depend on the value of <span class="math inline">x</span> (from homoscedasticity, we know <span class="math inline">\sigma^2</span> is independent of <span class="math inline">x</span>, and also <span class="math inline">SST_x</span> is constant no matter the specific value of <span class="math inline">x</span> as it includes all values of <span class="math inline">x</span>).</li>
</ul>
<p>There is one issue: we know that <span class="math inline">\sigma^2 = E(u^2)</span> (see earlier in the section). However, we do not actually know the value of <span class="math inline">E(u^2)</span>! Remember, that is the error term <span class="math inline">u</span> - while we only know the residual term <span class="math inline">\hat u</span>.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="standard-errors-in-simple-linear-regression" class="level1">
<h1>2.4.3: Standard Errors in Simple Linear Regression</h1>
<p>We know, as shown in the last section, that the variance of the sampling distribution is:</p>
<p><span class="math display">
Var(\hat\beta) = \frac{\sigma^2}{SST_x}
</span></p>
<p>If we want the standard deviation of the sampling distribution, we simply take the square root:</p>
<p><span class="math display">
sd(\hat\beta_1) = \frac{\sigma}{\sqrt{SST_x}}
</span></p>
<p>There is one issue: we know that <span class="math inline">\sigma^2 = E(u^2)</span> (see previous section for proof). However, we do not actually know the value of <span class="math inline">E(u^2)</span>! Remember, that is the error term <span class="math inline">u</span> - while we only know the residual term <span class="math inline">\hat u</span>.</p>
<p>So, what we can do is well, simply replace <span class="math inline">u</span> with its estimate, <span class="math inline">\hat u</span>.</p>
<ul>
<li>Recall that <span class="math inline">u_i = y_i - \beta_0 - \beta_1 x_i</span></li>
<li>And <span class="math inline">\hat u_i = y_i - \hat\beta_0 - \hat\beta_1 x_i</span>.</li>
</ul>
<p>So naturally, instead of <span class="math inline">\sigma^2 = E(u^2)</span>, we could estimate it with <span class="math inline">E(\hat u^2)</span>. Mathematically:</p>
<p><span class="math display">
\begin{split}
\hat\sigma^2 = E(\hat u^2) &amp; = \frac{1}{n} \sum\limits_{i=1}^n \hat u_i^2 \\
&amp; = SSR/n
\end{split}
</span></p>
<ul>
<li>Where <span class="math inline">SSR</span> is the square sum of residuals (see <a href="https://statsnotes.github.io/theory/4.html#r-squared-and-goodness-of-fit">1.4.6</a> for more details).</li>
</ul>
<p>However, there is an issue with this estimate of <span class="math inline">\sigma^2</span>. It is biased - the expected value is actually slightly less than <span class="math inline">\sigma^2</span>.</p>
<p>We will not prove this mathematically, but this bias is because OLS imposes two conditions on its estimation process (that we discussed in <a href="https://statsnotes.github.io/theory/6.html#residuals-and-properties-of-ols-estimator">1.6.1</a>):</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n \hat u_i = 0 \\
&amp; \sum\limits_{i=1}^n x_i \hat u_i = 0
\end{split}
</span></p>
<ul>
<li>The actual error term <span class="math inline">u_i</span> (not the OLS residuals <span class="math inline">\hat u_i</span>) do not have these restrictions.</li>
</ul>
<p>We can adjust the estimator to be more accurate by including a degrees of freedom adjustment. So, instead of <span class="math inline">SSR/n</span>, we can do <span class="math inline">SSR/(n-2)</span>. Thus, our estimator for <span class="math inline">\sigma^2</span> is:</p>
<p><span class="math display">
\hat\sigma^2 = \frac{SSR}{n-2} = \frac{\sum_{i=1}^n \hat u_i^2}{n-2}
</span></p>
<p>With that estimate of <span class="math inline">\sigma^2</span>, we can plug it back into our formula for the standard deviation of <span class="math inline">\hat\beta_1</span>.</p>
<ul>
<li>We call this standard deviation the standard error (as discussed in <a href="https://statsnotes.github.io/theory/2.html#uncertainty-in-estimates-and-sampling-distributions">1.2.3</a>)</li>
</ul>
<p><span class="math display">
\widehat{se}(\hat\beta_1) = \frac{\hat\sigma}{\sqrt{SST_x}}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Standard Errors for Simple Linear Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The standard error for the coefficient <span class="math inline">\hat\beta_1</span> from an OLS estimator in a simple linear regression is:</p>
<p><span class="math display">
\widehat{se}(\hat\beta_1) = \frac{\hat\sigma}{\sqrt{SST_x}}
</span></p>
<p>Where <span class="math inline">\hat\sigma</span> is defined as:</p>
<p><span class="math display">
\hat\sigma = \sqrt{\frac{SSR}{n-2}} = \sqrt{\frac{\sum_{i=1}^n \hat u_i^2}{n-2}}
</span></p>
<p>And where <span class="math inline">SST_x</span> is defined as:</p>
<p><span class="math display">
SST_x = \sum_{i=1}^n(x_i - \bar x)^2
</span></p>
</div>
</div>
<p>When homoscedasticity is not met, this standard error is invalid. See <a href="https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-bivariate-regression">1.7.7</a> for more information on how to do estimates when this is violated.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="variance-in-multiple-linear-regression" class="level1">
<h1>2.4.4: Variance in Multiple Linear Regression</h1>
<p>Now, we will do the same exercise, but for multiple linear regression.</p>
<p>Assuming homoscedasticity is met, we know <span class="math inline">Var(u|x_1, \dots, x_k) = \sigma^2</span>.</p>
<ul>
<li>When homoscedasticity is not met, nothing in this section applies. See <a href="https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-multiple-regression">1.7.8</a> for more information on how to do estimates when this is violated.</li>
</ul>
<p>Let us find the variance of OLS estimates (we will use <span class="math inline">\hat\beta_1</span> for simplicity, but this applies to any other coefficient <span class="math inline">\hat\beta_1 , \dots, \hat\beta_k</span>. In proving unbiasedness of OLS in <a href="https://statsnotes.github.io/theory/6.html#proof-of-ols-unbiasedness-in-multiple-linear-regression-1">1.6.4</a>, we got to this stage:</p>
<p><span class="math display">
\hat\beta_1 = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
</span></p>
<p>We know that <span class="math inline">\beta_1</span> is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in <span class="math inline">\hat\beta_1</span>. Thus, the second part is the variance in <span class="math inline">\hat\beta_1</span>.</p>
<p>Let us define <span class="math inline">w_i</span> as following, as a function of <span class="math inline">x_1, \dots, x_k</span>:</p>
<p><span class="math display">
w_i = \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
</span></p>
<p>This allows us to write <span class="math inline">\hat\beta_1</span> as:</p>
<p><span class="math display">
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^nw_i u_i
</span></p>
<p><br></p>
<p>Thus, we can proceed in the same way as the simple linear regression case (see <a href="https://statsnotes.github.io/theory/7.html#variance-of-ols-estimates-in-simple-linear-regression">1.7.2</a> for more details):</p>
<p><span class="math display">
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) &amp; = Var\left( \sum\limits_{i=1}^nw_i u_i \biggr|x_1, \dots ,x_k \right) \\ &amp; = \sum\limits_{i=1}^n Var(w_i u_i | x_1, \dots, x_k) \\
&amp; = \sum\limits_{i=1}^n w_i^2 \ Var(u_i | x_1, \dots, x_k) \\
&amp; = \sum\limits_{i=1}^n w_i^2 \sigma^2 \\
&amp; = \sigma^2 \sum\limits_{i=1}^n w_i^2
\end{split}
</span></p>
<p>Now, plugging back in <span class="math inline">w_i</span>, we get:</p>
<p><span class="math display">
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) &amp; = \sigma^2 \sum\limits_{i=1}^n w_i^2 \\
&amp; = \sigma^2 \sum_{i=1}^n \left( \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \right) \\
&amp; = \sigma^2 \frac{\sum_{i=1}^n \widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \\
&amp; = \frac{\sigma^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
</span></p>
<p>And assuming homoscedasticity (where <span class="math inline">Var(\hat\beta_1)</span> does not depend on <span class="math inline">x_1, \dots, x_k</span>, we thus know:</p>
<p><span class="math display">
Var(\hat\beta_1) = \frac{\sigma^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
</span></p>
<ul>
<li>When homoscedasticity is not met, this is not valid. See <a href="https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-multiple-regression">1.7.8</a> for more information on how to do estimates when this is violated.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="standard-errors-and-multicollinearity-in-multiple-regression" class="level1">
<h1>2.4.5: Standard Errors and Multicollinearity in Multiple Regression</h1>
<p>However, just like we mentioned in <a href="https://statsnotes.github.io/theory/7.html#standard-errors-in-simple-linear-regression">1.7.3</a>, the real value of <span class="math inline">\sigma^2</span> of a regression not calculable. This is an issue because our variance formula has <span class="math inline">\sigma^2</span> in the numerator.</p>
<p>Just like in simple linear regression (see <a href="https://statsnotes.github.io/theory/7.html#standard-errors-in-simple-linear-regression">1.7.3</a>), we can estimate <span class="math inline">\sigma^2</span> by using the residual term <span class="math inline">\hat u_i</span> with a degrees of freedom adjustment.</p>
<p><span class="math display">
\hat\sigma^2 = \frac{SSR}{n-k-1} = \frac{\sum_{i=1}^n \hat u_i^2}{n-k-1}
</span></p>
<ul>
<li>Where <span class="math inline">n</span> is the number of observations in our sample data.</li>
<li>Where <span class="math inline">k</span> is the number of explanatory variables in our model.</li>
</ul>
<p>Thus, with this estimate, we can calculate the <strong>standard errors</strong> (square root of variance) of our estimates of coefficients <span class="math inline">\hat\beta_j</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Standard Errors for Multiple Linear Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The standard error for the coefficient <span class="math inline">\hat\beta_j</span> from an OLS estimator in multiple linear regression is:</p>
<p><span class="math display">
\widehat{se}(\hat\beta_j) = \frac{\hat\sigma}{\sqrt{\sum_{i=1}^n \widetilde{r_{1i}}^2}}
</span></p>
<ul>
<li>We will never calculate this by hand, we will use a statistical software to do this.</li>
</ul>
<p>Where <span class="math inline">\hat\sigma</span> is defined as:</p>
<p><span class="math display">
\hat\sigma = \sqrt{\frac{SSR}{n-k-1}} = \sqrt{\frac{\sum_{i=1}^n \hat u_i^2}{n-k-1}}
</span></p>
</div>
</div>
<p>When homoscedasticity is not met, this is not valid. See <a href="https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-multiple-regression">1.7.8</a> for more information on how to do estimates when this is violated.</p>
<p><br></p>
<section id="multicollinearity" class="level3">
<h3 class="anchored" data-anchor-id="multicollinearity">Multicollinearity</h3>
<p>Notice how the denominator contains <span class="math inline">\sum\widetilde{r_{1i}}^2</span>. That means, the smaller <span class="math inline">\sum\widetilde{r_{1i}}^2</span> is, the larger the variance is.</p>
<ul>
<li><span class="math inline">\sum\widetilde{r_{1i}}^2</span> is smaller when our explanatory variable of interest <span class="math inline">x_1</span> is highly correlated with another explanatory variable, since <span class="math inline">\widetilde{r_{1i}}^2</span> represents the part of <span class="math inline">x_1</span> that is uncorrelated with other explanatory variables.</li>
<li>This means that if we have highly correlated explanatory variables, our variance will in our OLS estimates will be high.</li>
</ul>
<p>This is something important to think about when choosing explanatory variables. We want to include all confounding variables, but highly correlated confounders will make our variance very high for our estimates.</p>
<ul>
<li>One way of dealing with this is dimensional reduction techniques, which we will discuss in Part IV of the guide dealing with multivariate analysis.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="robust-standard-errors-for-bivariate-regression" class="level1">
<h1>2.4.6: Robust Standard Errors for Bivariate Regression</h1>
<p>MLR.5 Homoscedasticity states that <span class="math inline">Var(u|x_1, \dots x_k) = \sigma^2</span>. Or in other words, no matter the value of <span class="math inline">x_1, \dots, x_k</span>, the variance of the error term is constant at <span class="math inline">\sigma^2</span>.</p>
<p>When MLR.5 Homoscedasticity is violated, the OLS estimator is still unbiased, however, it is no longer the unbiased linear estimator with the least variance.</p>
<p>More importantly for us however, is that heteroscedasticity invalidates the variance and standard error formulas we have calculated earlier.</p>
<p><br></p>
<p>Let us calculate these standard errors under heteroscedasticity. For simplicity, consider the bivariate regression:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_i + u_i
</span></p>
<p>However, without MLR.5 Homoscedasticity, variance <span class="math inline">Var(u|x)</span> is no longer constant at <span class="math inline">\sigma^2</span>. Instead, <span class="math inline">Var(u|x)</span> varies depending on the value of <span class="math inline">x</span>. Let us define the variance as:</p>
<p><span class="math display">
Var(u_i|x_i) = \sigma_i^2
</span></p>
<ul>
<li>Where <span class="math inline">\sigma^2_1</span> is the error term variance for <span class="math inline">x_1</span>, and <span class="math inline">\sigma_2^2</span> is the error term variance for <span class="math inline">x_2</span>, and so on <span class="math inline">\sigma_3^2, \dots, \sigma_n^2</span>.</li>
</ul>
<p>Then, recalling from the start of <a href="https://statsnotes.github.io/theory/7.html#variance-of-ols-estimates-in-simple-linear-regression">1.7.2</a>, we have the following formula for the simple linear regression <span class="math inline">\hat\beta_1</span> estimate:</p>
<p><span class="math display">
\hat\beta_1 = \beta_1 + \sum_{i=1}^nw_i u_i
</span></p>
<ul>
<li>Where <span class="math inline">w_i = \frac{x_i - \bar x}{\sum(x_i - \bar x)^2}</span>, which is a function of random variable <span class="math inline">x</span>.</li>
<li>We could also write <span class="math inline">w_i</span> as <span class="math inline">\frac{x_i - \bar x}{SST_x}</span>.</li>
</ul>
<p>We know that <span class="math inline">\beta_1</span> is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in <span class="math inline">\hat\beta_1</span>. Thus, the second part is the variance in <span class="math inline">\hat\beta_1</span>. Thus:</p>
<p><span class="math display">
\begin{split}
Var(\hat\beta_1|x) &amp; = Var \left( \sum\limits_{i=1}^n w_i u_i \biggr| x \right) \\
&amp; = \sum\limits_{i=1}^n Var(w_i u_i |x) \\
&amp; = \sum\limits_{i=1}^n w_i^2 \ Var (u_i | x) \\
&amp; = \sum\limits_{i=1}^nw_i^2 \ Var(u_i|x_i) \\
&amp; = \sum\limits_{i=1}^n w_i^2 \sigma_i^2
\end{split}
</span></p>
<p>Now, plugging back <span class="math inline">w_i</span> in, we get:</p>
<p><span class="math display">
\begin{split}
Var(\hat\beta_1|x) &amp; = \sum\limits_{i=1}^n w_i^2 \sigma_i^2 \\
&amp; = \sum\limits_{i=1}^n \left(\frac{x_i - \bar x}{SST_x} \right)^2 \sigma^2_i \\
&amp; = \frac{\sum_{i=1}^n (x_i - \bar x)^2 \sigma^2_i}{SST_x^2}
\end{split}
</span></p>
<p>Of course, just like with homoscedasticity, we do not know <span class="math inline">\sigma_i^2</span>, and have to estimate it with <span class="math inline">\hat u</span>. We do not need a degrees of freedom adjustment in this case. Thus, our estimate of variance is:</p>
<p><span class="math display">
\widehat{Var}(\hat\beta_1|x) = \frac{\sum_{i=1}^n (x_i - \bar x)^2 \hat u^2_i}{SST_x^2}
</span></p>
<p>And thus, the standard error (square root of variance) of our estimate <span class="math inline">\hat\beta_1</span> under homoscedasticity is the square root of our estimate of variance.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Robust Standard Error for Bivariate Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The heteroscedasticity-robust standard error for <span class="math inline">\hat\beta_1</span> in bivariate regression is:</p>
<p><span class="math display">
\widehat{se}(\hat\beta_1|x) = \sqrt{\frac{\sum_{i=1}^n (x_i - \bar x)^2 \hat u^2_i}{SST_x^2}}
</span></p>
<ul>
<li>Where SST is the total sum of squares <span class="math inline">SST = \sum(y_i - \bar y)^2</span>.</li>
</ul>
</div>
</div>
<p>We can use these robust standard errors to conduct hypothesis testing and confidence intervals as normal.</p>
<p>Nowadays, it is typical to use heteroscedasticity-robust standard errors as default, and only using normal standard errors if we can prove homoscedasticity is met.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="robust-standard-errors-for-multiple-regression" class="level1">
<h1>2.4.7: Robust Standard Errors for Multiple Regression</h1>
<p>Let us do the same as the last section, but for multiple regression.</p>
<p>Without MLR.5 Homoscedasticity, variance <span class="math inline">Var(u|x)</span> is no longer constant at <span class="math inline">\sigma^2</span>. Instead, <span class="math inline">Var(u|x)</span> varies depending on the value of <span class="math inline">x</span>. Let us define the variance as:</p>
<p><span class="math display">
Var(u_i|x_{1i}, \dots, x_{ki}) = \sigma_i^2
</span></p>
<p>Let us find the variance of OLS estimates (we will use <span class="math inline">\hat\beta_1</span> for simplicity, but this applies to any other coefficient <span class="math inline">\hat\beta_1 , \dots, \hat\beta_k</span>. In proving unbiasedness of OLS in <a href="https://statsnotes.github.io/theory/6.html#proof-of-ols-unbiasedness-in-multiple-linear-regression-1">1.6.4</a>, we got to this stage:</p>
<p><span class="math display">
\hat\beta_1 = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
</span></p>
<p>We know that <span class="math inline">\beta_1</span> is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in <span class="math inline">\hat\beta_1</span>. Thus, the second part is the variance in <span class="math inline">\hat\beta_1</span>.</p>
<p>Let us define <span class="math inline">w_i</span> as following, as a function of <span class="math inline">x_1, \dots, x_k</span>:</p>
<p><span class="math display">
w_i = \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
</span></p>
<p>This allows us to write <span class="math inline">\hat\beta_1</span> as:</p>
<p><span class="math display">
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^nw_i u_i
</span></p>
<p><br></p>
<p>Thus, we can proceed in the same way as the simple linear regression case in the last section.</p>
<p><span class="math display">
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) &amp; = Var\left( \sum\limits_{i=1}^nw_i u_i \biggr|x_1, \dots ,x_k \right) \\ &amp; = \sum\limits_{i=1}^n Var(w_i u_i | x_1, \dots, x_k) \\
&amp; = \sum\limits_{i=1}^n w_i^2 \ Var(u_i | x_1, \dots, x_k) \\
&amp; = \sum\limits_{i=1}^n w_i^2 \sigma^2_i
\end{split}
</span></p>
<p>Now, plugging back in <span class="math inline">w_i</span>, we get:</p>
<p><span class="math display">
\begin{split}
Var(\hat\beta_1|x) &amp; = \sum\limits_{i=1}^n w_i^2 \sigma_i^2 \\
&amp; = \sum\limits_{i=1}^n \left(\frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \right)^2 \sigma^2_i \\
&amp; = \frac{\sum_{i=1}^n \widetilde{r_{1i}}^2 \sigma^2_i}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
\end{split}
</span></p>
<p>Of course, just like before, we do not know <span class="math inline">\sigma_i^2</span>, and have to estimate it with <span class="math inline">\hat u</span>. Thus, our estimate of variance is:</p>
<p><span class="math display">
\widehat{Var}(\hat\beta_1) = \frac{\sum_{i=1}^n \widetilde{r_{1i}}^2 \hat u_i^2}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
</span></p>
<p>And thus, the standard error (square root of variance) of our estimate <span class="math inline">\hat\beta_1</span> under homoscedasticity is the square root of our estimate of variance.</p>
<ul>
<li>This can be generalised to any other coefficient <span class="math inline">\hat\beta_1, \dots, \hat\beta_k</span>.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Robust Standard Error for Multiple Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The heteroscedasticity-robust standard error for <span class="math inline">\hat\beta_1</span> in multiple regression is:</p>
<p><span class="math display">
\widehat{se}(\hat\beta_j) = \sqrt{\frac{\sum_{i=1}^n \widetilde{r_{ji}}^2 \hat u^2_i}{\sum_{i=1}^n\widetilde{r_{ji}}^2}}
</span></p>
</div>
</div>
<p>We can use these robust standard errors to conduct hypothesis testing and confidence intervals as normal.</p>
<p>Nowadays, it is typical to use heteroscedasticity-robust standard errors as default, and only using normal standard errors if we can prove homoscedasticity is met.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>