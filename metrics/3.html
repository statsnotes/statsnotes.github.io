<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistics for Social Scientists</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="3_files/libs/clipboard/clipboard.min.js"></script>
<script src="3_files/libs/quarto-html/quarto.js"></script>
<script src="3_files/libs/quarto-html/popper.min.js"></script>
<script src="3_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="3_files/libs/quarto-html/anchor.min.js"></script>
<link href="3_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="3_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="3_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="3_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="3_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Statistics for Social Scientists</h1>
            <p class="subtitle lead">Lesson 2.3: Properties of the OLS Estimator</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Lesson 2.3: Properties of the OLS Estimator</h2>
   
  <ul class="collapse">
  <li><a href="#residuals-and-properties-of-ols-estimator" id="toc-residuals-and-properties-of-ols-estimator" class="nav-link active" data-scroll-target="#residuals-and-properties-of-ols-estimator">2.3.1: Residuals and Properties of OLS Estimator</a></li>
  <li><a href="#unbiasedness-of-ols-under-the-gauss-markov-theorem" id="toc-unbiasedness-of-ols-under-the-gauss-markov-theorem" class="nav-link" data-scroll-target="#unbiasedness-of-ols-under-the-gauss-markov-theorem">2.3.2: Unbiasedness of OLS Under the Gauss-Markov Theorem</a></li>
  <li><a href="#proof-of-ols-unbiasedness-in-simple-linear-regression" id="toc-proof-of-ols-unbiasedness-in-simple-linear-regression" class="nav-link" data-scroll-target="#proof-of-ols-unbiasedness-in-simple-linear-regression">2.3.3: Proof of OLS Unbiasedness in Simple Linear Regression</a></li>
  <li><a href="#proof-of-ols-unbiasedness-in-multiple-linear-regression" id="toc-proof-of-ols-unbiasedness-in-multiple-linear-regression" class="nav-link" data-scroll-target="#proof-of-ols-unbiasedness-in-multiple-linear-regression">2.3.4: Proof of OLS Unbiasedness in Multiple Linear Regression</a></li>
  <li><a href="#asymptotic-consistency-of-ols" id="toc-asymptotic-consistency-of-ols" class="nav-link" data-scroll-target="#asymptotic-consistency-of-ols">2.3.5: Asymptotic Consistency of OLS</a></li>
  <li><a href="#exogeneity-and-endogeneity" id="toc-exogeneity-and-endogeneity" class="nav-link" data-scroll-target="#exogeneity-and-endogeneity">2.3.6: Exogeneity and Endogeneity</a></li>
  <li><a href="#ols-and-the-conditional-expectation-function" id="toc-ols-and-the-conditional-expectation-function" class="nav-link" data-scroll-target="#ols-and-the-conditional-expectation-function">2.3.7: OLS and the Conditional Expectation Function</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the last lesson, we discussed the OLS estimator for linear regressions. But we have to ask ourselves - is OLS a good estimator? Why should we use OLS? In this lesson, we focus on the properties of OLS, and the conditions under which OLS is a good estimator.</p>
<p>This lesson covers the following topics:</p>
<ul>
<li>Some simple algebraic properties of our best-fit plane through OLS estimation.</li>
<li>The conditions on which OLS estimation is an unbiased estimator for multiple regression (Gauss-Markov conditions), and how violations of these conditions will lead to biased estimates.</li>
<li>How OLS is asymptotically consistent with a large sample size, and how that allows us to weaken one of the unbiasedness conditions.</li>
<li>How OLS is also the best approximation of the conditional expectation function.</li>
</ul>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
<section id="residuals-and-properties-of-ols-estimator" class="level1">
<h1>2.3.1: Residuals and Properties of OLS Estimator</h1>
<p>Once we have our OLS estimates <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span> in simple linear regression, we can obtain both our fitted values, and the OLS residuals <span class="math inline">\hat u_i</span>:</p>
<p><span class="math display">
\begin{split}
\hat u_i &amp; = y_i - \hat y_i \\
&amp; = y_i - \hat\beta_0 - \hat\beta_1 x_i
\end{split}
</span></p>
<p>The figure below shows what a residual for a point looks like: the distance between the best fit line.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3479749458.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>The same applies to the residuals of multiple regression, with the addition of more parameters:</p>
<p><span class="math display">
\begin{split}\hat u_i &amp; = y_i - \hat y_i \\&amp; = y_i - (\hat \beta_0 + \hat\beta_1x_{1i} + \dots + \hat\beta_k x_{ki}) \\&amp; = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}\end{split}
</span></p>
<p><br></p>
<p>Recall that the OLS estimates <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span> are chosen to satisfy the following first order conditions for simple linear regression (see <a href="https://statsnotes.github.io/metrics/2.html#estimation-for-simple-linear-regression">2.2.2</a>):</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
</span></p>
<p>For multiple regression OLS estimates of <span class="math inline">\hat\beta_0 , \dots \hat\beta_k</span> are chosen to satisfy the following first order conditions (see <a href="https://statsnotes.github.io/metrics/2.html#estimation-for-multiple-linear-regression">2.2.4</a>):</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\&amp; \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
&amp; \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>Using these properties, we can plug in <span class="math inline">\hat u_i</span> from above to get the above conditions:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n \hat u_i = 0 \\
&amp; \sum\limits_{i=1}^n x_{ji} \hat u_i = 0
\end{split}
</span></p>
<ul>
<li>Where <span class="math inline">x_j</span> is just <span class="math inline">x</span> in simple linear regression, and <span class="math inline">x_j</span> is any <span class="math inline">x_1, \dots, x_k</span> in multiple linear regression</li>
</ul>
<p>We can use these conditions to obtain a few properties about the OLS best-fit line.</p>
<p><br></p>
<section id="property-1-sum-of-residuals-is-0" class="level3">
<h3 class="anchored" data-anchor-id="property-1-sum-of-residuals-is-0">Property 1: Sum of Residuals is 0</h3>
<p>OLS residuals always add up to zero (for both simple and multiple regression), since:</p>
<p><span class="math display">
\sum\limits_{i=1}^n \hat u_i = 0
</span></p>
<p>This property ensures that the average value of <span class="math inline">y_i</span> in our data is the same as the average value of our predictions <span class="math inline">\hat y_i</span>.</p>
<p><br></p>
</section>
<section id="property-2-no-covariance-between-x-and-residual." class="level3">
<h3 class="anchored" data-anchor-id="property-2-no-covariance-between-x-and-residual.">Property 2: No Covariance Between <span class="math inline">x</span> and Residual.</h3>
<p>From above, we know the following to be true:</p>
<p><span class="math display">
\sum\limits_{i=1}^n \hat u_i = 0
</span></p>
<p>Which also means that <span class="math inline">\bar{\hat u} = 0</span> (since an average is just the sum divided by the number of observations, and the sum is equal to 0).</p>
<p>We also know that:</p>
<p><span class="math display">
\sum\limits_{i=1}^n x_{ji} \hat u_i = 0
</span></p>
<ul>
<li>Where <span class="math inline">x_j = x</span> for simple linear regression, and <span class="math inline">x_j = x_1, \dots, x_k</span> for multiple linear regression.</li>
</ul>
<p>Now, recall the formula for covariance discussed in <a href="https://statsnotes.github.io/theory/3.html#quantifying-relationships-with-covariance">1.3.4</a>:</p>
<p><span class="math display">
Cov(x,y) = \frac{1}{n}\sum\limits_{i=1}^n [(x_i - \bar x)(y_i - \bar y)] \\
</span></p>
<p>Thus, the covariance between <span class="math inline">x_j</span> (for notation simplicity, just <span class="math inline">x</span>) and <span class="math inline">\hat u</span> is:</p>
<p><span class="math display">
\begin{split}
Cov(x, \hat u) &amp; = \frac{1}{n}\sum\limits_{i=1}^n [(x_i - \bar x)(\hat u_i - \bar{\hat u})] \\
&amp; = \frac{1}{n}\sum\limits_{i=1}^n(x_i \hat u_i - x_i \bar{\hat u} - \bar x \hat u_i + \bar x \bar {\hat u}) \\
&amp; = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - \sum\limits_{i=1}^n x_i\bar{\hat u} - \sum\limits_{i=1}^n \bar x \hat u_i + \sum\limits_{i=1}^n\bar x \bar{\hat u} \right) \\
&amp; = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - \bar{\hat u}\sum\limits_{i=1}^n x_i - \bar x\sum\limits_{i=1}^n \hat u_i + \bar{\hat u}\sum\limits_{i=1}^n\bar x  \right) \\
&amp; = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - 0\sum\limits_{i=1}^n x_i - \bar x\sum\limits_{i=1}^n \hat u_i + 0\sum\limits_{i=1}^n\bar x  \right) \\
&amp; = \frac{1}{n}(0 -0-\bar x(0) + 0) \\
&amp; = 0
\end{split}
</span></p>
<p>Thus, the covariance (and thus correlation) between <span class="math inline">x_j</span> and <span class="math inline">\hat u</span> must be zero.</p>
<p><br></p>
</section>
<section id="property-3-regression-line-passes-through-means" class="level3">
<h3 class="anchored" data-anchor-id="property-3-regression-line-passes-through-means">Property 3: Regression Line Passes Through Means</h3>
<p>Remember our solution for <span class="math inline">\hat\beta_0</span> in OLS for simple linear regression was <span class="math inline">\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}</span>. Rearranging this equation, we get:</p>
<p><span class="math display">
\begin{split}
\hat\beta_0  &amp; = \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat\beta_0 + \hat\beta_1 \bar x &amp; = \bar y \\
\bar y &amp; = \hat\beta_0 + \hat\beta_1 \bar x
\end{split}
</span></p>
<p>Thus, the OLS estimated best-fit line always passes though point <span class="math inline">(\bar x, \bar y)</span> (the means of our data). The same property applies to multiple linear regression, but for point <span class="math inline">(\bar x_1, \dots, \bar x_k, \bar y)</span>.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="unbiasedness-of-ols-under-the-gauss-markov-theorem" class="level1">
<h1>2.3.2: Unbiasedness of OLS Under the Gauss-Markov Theorem</h1>
<p>An unbiased estimator, if we recall from <a href="https://statsnotes.github.io/metrics/1.html#unbiasedness-of-estimators">2.1.2</a>, means that over many different estimates, the expected value of all the estimates is the true parameter value: <span class="math inline">E(\hat{\theta}_n) = \theta</span>.</p>
<p>Unbiasedness is desirable property of estimators. The Gauss-Markov Theorem proves that OLS is unbiased when 4 conditions are met.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Gauss-Markov Theorem for Simple Linear Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Gauss-Markov Theorem states that under 4 conditions, the OLS estimator for simple linear regression is an unbiased estimator for <span class="math inline">\beta_1</span>.</p>
<ul>
<li><strong>SLR.1 (Linearity in Parameters)</strong>: The parameters of the model are linear.</li>
<li><strong>SLR.2 (Random Sampling)</strong>: All observations in our sample are randomly sampled from the population.</li>
<li><strong>SLR.3 (Sample Variation in</strong> <span class="math inline">x</span><strong>)</strong>: <span class="math inline">Var(x) ≠ 0</span>.</li>
<li><strong>SLR.4 (Zero Conditional Mean)</strong>. <span class="math inline">E(u|x) = 0</span>.</li>
</ul>
</div>
</div>
<p>For multiple linear regression, the conditions are similar but slightly different (specifically, assumption 3 and 4 are slightly different).</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Gauss-Markov Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Gauss-Markov Theorem states that under 4 conditions, the OLS estimator for multiple linear regression is an unbiased estimator for <span class="math inline">\beta_j</span>.</p>
<ul>
<li><strong>MLR.1 (Linearity in Parameters)</strong>: The parameters of the model are linear.</li>
<li><strong>MLR.2 (Random Sampling)</strong>: All observations in our sample are randomly sampled from the population.</li>
<li><strong>MLR.3 (No Perfect Mutlicollinearity)</strong>: There are no exact linear relationships among variables (where correlation coefficient equals 1 or -1).</li>
<li><strong>MLR.4 (Zero Conditional Mean)</strong>. <span class="math inline">E(u|x_1, \dots, x_k) = 0</span>.</li>
</ul>
</div>
</div>
<p>This section will be focused on the conditions. The next sections will prove the unbiasedness of OLS under these conditions.</p>
<p><br></p>
<section id="assumption-slr.1-and-mlr.1-linearity" class="level3">
<h3 class="anchored" data-anchor-id="assumption-slr.1-and-mlr.1-linearity">Assumption SLR.1 and MLR.1: Linearity</h3>
<p>Assumption SLR.1 and MLR.1 state that a model must be <strong>linear in parameters</strong>.</p>
<p>This means that the parameters of the model <span class="math inline">\beta_0, \dots, \beta_k</span> must not be multiplied together - they must be added together.</p>
<p>Note: This does not mean that the actual regression line must be linear - only the parameters/coefficients must not be multiplied, the variables can be. For example, the following model is still linear in parameters:</p>
<p><span class="math display">
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2^2 + \beta_3 x_1 x_2 + u
</span></p>
<p><br></p>
<p><br></p>
</section>
<section id="assumption-slr.2-and-mlr.2-random-sampling" class="level3">
<h3 class="anchored" data-anchor-id="assumption-slr.2-and-mlr.2-random-sampling">Assumption SLR.2 and MLR.2: Random Sampling</h3>
<p>This assumption says that all observations in our sample are randomly sampled from the same population.</p>
<p>The error term <span class="math inline">u</span> is some random variable (with its own probability distribution), that can be defined by its expectation <span class="math inline">E(u)</span>.</p>
<ul>
<li>If we randomly select one observation <span class="math inline">i</span> from the data, each observation <span class="math inline">i</span> has an equal chance of being selected.</li>
<li>The error term for that observation, <span class="math inline">u_i</span> should also have the same expectation as the random variable <span class="math inline">u</span>, since each observation <span class="math inline">i</span> within <span class="math inline">u</span> has the same chance of being selected.</li>
</ul>
<p>Thus, random sampling allows us to say <span class="math inline">E(u) = E(u_i)</span>.</p>
<p><br></p>
</section>
<section id="assumption-slr.3-sample-variation-in-x" class="level3">
<h3 class="anchored" data-anchor-id="assumption-slr.3-sample-variation-in-x">Assumption SLR.3: Sample Variation in <span class="math inline">x</span></h3>
<p>This assumption says that for our explanatory variable, <span class="math inline">Var(x) ≠ 0</span>.</p>
<p>Basically, not all <span class="math inline">x</span> values can be the same exact number, there must be at least some variation.</p>
<p><br></p>
</section>
<section id="assumption-mlr.3-no-perfect-multicollinearity" class="level3">
<h3 class="anchored" data-anchor-id="assumption-mlr.3-no-perfect-multicollinearity">Assumption MLR.3: No Perfect Multicollinearity</h3>
<p>This assumption says that <span class="math inline">\sum \widetilde{r_{ij}}^2 = 0</span>.</p>
<ul>
<li>If <span class="math inline">\sum\widetilde{r_{ij}}^2 = 0</span>, that means there is no part of <span class="math inline">x_j</span> that is completely uncorrelated with any other explanatory variable. In other words, that means <span class="math inline">x_j</span> is <strong>perfectly correlated</strong> to at least one other explanatory variable, or a linear function of multiple other explanatory variables.</li>
</ul>
<p>But practically, what does that mean when choosing our explanatory variables?</p>
<ul>
<li>It means that we cannot choose measurements of the same concept in different units.</li>
<li>For example, you cannot include the two variables <em>height in inches</em> and <em>height in cm</em> in the same regression. This is because both variables are perfectly correlated, just differing only by a common factor.</li>
</ul>
<p><br></p>
</section>
<section id="assumption-slr.4-and-mlr.4-zero-conditional-mean" class="level3">
<h3 class="anchored" data-anchor-id="assumption-slr.4-and-mlr.4-zero-conditional-mean">Assumption SLR.4 and MLR.4: Zero-Conditional Mean</h3>
<p>In the population, the error term <span class="math inline">u</span> must have an expectation of 0, given all values of <span class="math inline">x</span>. Mathematically:</p>
<p><span class="math display">
E(u|x_1, \dots x_k) = 0 \text{ for all } (x_1, \dots , x_k)
</span></p>
<ul>
<li>For simple linear regression, it is just <span class="math inline">E(u|x)</span> (without the extra <span class="math inline">x</span> variables).</li>
</ul>
<p>And using condition MLR.2 Random Sampling (from above), we also know that:</p>
<p><span class="math display">
E(u|x_1, \dots ,x_k) = E(u_i|x_{1i}, \dots, x_{ki}) = 0
</span></p>
<ul>
<li>In simple linear regression, <span class="math inline">E(u_i|x_i) = 0</span>.</li>
</ul>
<p>This is the <u>key assumption</u> of OLS, since it is the one that is most frequently violated. We will discuss this in more detail in section <a href="https://statsnotes.github.io/metrics/3.html#exogeneity-and-endogeneity">2.3.6</a>.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="proof-of-ols-unbiasedness-in-simple-linear-regression" class="level1">
<h1>2.3.3: Proof of OLS Unbiasedness in Simple Linear Regression</h1>
<p>In the last section, we covered the assumptions for OLS being unbiased. In this section, we will use those assumptions to prove the unbiasedness of OLS in simple linear regression. Or in other words, we want to show <span class="math inline">E(\hat\beta_1) = \beta_1</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><strong>Property 1:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
</span></p>
<ul>
<li>This is because we can expand the left to <span class="math inline">\sum x_i - \sum \bar x</span>.</li>
<li>Then, we know <span class="math inline">\sum x_i = \sum \bar x</span> (by the formula for mean), so <span class="math inline">\sum x_i - \sum \bar x = 0</span>.</li>
</ul>
<p><strong>Property 2:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y})
</span></p>
<ul>
<li>This is because on the right side can expand to <span class="math inline">\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]</span>.</li>
<li>Then, split into <span class="math inline">\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)</span>.</li>
<li>We know that by property 1 (which applies to any variable), <span class="math inline">\sum (y_i - \bar y) = 0</span>. Thus, the right side disappears, and we are left with <span class="math inline">\sum x_i (y_i - \bar y)</span>.</li>
</ul>
<p><strong>Property 3:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
</span></p>
<ul>
<li>Start by expanding right side to <span class="math inline">\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]</span></li>
<li>Which splits into <span class="math inline">\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)</span></li>
<li>By the first property, we know <span class="math inline">\sum x_i - \bar x = 0</span>, so we are only left with <span class="math inline">\sum x_i (x_i - \bar x)</span></li>
</ul>
</div>
</div>
<p><br></p>
<p>We want to show <span class="math inline">E(\hat\beta_1) = \beta_1</span>. Let us start off with the OLS estimator (which we derived in <a href="https://statsnotes.github.io/metrics/2.html#estimation-for-simple-linear-regression">2.2.2</a>):</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SLR.3 Variance in <span class="math inline">x</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>The existence of <span class="math inline">\hat{\beta}_1</span> is guaranteed by SLR.3 <span class="math inline">Var(x) ≠ 0</span>, since we cannot divide by 0.</p>
</div>
</div>
<p><br></p>
<p>Let us look at the numerator. Let us expand the numerator:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
= &amp; \sum\limits_{i=1}^n [(x_i - \bar x)y_i - (x_i - \bar x) \bar y] \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \sum\limits_{i=1}^n (x_i - \bar x) \bar y \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
\end{split}
</span></p>
<p>We know that <span class="math inline">\sum (x_i - \bar x) = 0</span> (from the properties above). Thus, we can further simplify to:</p>
<p><span class="math display">
\begin{split}
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y(0) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i
\end{split}
</span></p>
<p>Now, let us play with the numerator more (note the properties of summation introduced earlier):</p>
<p><span class="math display">
\begin{split}
&amp; = \sum\limits_{i=1}^n (x_i - \bar{x})y_i \\
&amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(\beta_0 + \beta_1 x_i + u_i) \\
&amp; = \sum\limits_{i=1}^n(x_i - \bar{x}) \beta_0 + \sum\limits_{i=1}^n(x_i - \bar{x}) \beta_1  x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\
&amp; = \beta_0 \sum\limits_{i=1}^n(x_i - \bar{x}) + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x}) x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\
&amp; = 0 + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i\end{split}
</span></p>
<p>Now, putting the numerator back into the equation, we simplify:</p>
<p><span class="math display">
\begin{split}
\hat{\beta}_1 &amp; = \frac{\beta_1 \sum_{i=1}^n(x_i - \bar{x})^2 + \sum_{i=1}^n(x_i - \bar{x})u_i}{\sum_{i=1}^n(x_i - \bar{x})^2} \\
&amp;  =  \beta_1 + \frac{\sum_{i=1}^n(x_i - \bar{x}) u_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&amp;  = \beta_1 + \sum\limits_{i=1}^n w_i u_i
\end{split}
</span></p>
<ul>
<li>Where <span class="math inline">w_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}</span>, which is a function of random variable <span class="math inline">x</span>. We could also write <span class="math inline">w_i</span> as <span class="math inline">\frac{x_i - \bar{x}}{SST_x}</span> (where <span class="math inline">SST_x</span> is total sum of squares for <span class="math inline">x</span>).</li>
</ul>
<p>Since <span class="math inline">w_i</span> is a function of <span class="math inline">x</span>, that means <span class="math inline">\hat\beta_1</span> is also a function of <span class="math inline">x</span> (depends on the value of <span class="math inline">x</span>).</p>
<p><br></p>
<p>Now we need to find the expectation <span class="math inline">E(\hat\beta_1)</span>. Thus, we have this equation:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1|x) &amp; = E \left( \beta_1 + \sum\limits_{i=1}^n w_i u_i \bigg| x \right) \\
&amp; = \beta_1 + \sum\limits_{i=1}^nE(w_iu_i|x)
\end{split}
</span></p>
<p>But what does <span class="math inline">\sum E(w_iu_i |x)</span> equal? This is where our other two Gauss-Markov Conditions come into play.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SLR.4 Zero Conditional Mean
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Zero-Conditional Mean assumption says <span class="math inline">E(u|x) = 0</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SLR.2 Random Sampling
</div>
</div>
<div class="callout-body-container callout-body">
<p>Random sampling, combined with zero-conditional mean, allows us to say:</p>
<p><span class="math display">
E(u|x) = E(u_i | x_i) = E(u_i|x) = 0
</span></p>
</div>
</div>
<p>This means that:</p>
<p><span class="math display">
E(w_i u_i|x) = w_i E(u_i|x) = 0
</span></p>
<p><br></p>
<p>Now knowing what <span class="math inline">E(w_iu_i|x)</span> is, let us plug it back into our equation:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1|x) &amp; = \beta_1 + \sum\limits_{i=1}^nE(w_iu_i|x) \\
&amp; = \beta_1 + \sum\limits_{i=1}^n0 \\
&amp; = \beta_1
\end{split}
</span></p>
<p>However, we have solved for <span class="math inline">E(\hat\beta_1 |x)</span>, and not <span class="math inline">E (\hat\beta_1)</span>. This is where the <strong>Law of Iterated Expectation</strong>. The law says the following:</p>
<p><span class="math display">
E(x) = E[E(x|y)]
</span></p>
<p>Thus, we can use this to conclude the proof:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1) &amp; = E[E(\hat\beta_1|x)] \\
&amp; = E(\beta_1) \\
&amp; = \beta_1
\end{split}
</span></p>
<ul>
<li>Since <span class="math inline">\beta_1</span> is the true value (a constant), its expectation is itself</li>
</ul>
<p>Thus, <span class="math inline">E(\hat\beta_1) = \beta_1</span>, proving the unbiasedness of OLS under the Gauss-Markov conditions.</p>
<ul>
<li>The most critical assumption, and the one most frequently violated, is SLR.4 Zero-Conditional Mean. We will discuss this assumption more as the lesson progresses.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="proof-of-ols-unbiasedness-in-multiple-linear-regression" class="level1">
<h1>2.3.4: Proof of OLS Unbiasedness in Multiple Linear Regression</h1>
<p>In the last section, we covered the proof of OLS unbiasedness for simple linear regression. Here, we cover the proof for multiple linear regression, which is similar, but has some differences.</p>
<p>For simplicity, let us focus on <span class="math inline">\hat\beta_1</span>. However, this can be generalised to any <span class="math inline">\hat\beta_2, \dots, \hat\beta_k</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summations
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><strong>Property 1:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n \widetilde{r_{1i}} = 0
</span></p>
<ul>
<li>This is because <span class="math inline">\widetilde{r_{1i}}</span> is a residual term of a OLS regression of outcome <span class="math inline">x_1</span> and explanatory variables <span class="math inline">x_2, \dots, x_k</span>, and we know OLS residuals sum to 0 (algebraic properties discussed in <a href="https://statsnotes.github.io/theory/6.html#residuals-and-properties-of-ols-estimator">1.6.1</a>).</li>
</ul>
<p><strong>Property 2:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n \widetilde{r_{1i}} x_{ji} = 0, \text{ given } j=2, \dots, k
</span></p>
<ul>
<li>Because for OLS, <span class="math inline">\sum x_i \hat u_i = 0</span> (<a href="https://statsnotes.github.io/theory/7.html#residuals-and-algebraic-properties-of-ols">1.7.1</a>), and we know <span class="math inline">\widetilde{r_{1i}}</span> is the residual <span class="math inline">\hat u_i</span> in a regression with explanatory variables <span class="math inline">x_2, \dots, x_k</span> and outcome variable <span class="math inline">x_1</span>.</li>
</ul>
<p><strong>Property 3:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n \widetilde{r_{1i}} x_{1i} = \sum\limits_{i=1}^n \widetilde{r_{1i}}(\hat x_{1i} + \sum\limits_{i=1}^n \widetilde{r_{1i}}) = \sum\limits_{i=1}^n \widetilde{r_{1i}}^2
</span></p>
<ul>
<li>Because we have the regression fitted values <span class="math inline">\hat x_{1i} = \hat \gamma_0 + \hat\gamma_1 x_2 + \dots + \hat\gamma_{k-1} x_{k} + \widetilde{r_{1i}}</span> from <a href="https://statsnotes.github.io/theory/5.html#regression-anatomy-formula-for-ols-estimation">1.5.6</a>.</li>
<li>And we know with regression, actual values are the predicted plus residual: <span class="math inline">y_i = \hat y_i + \hat u_i</span>. Thus, <span class="math inline">x_i = \hat x_i + \widetilde{r_{1i}}</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>Recall the regression anatomy solution of OLS for <span class="math inline">\hat\beta_1</span> (derived in <a href="https://statsnotes.github.io/metrics/2.html#regression-anatomy-formula-for-ols-estimation">2.2.6</a>):</p>
<p><span class="math display">
\hat\beta_1 = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ y_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
</span></p>
<ul>
<li>Where <span class="math inline">\widetilde{r_{1i}}</span> is the part of <span class="math inline">x_1</span> uncorrelated with <span class="math inline">x_2, \dots, x_k</span>.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.3 No Perfect Multicollinearity
</div>
</div>
<div class="callout-body-container callout-body">
<p>The existence of <span class="math inline">\hat\beta_1</span> is guaranteed by MLR.3 <span class="math inline">\sum\widetilde{r_{1i}}^2 ≠ 0</span>, since we cannot divide by 0.</p>
</div>
</div>
<p>Now, let us plug in <span class="math inline">y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i</span> into our regression anatomy formula:</p>
<p><span class="math display">
\begin{split}
\hat\beta_1 &amp; = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ y_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ (\beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i)}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
\end{split}
</span></p>
<p>Now, focusing on the numerator, and using the summation properties above, let us simplify:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n \widetilde{r_{1i}} \ (\beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i) \\
&amp; = \sum\limits_{i=1}^n (\widetilde{r_{1i}}\beta_0 + \widetilde{r_{1i}}\beta_1x_{1i} + \dots + \widetilde{r_{1i}}\beta_k x_{ki} + \widetilde{r_{1i}} u_i) \\
&amp; = \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_0 + \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_1 x_{1i} + \dots + \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_k x_{ki} + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
&amp; = \beta_0 \sum\limits_{i=1}^n \widetilde{r_{1i}} + \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}} x_{1i} + \dots + \beta_k \sum\limits_{i=1}^n \widetilde{r_{1i}} x_{ki} + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
&amp; = \beta_0 (0) + \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}}^2 + \beta_2 (0) + \dots + \beta_k (0) + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
&amp; = \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}}^2 + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i
\end{split}
</span></p>
<p>Now, putting the numerator back in, we can simplify:</p>
<p><span class="math display">
\begin{split}
\hat\beta_1 &amp; = \frac{\beta_1 \sum_{i=1}^n \widetilde{r_{1i}}^2 + \sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; \\
&amp; = \frac{\beta_1 \sum_{i=1}^n \widetilde{r_{1i}}^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2} + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; \\
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
</span></p>
<p><br></p>
<p>Now, we want to find <span class="math inline">E(\hat\beta_1)</span>. Note that the second part of the equation is a function of <span class="math inline">u_i</span>, of which itself is a function of all explanatory variables <span class="math inline">x_{1i}, \dots, x_{ki}</span>. Thus, we know:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1 | x_{1i}, \dots x_{ki}) &amp; = \beta_1 + E\left( \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \biggr|x_{1i},\dots, x_{ki} \right) \\
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ E(u_i|x_{1i}, \dots,x_{ki})}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
</span></p>
<p>But what is <span class="math inline">E(u_i|x_{1i}, \dots , x_{ki})</span>? We can use two Gauss-Markov conditions to evaluate this:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.4 Zero Conditional Mean
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Zero-Conditional Mean assumption says <span class="math inline">E(u|x_1, \dots, x_k) = 0</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.2 Random Sampling
</div>
</div>
<div class="callout-body-container callout-body">
<p>Random sampling, combined with Zero-Conditional Mean, allows us to say:</p>
<p><span class="math display">
E(u|x_1, \dots, x_k)=E(u_1|x_{1i}, \dots, x_{ki}) = 0
</span></p>
</div>
</div>
<p>Thus, plugging that in to our formula, we get:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1 | x_{1i}, \dots x_{ki})
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ E(u_i|x_{1i}, \dots,x_{ki})}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ (0)}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; = \beta_1 + 0 \\
&amp; = \beta_1
\end{split}
</span></p>
<p>Now, just like in simple linear regression, we use the <strong>law of iterated expectations</strong> to conclude this proof:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1) &amp; = E[E(\hat\beta_1|x_{1i}, \dots, x_{ki})] \\
&amp; = E(\beta_1) \\
&amp; = \beta_1
\end{split}
</span></p>
<p>Thus, OLS is unbiased under the Gauss-Markov conditions.</p>
<ul>
<li>The most critical assumption, and the one most frequently violated, is SLR.4 Zero-Conditional Mean. We will discuss this assumption more as the lesson progresses.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="asymptotic-consistency-of-ols" class="level1">
<h1>2.3.5: Asymptotic Consistency of OLS</h1>
<p>We have proven OLS is unbiased under the 4 Gauss-Markov conditions. The key assumption (as mentioned multiple times) is Zero-Conditional Mean:</p>
<p><span class="math display">
E(u|x_1, \dots x_k) = 0
</span></p>
<p>This assumption means that the error term <span class="math inline">u</span> cannot be correlated with any <span class="math inline">x_1, \dots x_k</span>, OR any function of <span class="math inline">x_1, \dots, x_k</span>. This is quite restrictive.</p>
<p>However, this assumption can be weakened to a new assumption:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Zero Mean and Exogeneity Assumption
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can weaken <em>MLR.4 Zero-Conditional Mean</em> to the <em>Zero-Mean and Exogeneity assumption</em>:</p>
<p><span class="math display">
\begin{split}
&amp; E(u) = 0 \\
&amp; Cov(x_j, u) = 0, \quad \text{for } j = 1,\dots k
\end{split}
</span></p>
<p>The second assumption can also be rewritten as:</p>
<p><span class="math display">
E(x_j u) = 0
</span></p>
<ul>
<li>The proof of this is almost identical to the proof of <span class="math inline">\sum x \hat u = 0</span> implying <span class="math inline">Cov(x, \hat u) = 0</span> we did in section 2.3.1.</li>
</ul>
<p>Note: if we meet the full MLR.4 Zero-Conditional Mean assumption, we will automatically meet this weakened assumption. However, the reverse is not true.</p>
<p><u>Under this new assumption (and the other 3 Gauss-Markov assumptions), OLS is biased in small sample sizes, but asymptotically consistent.</u></p>
</div>
</div>
<p>This assumption is weaker than our original assumption MLR.4 Zero-Conditional Mean, as we no longer need to have <span class="math inline">u</span> uncorrelated with any function of <span class="math inline">x_1, \dots, x_k</span>. We only need <span class="math inline">u</span> to be uncorrelated with each <span class="math inline">x_j</span> individually.</p>
<p>Under this new assumption (and the other 3 Gauss-Markov assumptions), OLS is biased in small sample sizes, but asymptotically consistent. In other words:</p>
<p><span class="math display">
\text{plim}(\hat\beta_j) = \beta_j
</span></p>
<p><br></p>
<section id="proof-of-ols-asymptotic-consistency" class="level3">
<h3 class="anchored" data-anchor-id="proof-of-ols-asymptotic-consistency">Proof of OLS Asymptotic Consistency</h3>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Ensure you understand section <a href="https://statsnotes.github.io/metrics/1.html#asymptotic-consistency-of-estimators">2.1.5</a> well before starting this section.</p>
</div>
</div>
<p>In section 2.3.3, we got to this point when proving unbiasdeness:</p>
<p><span class="math display">
\hat\beta_1=  \beta_1 + \frac{\sum_{i=1}^n(x_i - \bar{x}) u_i}{\sum_{i=1}^n (x_i - \bar{x})^2}
</span></p>
<p>We can add <span class="math inline">\frac{1}{n}</span> (or <span class="math inline">n^{-1}</span>) to the top and bottom of the fraction (which cancel each other out, keeping the equation equivalent):</p>
<p><span class="math display">
\hat\beta_1=  \beta_1 + \frac{n^{-1} \sum_{i=1}^n(x_i - \bar{x}) u_i}{n^{-1} \sum_{i=1}^n (x_i - \bar{x})^2}
</span></p>
<p>Now, let us expand the numerator, and simplify, and we get:</p>
<p><span class="math display">
\begin{split}
\hat\beta_1 &amp; =  \beta_1 + \frac{n^{-1} \sum_{i=1}^n x_i u_i - n^{-1} \sum_{i=1}^n \bar x u_i}{n^{-1} \sum_{i=1}^n (x_i - \bar{x})^2} \\
&amp; =  \beta_1 + \frac{n^{-1} \sum_{i=1}^n x_i u_i - \bar x \ n^{-1} \sum_{i=1}^n u_i}{n^{-1} \sum_{i=1}^n (x_i - \bar{x})^2} \\
&amp; =  \beta_1 + \frac{ \overline{xu} - \bar x \bar u}{S.Var(x)}
\end{split}
</span></p>
<ul>
<li>Where <span class="math inline">S.Var(x)</span> is the sample variance of <span class="math inline">x</span>.</li>
</ul>
<p>We want to find <span class="math inline">\text{plim}(\hat\beta_1)</span>. We will need a few properties for this:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of Probability Limits
</div>
</div>
<div class="callout-body-container callout-body">
<p>We know these general rules about probability limits (see <a href="https://statsnotes.github.io/metrics/1.html#asymptotic-consistency-of-estimators">2.1.5</a>)</p>
<p><span class="math display">
\begin{split}
&amp; \text{plim}(\bar x_n) = \mu _x \\
&amp; \text{plim}(S.Var(x_i)) = Var(x_i) \\
&amp; \text{plim}(S.Cov(x_i, y_i)) = Cov (x_i, y_i)
\end{split}
</span></p>
<p>The other properties are about algebra with probability limits. Assume <span class="math inline">\text{plim} (u_n) = a</span>, and <span class="math inline">\text{plim}(v_n) = b</span>. Then, the following are true:</p>
<p><span class="math display">
\begin{split}
&amp; \text{plim} (u_n + v_n) = a + b \\
&amp; \text{plim} (u_n v_n) = ab \\
&amp; \text{plim} (u_n v_n) = a/b
\end{split}
</span></p>
</div>
</div>
<p>Knowing this, we then know that:</p>
<p><span class="math display">
\begin{split}
\text{plim}(\hat\beta_1) &amp; = \beta_1 + \frac{ \text{plim}(\overline{xu}) - \text{plim}( \bar x \bar u) }{ \text{plim}(S.Var(x))} \\
&amp; = \beta_1 + \frac{ \text{plim}(\overline{xu}) - \text{plim}(\bar x) \text{plim}(\bar u) }{ \text{plim}(S.Var(x))} \\
&amp;  = \beta_1 + \frac{ E(xu) - E(x) E(u) }{Var(x)} \\
\end{split}
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Zero-Mean and Exogeneity Assumption
</div>
</div>
<div class="callout-body-container callout-body">
<p>The weakened Zero-Mean and Exogeneity assumption states:</p>
<p><span class="math display">
\begin{split}
&amp; E(u) = 0 \\
&amp; E(xu) = 0
\end{split}
</span></p>
</div>
</div>
<p>Using this assumption, we can conclude the proof:</p>
<p><span class="math display">
\begin{split}
\text{plim}(\hat\beta_1) &amp; = \beta_1 + \frac{ E(xu) - E(x) E(u) }{Var(x)} \\
&amp; = \beta_1 + \frac{ 0 - E(x)0}{Var(x)} \\
&amp; = \beta_1 + \frac{ 0 }{Var(x)} \\
&amp; = \beta_1+0 \\
\text{plim}(\hat\beta_1) &amp; = \beta_1
\end{split}
</span></p>
<p>Thus, OLS is asymptotically consistent under a weakened version of MLR.4 Zero-Conditional Mean - called the Zero-Mean and Exogeneity Assumption.</p>
<ul>
<li>Under this weakened assumption (without meeting the full MLR.4 assumption), OLS is <u>biased but consistent</u>.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="exogeneity-and-endogeneity" class="level1">
<h1>2.3.6: Exogeneity and Endogeneity</h1>
<p>We have proven that OLS is unbiased under the key assumption of MLR.4 Zero-Conditional Mean:</p>
<p><span class="math display">
E(u|x_1, \dots x_k) = 0
</span></p>
<p>We also proven that OLS is asymptotically consistent under a weaker version of the assumption: Zero-Mean and Exogeneity:</p>
<p><span class="math display">
\begin{split}
&amp; E(u) = 0 \\
&amp; Cov(x_j, u) = 0, \quad \text{for } j = 1,\dots k
\end{split}
</span></p>
<p>The critical part of both assumptions is <strong>Exogeneity</strong>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Exogeneity and Endogeneity
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Exogeneity</strong> is defined as each explanatory variable being uncorrelated with the error term:</p>
<p><span class="math display">
Cov(x_j, u)=0 \quad \text{for } j =1, \dots, k
</span></p>
<p>If this is violated, it is called <strong>endogeneity</strong>. An endogenous regressor is defined as an explanatory variable that is correlated with the error term.</p>
<p>If exogeneity is violated and endogeneity is present, two things happen:</p>
<ol type="1">
<li>We fail the “weak” Zero-Mean and Exogeneity condition, meaning OLS is not asymptotically consistent if we have endogeneity.</li>
<li>We fail MLR.4 Zero-Conditional Mean, meaning OLS is biased.</li>
</ol>
<p>Thus, Exogeneity is a neccesary condition for OLS estimator to be reliable for estimation.</p>
</div>
</div>
<p><br></p>
<p>We want to avoid Endogeneity, as it means OLS is both biased and asymptotically inconsistent.</p>
<ul>
<li>That basically means when endogeneity is present, under no circumstances can we rely on the accuracy of the estimates of OLS.</li>
</ul>
<p>But what causes Endogeneity? The main cause of endogeneity is Omitted Variable Bias (covered in <a href="https://statsnotes.github.io/intro/5.html#omitted-variable-bias-and-confounding-variables">1.5.3</a>).</p>
<ul>
<li>The omitted confounding variable <span class="math inline">z</span>’s effect is mostly subsumed into the error term <span class="math inline">u_i^S</span> of the short regression.</li>
<li>But some bit of <span class="math inline">z</span> (that is correlated with <span class="math inline">x</span>) is included in our coefficient (specifically, <span class="math inline">\beta_2 \delta_1</span>).</li>
<li>That means our explanatory variable <span class="math inline">x</span> will be correlated with the error term, meaning we have endogeneity and unreliable OLS estimates.</li>
</ul>
<p>However, there are other factors that can cause endogeneity as well.</p>
<ul>
<li>Simultaneity can cause endogeneity: when both the explanatory variable and outcome variable explain each other.</li>
<li>Measurement error can also cause endogeneity.</li>
</ul>
<p><br></p>
<p>How can we solve this problem?</p>
<ul>
<li>The easiest way is to add more control variables.</li>
<li>If we include every single possible confounder that is correlated both with our explanatory variable of interest, and the outcome variable, then, there will be no more omitted variable bias, and no more endogeneity.</li>
</ul>
<p>However, as we will discuss a lot more in Part III on applied econometrics and causal inference, including all confounding variables is often impossible, as many social science situations will have thousands of them, many of them <strong>unobservable</strong> or impossible to measure.</p>
<ul>
<li>Thus, there are also other techniques to “unbias” OLS without including all possible confounders. We will discuss an example of this in <a href="https://statsnotes.github.io/metrics/6.html">lesson 2.6</a> on instrumental variables.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="ols-and-the-conditional-expectation-function" class="level1">
<h1>2.3.7: OLS and the Conditional Expectation Function</h1>
<p>We previously discussed random variables and distributions in <a href="https://statsnotes.github.io/theory/1.html">lesson 1.1</a>. There, we learned that random variables can be characterised by distributions, which can be summarised with the expectation and variance.</p>
<p><span class="math inline">y</span> in a regression is also a random variable.</p>
<ul>
<li>For example, imagine income was <span class="math inline">y</span>.</li>
<li>There is a distribution of income in a population - such that if we randomly selected someone from the population, there would be a probability associated with selecting someone with an income between $60,000-$70,000.</li>
</ul>
<p>We can characterise the distribution of <span class="math inline">y</span> with its expected value: <span class="math inline">E(y)</span>.</p>
<p>A <strong>conditional expectation function</strong> says that the value of <span class="math inline">E(y)</span> depends on the value of <span class="math inline">x</span>. We notate a conditional expectation function as <span class="math inline">E(y|x)</span>.</p>
<ul>
<li>For example, imagine <span class="math inline">y</span> is income and <span class="math inline">x</span> is age.</li>
<li>A conditional expectation function <span class="math inline">E(y|x)</span> says that as <span class="math inline">x</span> (age) changes, the expected value of <span class="math inline">y</span> (income) also changes.</li>
<li>For example, you would probably expect the expected value of a 20 year old’s income to be different than a 50 year old’s.</li>
</ul>
<p><br></p>
<p>The Ordinary Least Squares Regression line also is the best linear approximation of the conditional expectation function <span class="math inline">E(y|x)</span>. That means, we can view a regression as also a conditional expectation function.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Linear Conditional Expectation Function
</div>
</div>
<div class="callout-body-container callout-body">
<p>The conditional expectation function is some function which describes the conditional expectation of <span class="math inline">y</span> given <span class="math inline">x</span>. This function can be linear, or not:</p>
<p><span class="math display">
E(y_i|x_i) = m(x_i)
</span></p>
<ul>
<li>Where <span class="math inline">m(x_i)</span> is some function (does not have to be linear).</li>
</ul>
<p>A best linear approximation of a conditional expectation function, can take the following form:</p>
<p><span class="math display">
E(y_i|x_i) = b_0 + b_1x_i
</span></p>
<ul>
<li>Where <span class="math inline">b_0</span> and <span class="math inline">b_1</span> are the parameters/coefficients of the model.</li>
<li>Where <span class="math inline">E(y_i|x_i)</span> is the expectation of the conditional distribution <span class="math inline">y|x</span>.</li>
<li>Where the conditional distribution <span class="math inline">y|x</span> has variance <span class="math inline">\sigma^2</span> (this assumption we will explore in more detail in the <a href="https://statsnotes.github.io/theory/7.html">next lesson</a>).</li>
</ul>
<p>The best linear approximation of the conditional expectation function is defined as the parameters that minimise the mean squared errors (MSE).</p>
<p><span class="math display">
\begin{split}
MSE &amp; = E(y_i - E(y_i|x_i))^2 \\
&amp; = \frac{1}{n}\sum\limits_{i=1}^n( y_i - E(y_i|x_i))^2
\end{split}
</span></p>
</div>
</div>
<p><br></p>
<p>What we want to prove is that the OLS estimates <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span> best estimate the parameters <span class="math inline">b_0</span> and <span class="math inline">b_1</span> of the Conditional Expectation Function, which means that if true, OLS is the best linear approximation of the conditional expectation function.</p>
<p>Suppose we have the conditional expectation function:</p>
<p><span class="math display">
E(y_i|x_i) = b_0 + b_1x_i
</span></p>
<p>We also know that our typical regression equation is:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_i + u_i
</span></p>
<p>We know that <span class="math inline">E(u_i|x_i) = 0</span>. Let us define <span class="math inline">u_i</span> as the following:</p>
<p><span class="math display">
u_i = y_i - E(y_i|x_i)
</span></p>
<p>If the above defined <span class="math inline">u_i</span> is true, <span class="math inline">E(u_i|x_i)</span> should also be equal to 0. So, let us plug in the above <span class="math inline">u_i</span> into <span class="math inline">E(u_i | x_i)</span>.</p>
<p><span class="math display">
\begin{split}
E(u_i|x_i) &amp; = E(y_i - E(y_i|x_i) \ | \ x_i) \\
&amp; = E(y_i|x_i) - E(y_i|x_i) \\
&amp; = 0
\end{split}
</span></p>
<p>Thus, we know <span class="math inline">u_i = y_i - E(y_i|x_i)</span> to be true. Thus, rearranging, we know:</p>
<p><span class="math display">
y_i = E(y_i|x_i) + u_i
</span></p>
<p>We also know that <span class="math inline">y_i = \beta_0 + \beta_1 x_i + u_i</span>. Thus, the following is true:</p>
<p><span class="math display">
\begin{split}
E(y_i|x_i) + u_i &amp; = \beta_0 + \beta_1 + u_i \\
b_0 + b_1 + u_i &amp; = \beta_0 + \beta_1 + u_i \\
b_0 + b_1 &amp; = \beta_0 + \beta_1
\end{split}
</span></p>
<p>Well, you might point out, it is still possible that <span class="math inline">b_1 ≠ \beta_1</span> in this scenario. We can go further. We know that the conditional expectation function minimises the mean of squared errors.</p>
<p><span class="math display">
\begin{split}
MSE &amp; = \min\limits_{b_0, b_1} E(y_i - E(y_i|x_i))^2 \\
&amp; = \min\limits_{b_0, b_1} E(y_i - \beta_0 - \beta_1x_i)^2
\end{split}
</span></p>
<p>The first order conditions are (using chain rule and partial derivatives, just like in the OLS derivation):</p>
<p><span class="math display">
\begin{split}
&amp; E(y_i - b_0 - b_1x_i) = 0 \\
&amp; E(x_i(y_i - b_0 - b_1x_i) = 0
\end{split}
</span></p>
<p><br></p>
<p>Now, recall our OLS minimisation conditions (from <a href="https://statsnotes.github.io/metrics/2.html#estimation-for-simple-linear-regression">2.2.3</a>)</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
</span></p>
<p>Since by definition, average/expectation is <span class="math inline">E(x) = \frac{1}{n} \sum x_i</span>, we can rewrite the OLS minimisation conditions as:</p>
<p><span class="math display">
\begin{split}
&amp; n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
</span></p>
<p>And since anything multiplied to a zero turns into zero, we can ignore the <span class="math inline">n</span> in the first order condition, and only focus on the expected value part. Thus, our conditions are:</p>
<p><span class="math display">
\begin{split}
&amp; E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
</span></p>
<p>Which as we can see, are the exact same minimisation conditions as the conditional expectation function. Thus, OLS is the best approximation of the conditional expectation function.</p>
<p><br></p>
<p>The fact that OLS best approximates the conditional expectation function is an immensely useful property of OLS.</p>
<ul>
<li>We know that the expectation of a random variable is the best guess of the random variable’s value (as explained in <a href="https://statsnotes.github.io/theory/1.html#expected-value-and-mean">1.1.4</a>).</li>
<li>We also now know that OLS best approximates the conditional expectation function <span class="math inline">E(y|x)</span>.</li>
<li>Thus, OLS estimates <span class="math inline">\hat y</span> are the best guess of the value of <span class="math inline">y</span>, given any values of <span class="math inline">x</span>. This is immensely useful in both prediction purposes and causal estimation purposes (which we will focus on in later parts of the guide).</li>
</ul>
<p>This is also why when discussing interpretation of OLS coefficients in both simple and multiple linear regression, we have been describing the change in the <u>expected</u> value of <span class="math inline">y</span>.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>