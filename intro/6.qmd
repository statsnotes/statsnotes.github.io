---
title: "Statistics for Social Scientists"
subtitle: "Lesson 1.6: Categorical Variables in Regression"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Lesson 1.6: Categorical Variables in Regression"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

::: callout-note
## Lesson Overview

In the last few lessons, we have discussed the linear regression model, focusing on continuous $x$ variables. In this lesson, we explore how binary and categorical variables, which are very frequent in the social sciences, can be used in regression with OLS.

This lesson covers the following topics:

-   What categorical variables are (including binary and ordinal variables).
-   How binary and categorical explanatory variables work in regression.
-   How binary outcome variables work in the framework of the linear probability model.
-   The limitations of linear regression when it comes to binary and categorical outcome variables.
:::

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.6.1: What are Categorical Variables

The three main types of variables in statistics are **categorical**, **ordinal**, and **continuous** variables. What are these different types of variables, and how do they differ?

<br />

**Categorical variables** are variables that have discrete categories, that cannot be ordered.

1.  Discrete categories means that there are different "categories" of outcomes, and you can only get values in those categories. For example, if you roll a dice, you can only get one of 6 outcomes. You cannot get outcome 3.4234783 - that is not a category.
2.  Categories cannot be ordered. For example, take the categories Microsoft, Apple, and Google. There is no "order" to these. For example, we know $5 > 1$. But we cannot say Microsoft \> apple, or Google \> apple.

Categorical variables are described by discrete random variables, as discussed in [1.1.3](https://statsnotes.github.io/theory/1.html#probability-mass-and-density-functions), and are described by probability mass functions.

A special type of categorical variable is the **binary/dummy variable**, which is a variable that has only two categories.

-   These are extremely common in social sciences. For example, variables involving treatment/control groups, true/false, did/didn't, yes/no, are all examples of binary/dummy variables.

[Categorical variables require special treatment in linear regression (and regression models in general), which we will discuss in this lesson.]{.underline}

<br />

**Ordinal variables** are variables that have discrete categories, that can be ordered.

-   Discrete categories means the same thing as above with categorical variables.

However, ordinal variables are ordered.

-   For example, pretent you fill out a survey ranking your experience with a service, that gives you the options *poor experience, neutral, good experience*. We can say that *poor experience \< neutral*, and *neutral \< good experience*. These categories can be ordered.
-   Or for example, you are to giving a star rating to a restaurant: either 1, 2, 3, 4, or 5 stars. You cannot give 4.5 stars, or 3.432748 stars, so there are discrete categories. However you can order these categories: clearly 5 stars \> 3 stars.

However, we cannot say anything about the *distance* between categories.

-   For example, we cannot say that the distance between *poor experience* and *neutral* is equal to the distance between *neutral* and *good experience*.
-   We can only conclude on the order, not the distance.

Ordinal variables can be "treated" as continuous variables in a regression setting (see below for what continuous variables are).

-   However, they can also be treated as categorical variables (if you have some reason to believe the ordering is arbitrary and not useful).

<br />

**Continuous** variables are variables that are, well, continuous. More specifically, continuous variables have 2 characteristics:

1.  They have infinite subdivisions between any two values - there is always some value in between two different values. For example, take two values: $1$ and $2$. In a continuous variable, $1.5$ is in between them. Take another example of two values: $1$ and $1.000001$. In a continuous variable, $1.00000000001$ is between those two values.
2.  The distance between values is consistent and meaningful. For example, the distance between 3 and 1 is 2, and the distance between 11 and 9 is also 2. That distance 2 is consistent and meaningful. This is in contrast to ordinal variables.

An example of a continuous variable is temperature:

-   Temperature has infinite subdivisions: We can have temperature at 20.34372 Celsius.
-   Temperature also has consistent distance between values. Going from 5 to 6 Celsius, or 7 to 8 Celsius, are all a 1 Celsius increase.

Continuous variables can be described by probability density functions as discussed in [1.1.3](https://statsnotes.github.io/theory/1.html#probability-mass-and-density-functions).

<br />

Continuous variables have been what we have discussed so far in regression.

-   All of our previous interpretation has focused on assuming both $x$ and $y$ are continuous (or ordinal if we treat the ordinal variable as continuous).

However, categorical variables have slightly different interpretations and implementations, due to their unique characteristics of discreteness and unorderedness. This chapter focuses on how we can adjust the linear regression model to fit these characteristics.

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.6.2: Binary Explanatory Variables in Regression

**Binary explanatory** variables are categorical variables with 2 categories, 0 and 1.

-   Binary explanatory variables are extremely common in the social sciences. They can include things such as yes/no questions, treatment/control, true/false questions, voted/did not vote, etc.

Implementing binary explanatory variables is very simple in regression - we just include them in our regression model as a normal $x$:

$$
y_i = \beta_0 + \beta_1x_i + u_i
$$

-   Where $x$ is a binary explanatory variable.

However, Binary explanatory variables will change the interpretations of our coefficients, and slightly change the estimation process of OLS.

<br />

How are binary explaantory variables interpreted in linear regression? We can "solve" for these interpretations. Assume $x$ has two categories $x=0$ and $x=1$:

-   I will use the conditional expectation function since it makes more sense (as you will see in a second).

$$
\begin{split}
&E(y|x=0) = \hat\beta_0 + \hat\beta_1(0) = \hat\beta_0 \\
& E(y|x=1) = \hat\beta_0 + \hat\beta_1(1) = \hat\beta_0 + \hat\beta_1 \\
& E(y|x=1) - E(y|x=0) = (\hat\beta_0 + \hat\beta_1) - \hat\beta_0 = \hat\beta_1
\end{split}
$$

Thus, through this, we can see:

-   The expected value of $y$ when $x = 0$ is $\hat\beta_0$
-   The expected value of $y$ when $x = 1$ is $\hat\beta_0 + \hat\beta_1$
-   The difference between the expected values of $y$ when $x=1$ and $x=0$ is $\hat\beta_1$

Thus, we can interpret the coefficients as following:

::: callout-tip
## Interpretation of Coefficient with a Binary Explanatory Variable

When $x$ is a binary explanatory variable:

-   $\hat\beta_0$ is the expected value of $y$ given an observation in category $x = 0$
-   $\hat\beta_0 + \hat\beta_1$ is the expected value of $y$ given an observation in category $x = 1$
-   $\hat\beta_1$ is the expected difference in $y$ between the categories $x=1$ and $x=0$.
:::

::: callout-warning
## Warning!

Note how I have been using the word **relationship**, not causal effect.

Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).
:::

<br />

We can see here that our $\hat\beta_1$ is the expected [difference-in-means]{.underline} of the two categories of $x$.

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.6.3: Categorical Explanatory Variables in Regression

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.6.4: Interpreting Categorical Explanatory Variables

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.9.5: The Dummy Variable Trap

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.9.6: The Linear Probability Model

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.9.7: Limitations of Linear Regression
