<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistics for Social Scientists</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="6_files/libs/clipboard/clipboard.min.js"></script>
<script src="6_files/libs/quarto-html/quarto.js"></script>
<script src="6_files/libs/quarto-html/popper.min.js"></script>
<script src="6_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="6_files/libs/quarto-html/anchor.min.js"></script>
<link href="6_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="6_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="6_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="6_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="6_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Statistics for Social Scientists</h1>
            <p class="subtitle lead">Lesson 1.6: Categorical Variables in Regression</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Lesson 1.6: Categorical Variables in Regression</h2>
   
  <ul class="collapse">
  <li><a href="#what-are-categorical-variables" id="toc-what-are-categorical-variables" class="nav-link active" data-scroll-target="#what-are-categorical-variables">1.6.1: What are Categorical Variables</a></li>
  <li><a href="#binary-explanatory-variables-in-regression" id="toc-binary-explanatory-variables-in-regression" class="nav-link" data-scroll-target="#binary-explanatory-variables-in-regression">1.6.2: Binary Explanatory Variables in Regression</a></li>
  <li><a href="#categorical-explanatory-variables-in-regression" id="toc-categorical-explanatory-variables-in-regression" class="nav-link" data-scroll-target="#categorical-explanatory-variables-in-regression">1.6.3: Categorical Explanatory Variables in Regression</a></li>
  <li><a href="#interpreting-categorical-explanatory-variables" id="toc-interpreting-categorical-explanatory-variables" class="nav-link" data-scroll-target="#interpreting-categorical-explanatory-variables">1.6.4: Interpreting Categorical Explanatory Variables</a></li>
  <li><a href="#the-dummy-variable-trap" id="toc-the-dummy-variable-trap" class="nav-link" data-scroll-target="#the-dummy-variable-trap">1.9.5: The Dummy Variable Trap</a></li>
  <li><a href="#binary-outcomes-and-the-linear-probability-model" id="toc-binary-outcomes-and-the-linear-probability-model" class="nav-link" data-scroll-target="#binary-outcomes-and-the-linear-probability-model">1.9.6: Binary Outcomes and the Linear Probability Model</a></li>
  <li><a href="#limitations-of-linear-regression-for-non-continuous-outcomes" id="toc-limitations-of-linear-regression-for-non-continuous-outcomes" class="nav-link" data-scroll-target="#limitations-of-linear-regression-for-non-continuous-outcomes">1.9.7: Limitations of Linear Regression for Non-Continuous Outcomes</a></li>
  <li><a href="#implementation-in-r" id="toc-implementation-in-r" class="nav-link" data-scroll-target="#implementation-in-r">Implementation in R</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the last few lessons, we have discussed the linear regression model, focusing on continuous <span class="math inline">x</span> variables. In this lesson, we explore how binary and categorical variables, which are very frequent in the social sciences, can be used in regression with OLS.</p>
<p>This lesson covers the following topics:</p>
<ul>
<li>What categorical variables are (including binary and ordinal variables).</li>
<li>How binary and categorical explanatory variables work in regression.</li>
<li>How binary outcome variables work in the framework of the linear probability model.</li>
<li>The limitations of linear regression when it comes to binary and categorical outcome variables.</li>
</ul>
</div>
</div>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
<section id="what-are-categorical-variables" class="level1">
<h1>1.6.1: What are Categorical Variables</h1>
<p>The three main types of variables in statistics are <strong>categorical</strong>, <strong>ordinal</strong>, and <strong>continuous</strong> variables. What are these different types of variables, and how do they differ?</p>
<ul>
<li>We will first introduce the main types of variables.</li>
<li>Then, we will introduce the implications for these variables in regression.</li>
</ul>
<p><br></p>
<section id="categorical-variables" class="level3">
<h3 class="anchored" data-anchor-id="categorical-variables">Categorical Variables</h3>
<p>Categorical variables are variables which have a group of different categories in which each observation can fall in. These categories are discrete, distinct, and have no natural order.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: State
</div>
</div>
<div class="callout-body-container callout-body">
<p>An example of a categorical variable is <em>state of birth</em>. For example, the variable could have the categories: New York, California, Nevada, etc..</p>
<ul>
<li>There are a finite number of categories - there are 50 categories. For simplicity, we can label each category with a number between 1 and 50.</li>
<li>These categories are discrete and distinct: you cannot have any individual <span class="math inline">i</span> who was born in category 3.5 - they have to either be born in category 3, or category 4, or one category. They cannot be born in-between categories.</li>
<li>These categories have no natural order: there is no “inherent” way to organise states. Of course, we could order them by population, alphabetical order, etc., but these are all choices we have to make - there is no <strong>natural</strong> order.</li>
</ul>
</div>
</div>
<p>A sub-category of categorical variables are <strong>binary variables</strong>. These are categorical variables with only 2 categories. We can label each category as either 0 or 1.</p>
<p>Binary variables are extremely common, including true/false, yes/no, voted/did not vote, etc.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Gender
</div>
</div>
<div class="callout-body-container callout-body">
<p>An example of a binary variable is <em>gender</em>. This variable has two categories: female and non-female (male).</p>
</div>
</div>
<p><br></p>
</section>
<section id="ordinal-variables" class="level3">
<h3 class="anchored" data-anchor-id="ordinal-variables">Ordinal Variables</h3>
<p>Ordinal variables are very similar to categorical variables. They also have categories that are discrete and distinct. However, the main difference is that ordinal variable categories <u>have a natural order</u>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Agreement
</div>
</div>
<div class="callout-body-container callout-body">
<p>In many surveys, you will have a variable called agreement. This variable has 3 categories: disagree, neutral, and agree.</p>
<ul>
<li>There are a finite number of categories - there are 3 categories. For simplicity, we can label each category with a number between 1 and 3.</li>
<li>These categories are discrete and distinct: you cannot have any individual <span class="math inline">i</span> who responds with category 1.5 - they have to choose one of the three options.</li>
<li>However, these categories have a natural order. Clearly, in order of agreement, we can order the categories: disagree, neutral, and agree.</li>
</ul>
</div>
</div>
<p><br></p>
</section>
<section id="continuous-variables" class="level3">
<h3 class="anchored" data-anchor-id="continuous-variables">Continuous Variables</h3>
<p>Continuous variables can take on all numeric values (including decimals) within a range.</p>
<ul>
<li>This means they are not discrete. For example, 3 and 4 are not discrete outcomes in a continuous variable, because you can have values of 3.5, 3.1, 3.3248932, etc.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Temperature
</div>
</div>
<div class="callout-body-container callout-body">
<p>Temperature is a continuous variable. You can have values of 0 Celsius, 50 Celsius, 32.3247 Celsius, etc.</p>
</div>
</div>
<p>There is an extension of continuous variables: <strong>Count Variables</strong>.</p>
<ul>
<li>Count variables have values that start from 0, and go to infinity. They cannot be negative.</li>
<li>Interestingly, count variables should in theory only have full integer values. This seems a little more similar to an ordinal variable, however, count variables tend to fit better with continuous variables in terms of statistical models.</li>
</ul>
<p><br></p>
</section>
<section id="types-of-variables-and-regression" class="level3">
<h3 class="anchored" data-anchor-id="types-of-variables-and-regression">Types of Variables and Regression</h3>
<p>We discussed three types of variables: categorical, ordinal, and continuous. But what are the implications of these on linear regression models?</p>
<p>Up until now (Lesson <a href="https://statsnotes.github.io/intro/4.html">1.4</a> and <a href="https://statsnotes.github.io/intro/5.html">1.5</a>), we have focused on continuous variables. Continuous variables always work with linear regression, either as explanatory variables <span class="math inline">x</span> or outcome variables <span class="math inline">y</span>.</p>
<ul>
<li>This is the same with Count Variables. However, there are specialised regression models for <strong>Count Outcome Variables</strong> (negative binomial and poisson) that we will explore in a later lesson. These models have some advantages over linear regression, however, linear regression still works.</li>
</ul>
<p><br></p>
<p>Categorical variables are a little different - we will explore this throughout the lesson.</p>
<p><br></p>
<p>Ordinal variables are perhaps the most complex.</p>
<ul>
<li>All ordinal variables (explanatory and outcome) can be treated as continuous variables in linear regression. There is generally no issue with this.</li>
<li>Ordinal explanatory variables can also be treated the same as categorical explanatory variables in linear regression (although, this is not common).</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="binary-explanatory-variables-in-regression" class="level1">
<h1>1.6.2: Binary Explanatory Variables in Regression</h1>
<p><strong>Binary explanatory</strong> variables are categorical variables with 2 categories, 0 and 1.</p>
<ul>
<li>Binary explanatory variables are extremely common in the social sciences. They can include things such as yes/no questions, treatment/control, true/false questions, voted/did not vote, etc.</li>
</ul>
<p>Implementing binary explanatory variables is very simple in regression - we just include them in our regression model as a normal <span class="math inline">x</span>:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_i + u_i
</span></p>
<ul>
<li>Where <span class="math inline">x</span> is a binary explanatory variable.</li>
</ul>
<p><br></p>
<p>However, Binary explanatory variables will change the interpretations of our coefficients, and slightly change the estimation process of OLS.</p>
<p>We can “solve” for these interpretations. Assume <span class="math inline">x</span> has two categories <span class="math inline">x=0</span> and <span class="math inline">x=1</span>. We can plug these two into our fitted values model:</p>
<p><span class="math display">
\begin{split}
&amp; \hat y_{i, \ x_i = 0} = \hat\beta_0 + \hat\beta_1(0) = \hat\beta_0 \\
&amp; \hat y_{i, \ x_i = 1} = \hat\beta_0 + \hat\beta_1(1) = \hat\beta_0 + \hat\beta_1 \\
&amp; \hat y_{i, \ x_i = 1} - \hat y_{i, \ x_i = 0} \ = \ (\hat\beta_0 + \hat\beta_1) - \hat\beta_0 = \hat\beta_1
\end{split}
</span></p>
<p>Thus, through this, we can see:</p>
<ul>
<li>The expected value of <span class="math inline">y</span> when <span class="math inline">x = 0</span> is <span class="math inline">\hat\beta_0</span></li>
<li>The expected value of <span class="math inline">y</span> when <span class="math inline">x = 1</span> is <span class="math inline">\hat\beta_0 + \hat\beta_1</span></li>
<li>The difference between the expected values of <span class="math inline">y</span> when <span class="math inline">x=1</span> and <span class="math inline">x=0</span> is <span class="math inline">\hat\beta_1</span></li>
</ul>
<p>Thus, we can interpret the coefficients as following:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Coefficient with a Binary Explanatory Variable
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x</span> is a binary explanatory variable:</p>
<ul>
<li><span class="math inline">\hat\beta_0</span> is the expected value of <span class="math inline">y</span> given an observation in category <span class="math inline">x = 0</span></li>
<li><span class="math inline">\hat\beta_0 + \hat\beta_1</span> is the expected value of <span class="math inline">y</span> given an observation in category <span class="math inline">x = 1</span></li>
<li><span class="math inline">\hat\beta_1</span> is the expected difference in <span class="math inline">y</span> between the categories <span class="math inline">x=1</span> and <span class="math inline">x=0</span>.</li>
</ul>
<p>We can see here that our <span class="math inline">\hat\beta_1</span> is the expected <u>difference-in-means</u> of the two categories of <span class="math inline">x</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how I have been using the word <strong>relationship</strong>, not causal effect.</p>
<p>Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).</p>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="categorical-explanatory-variables-in-regression" class="level1">
<h1>1.6.3: Categorical Explanatory Variables in Regression</h1>
<p>Categorical variables have multiple distinct and unordered categories. How do we include categorical variables as explanatory variables in linear regression?</p>
<p>What we typically do is to “transform” the categorical variable of many categories, into a series of binary variables. This is easier to see with an example.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Income Level Variable
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let us say we have a variable: <em>Income Level of the Country</em>. This variable has three categories: <em>Low Income (L), Middle Income (M), and High Income (H)</em>.</p>
<ul>
<li>This is a categorical variable with 3 categories.</li>
</ul>
<p>To include this in a regression, we will transform this variable into a set of binary variables. More specifically, we will create binary variables for <u>each category except one</u>, in the categorical variable.</p>
<p>For example, we could create 2 binary variables out of this regression:</p>
<ol type="1">
<li><em>Middle (M) Variable</em> <span class="math inline">x_M</span>: This variable takes <span class="math inline">x_M = 1</span> if a country is a middle income country, and <span class="math inline">x_M = 0</span> for a country in all other categories.</li>
<li><em>High (H) Variable</em> <span class="math inline">x_H</span>: This variable takes <span class="math inline">x_H = 1</span> if a country is a high income country, and <span class="math inline">x_H = 0</span> for a country in all other categories.</li>
</ol>
<p>What about the third category (low income) that we left out? Why do we not need 3 categorical variables?</p>
<ul>
<li><p>This is because if an observation is both <span class="math inline">x_M = 0</span> and <span class="math inline">x_H = 0</span> (not in either middle income or high income categories), we know that they will be in the low income category. So, we do not need an extra variable for that.</p></li>
<li><p>Another reason why we do not include the final cateogry is because of the issue of multicollinearity and the dummy variable trap, which is explained in <a href="https://statsnotes.github.io/intro/6.html#the-dummy-variable-trap">1.6.5</a>.</p></li>
</ul>
</div>
</div>
<p><br></p>
<p>So more generally, we always split a categorical variable into smaller binary variables, with one binary variable for each category <u>except one</u>.</p>
<ul>
<li>That one category left out is called the <strong>reference category</strong>.</li>
<li>There are no rules in choosing the reference category. We can choose any category, and the regression still works.</li>
<li>However, we often want to choose some meaningful category to leave out as the reference. This is because, as we will see in the next section, our regression results are interpreted in comparison to this reference category.</li>
</ul>
<p>So, this means if we have 10 categories, we will need 9 binary variables. If we have <span class="math inline">n</span> categories, we will need <span class="math inline">n-1</span> binary variables.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Linear Regression with a Polytomous Explanatory Variable
</div>
</div>
<div class="callout-body-container callout-body">
<p>A <strong>categorical explanatory variable</strong> with <span class="math inline">n</span> number of categories in <span class="math inline">x</span>, we would create <span class="math inline">n-1</span> dummy variables, and input it into a regression equation as follows:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_{1}x_{1} + ... + \beta_{k}x_{n-1} + u_i
</span></p>
<ul>
<li>Where <span class="math inline">\beta_0</span> is the mean of the reference category <span class="math inline">n</span>.</li>
<li>All the other categories <span class="math inline">1, \dots, n-1</span> get their own binary variables <span class="math inline">x_1, \dots, x_{n-1}</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>For example, let us return to the above example of <em>income level</em>.</p>
<ul>
<li><p>This variable has three categories: <em>Low Income (L), Middle Income (M), and High Income (H)</em>.</p></li>
<li><p>We will use <em>Low Income</em> as the reference category, just as previously (so binary variables for only <span class="math inline">x_M</span> middle income and <span class="math inline">x_H</span> high income).</p></li>
</ul>
<p>Let us make the outcome variable <em>crime</em>. So essentially, we want to see the relationship between the income level of a country, and its crime rate. Our regression will take the following form:</p>
<p><span class="math display">
\text{crime}_i = \beta_0 + \beta_1 x_{Mi} + \beta_2 x_{Hi} + u_i
</span></p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="interpreting-categorical-explanatory-variables" class="level1">
<h1>1.6.4: Interpreting Categorical Explanatory Variables</h1>
<p>So we have the categorical explanatory variable regression. Our fitted values model will take the following form:</p>
<p><span class="math display">
\hat y_i = \hat\beta_0 + \hat\beta_{1}x_{1} + ... + \hat\beta_{k}x_{n-1}
</span></p>
<p>How do we interpret the coefficients <span class="math inline">\hat\beta_0, \dots \hat\beta_k</span>?</p>
<p><br></p>
<p>The easiest way is to show these interpretations is with an example, and then generalise to all other categorical variables.</p>
<p>Let us use the same example from the previous section. Our explanatory variable is the income level of a country (categorical: low, medium, or high income), and our outcome variable is crime rate. We established that our regression model takes the form:</p>
<p><span class="math display">
\text{crime}_i = \beta_0 + \beta_1 x_{Mi} + \beta_2 x_{Hi} + u_i
</span></p>
<ul>
<li>Where <span class="math inline">x_M</span> is the binary variable for category <em>middle income</em>, and <span class="math inline">x_H</span> is the binary variable for category <em>high income</em>.</li>
</ul>
<p>So our fitted values regression will take the form:</p>
<p><span class="math display">
\widehat{\text{crime}}_i = \hat\beta_0 + \hat\beta_1x_{Mi} + \hat\beta_2x_{Hi}
</span></p>
<p>Let us calculate the predicted crime rate of each type of country: low income, middle income, and high income.</p>
<p><br></p>
<p>Let us say country <span class="math inline">i</span> is a <u>low income</u> country (reference category). That means they are not a middle income country (<span class="math inline">x_M = 0</span>), and they are not a high income country (<span class="math inline">x_H = 0</span>). Let us plug those in:</p>
<p><span class="math display">
\begin{split}
\widehat{\text{crime}}_i &amp; = \hat\beta_0 + \hat\beta_1x_{Mi} + \hat\beta_2x_{Hi} \\
\widehat{\text{crime}}_i &amp; = \hat\beta_0 + \hat\beta_1(0) + \hat\beta_2(0) \\
\widehat{\text{crime}}_i &amp; = \hat\beta_0 \\
\end{split}
</span></p>
<ul>
<li>Thus, a country in the low income category (reference category) has an expected outcome of <span class="math inline">\hat\beta_0</span>.</li>
</ul>
<p>Now, let us say country <span class="math inline">i</span> is a <u>middle income</u> country. That means they are a middle income country (<span class="math inline">x_M = 1</span>), and they are not a high income country (<span class="math inline">x_H = 0</span>). Let us plug those in:</p>
<p><span class="math display">
\begin{split}
\widehat{\text{crime}}_i &amp; = \hat\beta_0 + \hat\beta_1x_{Mi} + \hat\beta_2x_{Hi} \\
\widehat{\text{crime}}_i &amp; = \hat\beta_0 + \hat\beta_1(1) + \hat\beta_2(0) \\
\widehat{\text{crime}}_i &amp; = \hat\beta_0 + \hat\beta_1\\
\end{split}
</span></p>
<ul>
<li>Thus, a country in the middle income category has an expected outcome of <span class="math inline">\hat\beta_0 + \hat\beta_1</span>.</li>
<li>Thus, <span class="math inline">\hat\beta_1</span> is the <u>difference</u> between the outcome of a middle income country, and a low income country (because low income country is outcome <span class="math inline">\hat\beta_0</span>)</li>
</ul>
<p>Finally, let us say country <span class="math inline">i</span> is a <u>high income</u> country. That means they are not a middle income country (<span class="math inline">x_M = 0</span>), and they are a high income country (<span class="math inline">x_H = 1</span>). Let us plug those in:</p>
<p><span class="math display">
\begin{split}
\widehat{\text{crime}}_i &amp; = \hat\beta_0 + \hat\beta_1x_{Mi} + \hat\beta_2x_{Hi} \\
\widehat{\text{crime}}_i &amp; = \hat\beta_0 + \hat\beta_1(0) + \hat\beta_2(1) \\
\widehat{\text{crime}}_i &amp; = \hat\beta_0 + \hat\beta_2\\
\end{split}
</span></p>
<ul>
<li>Thus, a country in the high income category has an expected outcome of <span class="math inline">\hat\beta_0 + \hat\beta_2</span>.</li>
<li>Thus, <span class="math inline">\hat\beta_2</span> is the <u>difference</u> between the outcome of a high income country, and a low income country (because low income country is outcome <span class="math inline">\hat\beta_0</span>)</li>
</ul>
<p><br></p>
<p>We can generalise these interpretations to all categorical variables.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Categorical Explanatory Variables
</div>
</div>
<div class="callout-body-container callout-body">
<p>Taking what we have seen in the example above, we can interpret all categorical variables.</p>
<ul>
<li>We can see that the reference category (category without a binary variable) has an expected outcome of the intercept <span class="math inline">\hat\beta_0</span>.</li>
<li>We can also see that every other category has an expected outcome of <span class="math inline">\hat\beta_0 + \hat\beta_j</span>, where <span class="math inline">\hat\beta_j</span> is the coefficient multiplied to the category’s binary variable.</li>
<li>That also tells us that <span class="math inline">\hat\beta_j</span> is the <u>difference</u> between the expected outcome of the respective category and the reference category.</li>
</ul>
</div>
</div>
<p><br></p>
<p>In linear regression, we can run a hypothesis test. What does a statistically significant hypothesis test mean in a categorical explanatory variable.</p>
<ul>
<li>A statistically significant <span class="math inline">\hat\beta_j</span> indicates that <u>there is a significant difference in outcomes between the respective category and the reference category</u>.</li>
</ul>
<p>Note that it <u>does not mean that the categorical variable as a whole is statistically significant</u>. It only shows the statistical significance of a difference between a category and the reference category.</p>
<p>In order to test the significance of the relationship between the categorical goal as a whole and the outcome, we have to use a <strong>F-test</strong> (see <a href="https://statsnotes.github.io/intro/5.html#hypothesis-testing-with-more-than-one-coefficient">1.5.6</a>). Our hypotheses for the test will be:</p>
<ul>
<li><span class="math inline">M_0</span>: the regression model <u>without</u> the categorical variable (without any of the binary variables of the categories).</li>
<li><span class="math inline">M_a</span>: the regression model <u>with</u> the categorical variable.</li>
</ul>
<p>If the F-test is statistically significant, than we know that the categorical variable has a statistically significant relationship with the outcome variable.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="the-dummy-variable-trap" class="level1">
<h1>1.9.5: The Dummy Variable Trap</h1>
<p>Notice how so far, when covering binary and categorical variables, we always include one less binary variable than the number of categories.</p>
<ul>
<li><p>For binary variables, we have two categories, but only one variable in our regression.</p></li>
<li><p>For categorical variables, we have <span class="math inline">n</span> categories, but only <span class="math inline">n-1</span> binary variables in our regression.</p></li>
</ul>
<p>Why is this? Obviously, we do not need each category to have its own variable, since we can infer an observation in the final (reference) category is in that category, if it is not a part of any other category.</p>
<p>However, there is another reason why we always drop one category as the reference category: if we do not drop it, the regression does not work.</p>
<ul>
<li>This is because of <strong>multicollinearity</strong>, which is not allowed in linear regression.</li>
<li>Multicollinearity means that no explanatory variable can be <u>perfectly correlated</u> with any other explanatory variable, or correlated with any function of explanatory variables.</li>
</ul>
<p>The reason for this is because, actually, it is not mathematically possible to calculate regression results if there is multicollinearity.</p>
<ul>
<li>If there is multicollinearity, you will have to divide by 0 in the OLS formula.</li>
<li>We will discuss this in sections <a href="https://statsnotes.github.io/metrics/3.html#unbiasedness-of-ols-under-the-gauss-markov-theorem">2.3.2</a> and <a href="https://statsnotes.github.io/metrics/3.html#proof-of-ols-unbiasedness-in-multiple-linear-regression">2.3.4</a>.</li>
</ul>
<p>This issue of including all categories of the categorical variable as binary variables in the regression, is called the dummy variable trap.</p>
<p><br></p>
<p><u>We can always avoid the dummy variable trap by simply dropping one category, and treating it as a reference category.</u></p>
<ul>
<li>Just as we did in the previous section.</li>
</ul>
<p><br></p>
<p>We can actually solve this issue in another way - dropping the constant term.</p>
<p>Let us use the same example from the previous section. Our explanatory variable is the income level of a country (categorical: low (L), medium (M), or high (H) income), and our outcome variable is crime rate. We established that our regression model takes the form:</p>
<p><span class="math display">
\text{crime}_i = \beta_0 + \beta_1 x_{Mi} + \beta_2 x_{Hi} + u_i
</span></p>
<p>We can drop the intercept <span class="math inline">\beta_0</span>, which will allow us to add the reference category into our regression.</p>
<p><span class="math display">
\text{crime}_i = \beta_1x_{Li} + \beta_2 x_{Mi} + \beta_3 x_{Hi} + u_i
</span></p>
<ul>
<li>This slightly changes our interpretations. Now, each <span class="math inline">\beta</span> has the same interpretation - the average outcome in each respective category (if there are other explanatory variables, then, it is the average outcome in each respective category when all other explanatory variables equal 0).</li>
</ul>
<p>This solution is less common. However, it is used when no category makes sense as a reference category.</p>
<ul>
<li><p>For example, in our example, it makes sense to treat one category (low income) as a reference category. This is because we are comparing all other categories against that reference category, and it is meaningful to evaluate how much crime changes in comparison to low income, as our country becomes wealthier.</p></li>
<li><p>However, for other categorical variables, it makes less sense. For example, take the categorical variable <em>US State</em>. Which state should be the reference category? Why should we compare all states to Alaska, or Alabama, or New Hampshire? In this case, no reference category might make more sense.</p></li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="binary-outcomes-and-the-linear-probability-model" class="level1">
<h1>1.9.6: Binary Outcomes and the Linear Probability Model</h1>
<p>So far, we have focused on explanatory variables. What about outcome variables in regression.</p>
<ul>
<li>As we will see, outcome variables are much more trickier than their explanatory variable counterparts.</li>
</ul>
<p>First, let us talk about binary outcome variables, where outcome <span class="math inline">y</span> has two categories: <span class="math inline">y=0</span> and <span class="math inline">y=1</span>. When this is the case, our linear model changes quite a bit. Our typical regression model has looked like this:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_i + \dots + \beta_k x_{ki} +u_i
</span></p>
<p>However, when our outcome variable <span class="math inline">y</span> has categories <span class="math inline">y=0</span> and <span class="math inline">y=1</span>, this no longer makes sense.</p>
<ul>
<li>After all, the linear line <span class="math inline">y=mx+b</span> will produce <span class="math inline">y</span> values that are not exactly <span class="math inline">0</span> and <span class="math inline">1</span>. For example, the line can produce outcomes <span class="math inline">y=0.23</span>, <span class="math inline">y=0.5849</span>, etc.</li>
<li>These decimals do not really make sense as an outcome of <span class="math inline">y</span>, since it can only be <span class="math inline">0</span> or <span class="math inline">1</span>.</li>
</ul>
<p><br></p>
<p>So, instead of our linear model predicting outcome <span class="math inline">y</span>, they actually predict the probability of an observation <span class="math inline">i</span> being in category <span class="math inline">y=1</span>.</p>
<ul>
<li>Why is this the case? It is a little advanced. As we will cover later in <a href="https://statsnotes.github.io/metrics/3.html#ols-and-the-conditional-expectation-function">2.3.7</a>, linear regression <span class="math inline">y</span> is equal to the conditional expectation function <span class="math inline">E(y|x)</span>. We also know that the expectation <span class="math inline">E(y)</span>, if <span class="math inline">y</span> is binary, is the proportion of observations in <span class="math inline">y=1</span>. The proportion of observations in <span class="math inline">y=1</span> is also equal to the probability of picking an observation in category <span class="math inline">y=1</span>, if we are randomly selecting.</li>
</ul>
<p>So basically, our outcome of our regression is no longer the <span class="math inline">y</span> value of observation <span class="math inline">i</span>, but the probability of observation <span class="math inline">i</span> being in category <span class="math inline">y=1</span>. The probability of observation <span class="math inline">i</span> being in category <span class="math inline">y=1</span> is notated <span class="math inline">\pi_i</span> or <span class="math inline">Pr(y_i =1)</span>.</p>
<p><br></p>
<p>This modified model for probabilities is called the linear probability model.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Linear Probability Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>The linear probability model is the form of the linear model, when outcome <span class="math inline">y</span> is binary. The model takes the form:</p>
<p><span class="math display">
\pi_i = \beta_0 + \beta_1x_{1i} + \dots + \beta_kx_{ki}+u_i
</span></p>
<ul>
<li>Where <span class="math inline">\pi_i = Pr(y_i=1)</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>Our fitted values model will now be:</p>
<p><span class="math display">
\hat\pi_i = \hat\beta_0 + \hat\beta_1x_{1i} + \dots + \hat\beta_k x_{ki}
</span></p>
<p>Since our outcome of the linear probability model is now <span class="math inline">\hat\pi_i</span> instead of <span class="math inline">\hat y_i</span>, we will have to update our interpretations.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Linear Probability Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>The coefficients in the linear probability model are interpreted as following:</p>
<ul>
<li><span class="math inline">\hat\beta_j</span>: When <span class="math inline">x_j</span> increases by one unit, the probability of an observation being in category <span class="math inline">y=1</span> increases by <span class="math inline">\hat\beta_j</span>.</li>
<li><span class="math inline">\hat\beta_0</span>: When all explanatory variables equal 0, the probability of an observation being in category <span class="math inline">y=1</span> is equal to <span class="math inline">\hat\beta_0</span>.</li>
</ul>
<p>Hypothesis testing works the same way as in linear regression.</p>
</div>
</div>
<p>We can use all the techniques we have covered previously in the linear probability model, including binary and categorical explanatory variables.</p>
<p><br></p>
<p>There are some issues with the linear probability model, that we will cover in the next section.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="limitations-of-linear-regression-for-non-continuous-outcomes" class="level1">
<h1>1.9.7: Limitations of Linear Regression for Non-Continuous Outcomes</h1>
<p>There are many limitations of linear regression when it comes to binary, categorical, and ordinal outcome variables.</p>
<ul>
<li>This section will cover these limitations. We start with binary outcomes, then talk about categorical outcomes, before finishing with ordinal outcomes.</li>
</ul>
<p><br></p>
<section id="limitations-of-the-linear-probability-model" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-the-linear-probability-model">Limitations of the Linear Probability Model</h3>
<p>Let us start with binary outcome variables. We discussed how the linear probability model can be used to deal with binary outcomes <span class="math inline">y</span>.</p>
<p>However, the linear probability model has a huge weakness - impossible predictions.</p>
<ul>
<li>We know by the laws of probability, that any probability must be between 0 and 1.</li>
<li>However, a linear straight line (like the best-fit line in the linear probability model) goes from <span class="math inline">±∞</span> to <span class="math inline">±∞</span>.</li>
<li>That means that it is possible to get fitted outcomes <span class="math inline">\hat \pi_i</span> that are above 1 or below 0. This makes no sense.</li>
</ul>
<p>That means that the linear probability model may often make nonsensical predictions. This is a pretty big issue with the model.</p>
<ul>
<li>There are other issues, including inference issues, that we will cover in more detail in <a href="https://statsnotes.github.io/intro/8.html">lesson 1.8</a>..</li>
</ul>
<p>The most common solution to this problem is an alternative model: the logistic regression model.</p>
<ul>
<li>We will cover this model in <a href="https://statsnotes.github.io/intro/8.html">lesson 1.8</a>.</li>
<li>But simply speaking, it applies a “link function” which modifies the linear model so that it never goes beyond 0 or 1 in predictions.</li>
</ul>
<p>Since it addresses the main issues with the linear probability model, the logistic model is generally the preferred method of modelling binary <span class="math inline">y</span> outcomes, not the linear probability model introduced in this lesson.</p>
<ul>
<li>This is especially the case in prediction, where the linear probability model is almost never used.</li>
<li>However, the linear probability model is still used, especially in causal inference settings where the predictive outcomes are not the main concern.</li>
</ul>
<p><br></p>
</section>
<section id="limitations-relating-to-categorical-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="limitations-relating-to-categorical-outcomes">Limitations Relating to Categorical Outcomes</h3>
<p>If we have a categorical outcome variable <span class="math inline">y</span> with more than 2 categories, with categories having no natural order, the linear regression model <u>is never okay to use</u>.</p>
<p>The reason for this is quite simple - the linear regression model outputs numerical outcomes, such as <span class="math inline">y_i</span> or <span class="math inline">\pi_i</span>. Numerical outcomes always imply order - 2 is always greater than 1, 3 is always greater than 2, etc.</p>
<p>But if categorical outcome variables have no natural order, they make no sense to use in a regression model that only outputs results with order.</p>
<p>To address this issue, we will use the Multinomial Logistic Regression model, which will be introduced in <a href="https://statsnotes.github.io/intro/9.html">lesson 1.9</a>.</p>
<p><br></p>
</section>
<section id="limitations-relating-to-ordinal-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="limitations-relating-to-ordinal-outcomes">Limitations Relating to Ordinal Outcomes</h3>
<p>Ordinal variables can be used in linear regression - we just pretend they are continuous outcomes.</p>
<ul>
<li>For example, if our ordinal variable has categories <em>disagree, neutral, and agree</em>, we can just treat them as 0, 1, and 2, and pretend they are a continuous variable.</li>
</ul>
<p>This generally works without too many issues.</p>
<p>However, there is a limitation regarding ordinal variables in linear regression. Our output, <span class="math inline">y_i</span>, will tell us about the expected outcome given the values of our explanatory variables. However, they will not tell us the probabilities of being in each category.</p>
<p>For example, take the same ordinal variable as before with categories <em>disagree, neutral, and agree</em>, which we will label as 0, 1, and 2.</p>
<ul>
<li><p>The output <span class="math inline">y_i</span> will tell us the expected level of agreement given some explanatory variable <span class="math inline">x</span> values (like 1.3 for example).</p></li>
<li><p>However, it will not tell us the probability of a random person with certain <span class="math inline">x</span> values of being in each category of <em>disagree, neutral, and agree</em>. It only tells us the average outcome, not the individual probabilities of each category.</p></li>
</ul>
<p>The Ordinal Logistic Model, that will be discussed in <a href="https://statsnotes.github.io/intro/9.html">lesson 1.9</a>, will allow us to predict the individual category probabilities.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="implementation-in-r" class="level1">
<h1>Implementation in R</h1>
<p>To run linear regressions with binary/categorical variables, the syntax is the exact same as linear regression covered in the last 2 lessons.</p>
<p>The only thing we have to make sure is to ensure the binary/categorical variables we want to use are properly marked in R as binary/categorical variables, and not numeric variables.</p>
<ul>
<li>This is especially the case in many datasets, where categorical variables are coded in numbers, but they are not numeric variables and need to be treated as categorical variables.</li>
</ul>
<p><br></p>
<p>R calls categorical/binary variables <strong>factor variables</strong>.</p>
<p>R automatically treats double (true/false) and string (text) variables as factor variables, so we do not need to worry about this if our variables are already coded as such.</p>
<p>If our categorical variable is a numeric variable, we will need to convert it into a factor variable. This is simply done with the <em>as.factor()</em> function.</p>
<p><br></p>
<p>We can do this in the data cleaning stage, as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>variable <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(variable)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Replace <em>variable</em> with the name of your variable.</li>
</ul>
<p><br></p>
<p>We can also directly do this in a regression <em>feols()</em> (or <em>lm</em>) function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">feols</span>(y <span class="sc">|</span> x1 <span class="sc">+</span> <span class="fu">as.factor</span>(x2) <span class="sc">+</span> x3, <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>In this example, variable <em>x2</em> is being factored, while all the other variables are not.</li>
<li>Simply put <em>as.factor()</em> around the variable you want to make categorical.</li>
</ul>
<p><br></p>
<p>We talked about reference categories in this lesson. R will automatically choose a reference category when you factor it (typically the first or last category). However, you can manually change this reference category as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>variable <span class="ot">&lt;-</span> <span class="fu">relevel</span>(variable, <span class="at">ref =</span> <span class="st">"reference category name"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Replace <em>variable</em> with the name of your variable.</li>
<li>Replace <em>reference category name</em> with the name of the reference category you want to use.</li>
</ul>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>