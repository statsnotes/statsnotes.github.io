---
title: "Statistics for Social Scientists"
subtitle: "Lesson 1.7: Interactions and Functional Form"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Lesson 1.7: Interactions and Functional Form"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

::: callout-note
## Lesson Overview

This lesson further expands our regression toolkit, by introducing both interaction effects, and non-linear transformations of variables.

This lesson covers the following topics:

-   Heterogeneity in relationships and motivation of interaction effects.
-   How interaction effects work.
-   Polynomial and Logarithmic transformations of variables.
:::

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.1: Heterogeneity and Interactions

So far we have discussed the standard linear model:

$$
y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i
$$

The standard linear model assumes that the relationship of any explanatory variable $x_j$ with $y$ is always $\beta_j$, across every observation of $i$.

-   Essentially, it says that the relationship is **constant** across all units.

However, this might not always be the case. When the relationship between two variables is not constant across all units, there are **heterogenous effects**.

::: callout-tip
## Definition: Heterogenous Effects

Heterogenous effects are relationships between variables, that differ in magnitude between units, often due to the value of some third characteristic.

A few examples of heterogenous effects include:

-   The relationship between *education* and *income* may vary in magnitude depending on the *age* of the unit.
-   The relationship between *minimum wage* and *unemployment* may vary in magnitude between *rural and urban areas*.
-   The relationship between receiving *a patent* and *sales growth of a company* may differ depending on the *age of the firm*.

In all these examples, the magnitude of the relationship between two variables changes and depends on some third characteristic.
:::

<br />

Heterogenous effects are very common. We might want some way to incorporate them into regressions. Why?

-   First of all, incorporating a heterogeneous effect into a regression allows us to use hypothesis testing to test if there is statistically significant evidence of a heterogeneous effect.
-   Second of all, if there indeed is a statistically significant heterogenous effect, incorporating it in a regression can improve our prediction and estimation of our regression.

<br />

To incorporate heterogenous effects into regression, we do what is called an interaction effect.

::: callout-tip
## Definition: Interaction Effect

An interaction effect is a way to incorporate heterogeneity into regressions. It takes the form of two explanatory variables being multiplied together.

For example, if we want to test how the relationship between $x_1$ and $y$ changes with different values of $x_2$, our regression would be of the form:

$$
y_i = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2 + u_i
$$

-   Where $\beta_3 x_1 x_2$ is the interaction term.
-   Note: when we include an interaction term $\beta_3 x_1 x_2$, we must also include the independent terms of each variable in the regression, as we see above with $\beta_1 x_1$ and $\beta_2 x_2$.
:::

Naturally, our fitted interaction model after estimating parameters will be:

$$
\hat y = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_1x_2
$$

-   If our statistical hypothesis test of the interaction term coefficient $\hat\beta_3$ is significant, then we can be confident that there is a heterogenous effect in our regression, where the magnitude of effect of $x_1$ on $y$ depends on $x_2$.

Now that we know what an interaction term is, we will explore how interaction terms work and are interpreted for different types of interactions.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.2: Interactions with Two Dummies

Let us consider the simplest form of interactions. Let us say we have the fitted interaction model:

$$
\hat y_i = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_1x_2
$$

Where $x_1$ and $x_2$ are both binary (dummy) variables, with categories 0 and 1. How would we interpret our coefficients?

We can "calculate" the interpretation of coefficients by plugging in and testing the four possible combinations of $x_1$ and $x_2$:

1.  $x_1 = 0$, $x_2 = 0$.
2.  $x_1 = 1$, $x_2 = 0$.
3.  $x_1 = 0$, $x_2 = 1$.
4.  $x_1 = 1$, $x_2 = 1$.

<br />

Let us test the scenario where both $x_1$ and $x_2$ equal 0. Let us plug in $x_1 = 0$ and $x_2 = 0$ into our fitted values:

$$
\begin{split}
\hat y & = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_1x_2 \\
& = \hat\beta_0 + \hat\beta_1(0) + \hat\beta_2(0) + \hat\beta_3(0)(0) \\
& = \hat\beta_0
\end{split}
$$

Let us test the scenario where $x_1 = 1$ and $x_2 = 0$:

$$
\begin{split}
\hat y & = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_1x_2 \\
& = \hat\beta_0 + \hat\beta_1(1) + \hat\beta_2(0) + \hat\beta_3(1)(0) \\
& = \hat\beta_0 + \hat\beta_1
\end{split}
$$

Let us test the scenario where $x_1 = 0$ and $x_2 = 1$:

$$
\begin{split}
\hat y & = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_1x_2 \\
& = \hat\beta_0 + \hat\beta_1(0) + \hat\beta_2(1) + \hat\beta_3(0)(1) \\
& = \hat\beta_0 + \hat\beta_2
\end{split}
$$

Let us test the scenario where $x_1 = 1$ and $x_2 = 1$:

$$
\begin{split}
\hat y & = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_1x_2 \\
& = \hat\beta_0 + \hat\beta_1(1) + \hat\beta_2(1) + \hat\beta_3(1)(1) \\
& = \hat\beta_0 + \hat\beta_1 + \hat\beta_2 + \hat\beta_3
\end{split}
$$

<br />

What have we learned from this? Look at the first two scenarios. Going from $x_1 = 0$ and $x_2 = 0$ to $x_1 = 1$ and $x_2 = 0$, we go from expected $\hat\beta_0$ to $\hat\beta_0 + \hat\beta_1$.

-   The difference between the two scenarios is $\hat\beta_1$
-   This tells us that holding $x_2 = 0$ constant, and changing $x_1$ from $0$ to $1$, the change in $y$ is $\hat\beta_1$.

Now look at the last two scenarios. Going from $x_1 = 0$ and $x_2 = 1$ to $x_1 = 1$ and $x_2 = 1$, we go from expected $\hat\beta_0 + \hat\beta_2$ to $\hat\beta_0 + \hat\beta_1 + \hat\beta_2 + \hat\beta_3$.

-   The difference between the two scenarios is $\hat\beta_1 + \hat\beta_3$.
-   This tells us that holding $x_2 = 1$ constant, and changing $x_1$ from 0 to 1, the change in $y$ is $\hat\beta_1 + \hat\beta_3$.

Summarising this, we can come up with the following interpretations:

::: callout-tip
## Interpretation of Interaction of Two Dummies

The effect of $x_1$ depends on $x_2$ as follows:

-   When $x_2=0$, the effect of $x_1$ (going from 0 to 1) on $y$ is $\hat\beta_1$.
-   When $x_2 = 1$, the effect of $x_1$ (going from 0 to 1) on $y$ is $\hat\beta_1 + \hat\beta_3$.

Thus, if our hypothesis test says that $\hat\beta_3 â‰  0$, then we know that the relationship between $x_1$ and $y$ depends on the value of $x_2$, and thus, $x_1$ has a heterogenous effect on $y$.
:::

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.3: Interactions with One Dummy

Now, let us consider another scenario. Let us say we have the fitted interaction model:

$$
\hat y_i = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_1x_2
$$

Where $x_1$ is a continuous variable, and $x_2$ is a binary (dummy) variables, with categories 0 and 1. How would we interpret our coefficients?

-   This is probably the most common scenario. Examples include how years of education ( $x_1$ ) effects income ( $y$ ), depending on gender ( $x_2$ ).
-   Other examples include any continuous $x_1$ relationship on $y$ depending on $x_2$ being gender, developed/developing, treated/untreated, majority/minority, etc.

<br />

We can solve for the interpretations in a slightly different way. We know we have two scenarios of $x_2$: either $x_2 = 0$, or $x_2 = 1$. Let us plug in those values to see how our regression changes.

Let us say $x_2 = 0$. Let us plug that in:

$$
\begin{split}
\hat y & = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_1x_2 \\
& = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2(0) + \hat\beta_3x_1(0) \\
& = \hat\beta_0 + \hat\beta_1x_1
\end{split}
$$

Now, let us say $x_2 = 1$. Let us plug that in:

$$
\begin{split}
\hat y & = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_1x_2 \\
& = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2(1) + \hat\beta_3x_1(1) \\
& = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2 + \hat\beta_3x_1
\end{split}
$$

Let us reorganise that final equation a little bit into a more "useful" format:

$$
\begin{split}
\hat y & = \hat\beta_0 + \hat\beta_1 x_1 + \hat\beta_2 + \hat\beta_3x_1 \\
& = \hat\beta_0 + \hat\beta_2 + \hat\beta_1 x_1 + \hat\beta_3x_1 \\
& = \hat\beta_0 + \hat\beta_2 + (\hat\beta_1 + \hat\beta_3)x_1
\end{split}
$$

<br />

So now, we have two regression equations for the two potential values of $x_2$:

$$
\begin{split}
\hat y & = \hat\beta_0 + \hat\beta_1 x_1 \quad \text{for }x_2 = 0 \\
\hat y & = \hat\beta_0 + \hat\beta_2 + (\hat\beta_1 + \hat\beta_3)x_1 \quad \text{for }x_2 = 1 \\
\end{split}
$$

-   We can see for the first equation when $x_2 = 0$, the "slope" of the equation if $\hat\beta_1$.
-   For the second equation when $x_2 = 1$, the "slope" of the equation is $\hat\beta_1 + \hat\beta_3$.

We know the slope is the relationship between $x_1$ and $y$. Thus, we can interpret our results as follows:

::: callout-tip
## Interpretation of Interaction with One Dummy

The effect of $x_1$ depends on $x_2$ as follows:

-   When $x_2 = 0$, for every increase of one unit in $x_1$, there is a $\hat\beta_1$ expected change in $y$.

-   When $x_2 = 1$, for every increase of one unit in $x_1$, there is a $\hat\beta_1 + \hat\beta_3$ expected change in $y$.

Thus, if our hypothesis test says that $\hat\beta_3 â‰  0$, then we know that the relationship between $x_1$ and $y$ depends on the value of $x_2$, and thus, $x_1$ has a heterogenous effect on $y$.
:::

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.4: Interactions with Continuous Variables

Now, let us consider the most complex scenario. Let us say we have the fitted interaction model:

$$
\hat y_i = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_1x_2
$$

Where $x_1$ and $x_2$ are both continuous variables How would we interpret our coefficients?

<br />

We can no longer just plug in values of $x_2$, since now that $x_2$ is continuous, there are infinite possible values of $x_2$.

However, we can still find the effect of $x_1$ on $y$ through a **partial derivative**.

-   We know that the partial derivative of $\hat y$ in respect to $x_1$ will be the effect of $x_1$ on $y$, holding $x_2$ constant.

Thus, let us find the partial derivative of $\hat y$ in respect to $x_1$:

$$
\begin{split}
\frac{\partial \hat y_i}{\partial x_1} & = \frac{\partial}{\partial x_1} \left[ \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_1x_2 \right] \\
& = \frac{\partial}{\partial x_1}[\hat\beta_0] + \frac{\partial}{\partial x_1}[\hat\beta_1 x_1] + \frac{\partial}{\partial x_1}[\hat\beta_2x_2] + \frac{\partial}{\partial x_1}[\hat\beta_3x_1x_2] \\
& = 0 + \hat\beta_1 + 0 + \hat\beta_3x_2 \\
& = \hat\beta_1 + \hat\beta_3x_2
\end{split}
$$

Thus, the relationship between $x_1$ and $y$ is $\hat\beta_1 + \hat\beta_3 x_2$. Since $x_2$ is a part of the equation, if $\hat\beta_3 â‰  0$, then $x_2$ will effect the magnitude of the relationship between $x_1$ and $y$.

-   We can also interpret $\hat\beta_1$. When (and only when) $x_2 = 0$, the second term dissappears, and the relationship of $x_1$ and $y$ is $\hat\beta_1$.

::: callout-tip
## Interpretation of Interactions with Continuous Variables

For every increase of one unit in $x_1$, there is an expected $\hat\beta_1 + \hat\beta_3 x_2$ change in $y$.

-   $\hat\beta_1$ is the relationship between $x_1$ and $y$ given $x_2 = 0$.

Thus, if our hypothesis test says that $\hat\beta_3 â‰  0$, then we know that the relationship between $x_1$ and $y$ depends on the value of $x_2$, and thus, $x_1$ has a heterogenous effect on $y$.
:::

<br />

An example of this would be how the effect of economic development (GDP) on democratisation, may depend on the oil income of a country.

-   This is because, in political science, there generally is some consensus that higher GDP increases the change of a country being a democracy.
-   However, we also see many fabulously oil-rich gulf states seemingly buck this trend.

Let us assume democracy is measured on a continuous scale, GDP is a continuous variable, and oil income is a continuous variable. This relationship could be expressed with an interaction effect:

$$
\widehat{\text{democracy}}_i = \hat\beta_0 + \hat\beta_1 \text{gdp}_i + \hat\beta_2 \text{oil}_i + \hat\beta_3 \text{gdp}_i \text{oil}_i
$$

Given our interpretations, we know that:

-   For every one unit increase in GDP, there is an expected $\hat\beta_1 + \hat\beta_3 \text{oil}_i$ change in the democracy level of a country.
-   $\hat\beta_1$ is the effect of one unit increase in GDP on the level of democracy, given that oil incomes are 0.

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.5: Polynomial Transformations

Take a look at this relationship between literacy rate and infant mortality rate:

![](images/clipboard-767594843.png){width="100%"}

Which line seems like a more accurate prediction of the relationship between literacy rate and infant mortality rate - the linear straight line, or the curved blue line?

It seems as this relationship is better described with the blue line. This brings up the idea that not all relationships are linear - a straight line often is not the best description of relationships between variables.

This presents an issue for us - after all, our current linear regression model assumes that the relationship between $x$ and $y$ is linear:

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

What we can do is fit non-linear relationships. One of the most common ways is to use polynomial relationships. Just a quick mathematics reminder, we know that polynomial relationships are of the following sort:

-   Quadratic: $ax^2 + bx + c$
-   Cubic: $ax^3 + bx^2 + cx +d$
-   Quartic: $ax^4+bx^3+cx^2+dx+e$

<br />

The most commonly used polynomial relationship used is the quadratic transformation:

::: callout-tip
## Quadratic Transformation

The quadratic transformation of regression takes the following form:

$$
y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2 + u_i
$$

-   Note: we must always include all lower degree terms in our regression model. If we have $x^2$, we must also have $x$. This also applies to any other polynomial transformation.
-   We can also include other control variables as normal.
:::

After estimation with OLS, we get the fitted values equation:

$$
\hat y_i = \hat\beta_0 + \hat\beta_1 x_i + \hat\beta_2x_i^2
$$

<br />

How do we interpret these coefficients? Unfortunately, it is not as straight forward.

-   $\hat\beta_0$ maintains its intercept interpretation - still the expected value of $y$ when $x = 0$.
-   $\hat\beta_1$ and $\hat\beta_2$ are not interpretable in magnitude. This is because we cannot "hold the other term constant" like we did in multiple regression, because both terms contain the variable $x$, so changing one term will always change the other term.

Most usefully, $\hat\beta_2$ in a hypothesis test has the null hypothesis of $H_0: \beta_2 = 0$.

-   That means, if we have a statistically significant $\hat\beta_2$, we can reject that $\beta_2 = 0$.
-   What does $\beta_2 = 0$ mean? Well it means the $x^2$ term becomes 0, and we are left with a linear regression.

[Thus, if $\hat\beta_2$ is statistically significant, we can conclude there is a statistically significant non-linear relationship.]{.underline}

<br />

There are a few more things we can interpret.

First of all, we know that from mathematics, in the quadratic $ax^2 + bx + c$, if $a$ is positive, the parabola opens up, and if $a$ is negative, the parabola opens down.

In our regression, we know that $\hat\beta_2$ plays the $a$ role. Thus, if $\hat\beta_2$ is positive, we have a upward opening best-fit parabola, and when $\hat\beta_2$ is negative, we have a downward opening best-fit parabola.

<br />

Second, we can still find the effect of $x$ on $y$ through finding the derivative of $\hat y$ in respect to $x$:

$$
\begin{split}
\frac{d\hat y}{dx} & = \frac{d}{dx} \left[ \hat\beta_0 + \hat\beta_1 x_i + \hat\beta_2x_i^2 \right] \\
& = \frac{d}{dx}[\hat\beta_0] + \frac{d}{dx}[\hat\beta_1x_i] + \frac{d}{dx}[\hat\beta_2x_i^2] \\
& = \frac{d}{dx}[\hat\beta_0] + \frac{d}{dx}[\hat\beta_1x_i] + \hat\beta_2\frac{d}{dx}[x_i^2] \\
& = 0 + \hat\beta_1 + 2 \hat\beta_2x_i \\
& = \hat\beta_1 + 2 \hat\beta_2x_i
\end{split}
$$

Thus, for every one unit increase in $x$, the expected value of $y$ changes by $\hat\beta_1 + 2 \hat\beta_2x_i$.

<br />

Finally, we can find the *vertex* of the parabola. If we remember our mathematics, the vertex is when the parabola changes directions.

-   Or in other words, where the parabola has its maximum/minimum value.

We know we can find the max/min of a function through setting its derivative equal to 0, and solving for $x$. So, let us do that:

$$
\begin{split}
0 & = \frac{d\hat y}{dx} \\
0 & = \hat\beta_1 + 2\hat\beta_2x_i \\
-\hat\beta_1 & = 2 \hat\beta_2x_i \\
x_i & =  - \hat\beta_1/2\hat\beta_2
\end{split}
$$

So, we know the minimum/maximum point of the best-fit parabola occurs at $-\hat\beta_1/ 2 \hat\beta_2$. What is the significance of this?

-   Well, that is the $x$ value that either gets the maximum or minimum expected $y$. That could have some significance.
-   For example, if we are predicting how education $x$ effects income $y$, the maximum point of $y$ (most income) will indicate the optimal level of education, before more education reduces incomes.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.6: Logarithmic Transformations

Another very common transformation used on variables is the **logarithmic transformation**. These transformations are used for primarily three purposes:

1.  Just like polynomial transformations, they can be used to model non-linear relationships.
2.  Log transformation can also reduce heteroscedasticity in data, making it more homoscedastic. This is a little bit complicated, and we will discuss this more in [2.4.1](https://statsnotes.github.io/metrics/4.html#gauss-markov-and-homoscedasticity). For now, just know that it can help reduce the standard errors in our hypothesis testing.
3.  Log transformations can reduce the impact of outliers - which is useful if we do not want influential outliers having an outsized impact on our findings.

In particular, logarithmic transformations are used commonly when either our $x$ variables or $y$ variable has a heavily skewed distribution, which can cause non-linearity and strong outliers.

-   In fact, taking the log of a variable that is heavily skewed often results in a near-normal distribution.

The most common logarithmic transformation is called the linear-log regression, which is used for when our explanatory variable(s) are heavily skewed:

::: callout-tip
## Definition: Linear-Log Regression

The linear-log regression transformation takes the following form:

$$
y_i = \beta_0 + \beta_1 \log(x_i) + u_i
$$

-   Note: in statistics, when we say $\log(x_i)$, we mean with base $e$, such that $\log(x_i) = \ln (x_i)$.
:::

More simply, we take our explanatory variable $x$, and find $\log (x_i)$ for every observation within $x$. Then, we use the $\log (x_i)$ as our new explanatory variable.

<br />

Having an explanatory variable transformed with a logarithm will change our interpretations. First, our fitted values will take the following form:

$$
\hat y_i = \hat\beta_0 + \hat\beta_1 \log(x_i)
$$

We could interpret the linear-log regression coefficient $\beta_1$ simply as the expected change in $y$ for every one unit increase in $\log (x_i)$. However, that last part - a one unit in $\log (x_i)$, is really not meaningful. What is a one unit increase in $\log(\text{income}_i)$, or $\log(\text{education}_i)$?

Luckily, there is another more useful way to interpret this - by using a few properties of logarithms.

::: callout-note
## Useful Properties of Logarithms

A very useful property of logarithms is shown below:

$$
\log(e^ax) = \log (x) + \log (e^a) = \log (x) + a
$$
:::

We know that when $x_i = x$ (any value of $x$), $\hat y_i$ will equal:

$$
\begin{split}
\hat y_{i , x_i = x} & = \hat\beta_0 + \hat\beta_1 \log (x_i) \\
& = \hat\beta_0 + \hat\beta_1 \log(x)
\end{split}
$$

And with the property above, let us find our predicted value of $\hat y_i$, given an $x_i$ value of $e^a x$:

$$
\begin{split}
\hat y_{i, \ x_i = e^a x} & = \hat\beta_0 + \hat\beta_1\log(x_i) \\
& = \hat\beta_0 + \hat\beta_1 \log(e^ax) \\
& = \hat\beta_0 + \hat\beta_1(\log(x) + a) \\
& = \hat\beta_0 + \hat\beta_1 \log(x) + \hat\beta_1 a
\end{split}
$$

Now, let us find the difference in predicted $\hat y_i$, between when $x_i = e^a x$ and $x_i = x$ (or in other words, the change in $\hat y_i$ when we multiply $x$ by $e^a$):

$$
\begin{split}
\hat y_{i, \ x_i = e^a x} - \hat y_{i, \ x_i = x} & = \hat\beta_0 + \hat\beta_1 \log(x) + \hat\beta_1 a - (\hat\beta_0 + \hat\beta_1 \log(x)) \\
& = \hat\beta_0 + \hat\beta_1\log(x) + \hat\beta_1a - \hat\beta_0 - \hat\beta_1 \log(x) \\
& = \hat\beta_0 - \hat\beta_0 + \hat\beta_1\log(x) - \hat\beta_1 \log(x) + \hat\beta_1 a \\
& = \hat\beta_1 a
\end{split}
$$

This, this shows as that when we multiply our $x$ value by $e^a$, we will get an expected $\beta_1 a$ change in $y$.

But why is that useful at all? Well, we can choose some value of $a$ that can make this easy to interpret.

-   For example, if $a = 0.095$, then $e^a \approx 1.1$.
-   That means that when we multiply $x$ by 1.1, we will get an expected $0.095 \hat\beta_1$ change in $y$.
-   But what is multiplying $x$ by 1.1? Well, it means a 10% increase in $x$.

Thus, a 10% increase in $x$ is associated with an expected $0.095 \hat\beta_1$ change in $y$.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.7: Extensions of Logarithmic Transformations

In the last section, we discussed the linear-log regression, the most common form of logarithmic transformation. However, there are two other possible logarithmic transformations: log-linear, and log-log.

<br />

### Log-Linear Regression

The log-linear regression is similar to the previously covered linear-log regression, but instead of logging the $x$ variable, we now log the $y$ variable. This is useful when our $y$ variable is skewed or has outliers.

::: callout-tip
## Definition: Log-Linear Regression

The log-linear regression takes the following form:

$$
\log(y_i) = \beta_0 + \beta_1x_i + u_i
$$
:::

Our fitted values of a log-linear regression will be:

$$
\log(\hat y_i) = \hat\beta_0 + \hat\beta_1x_i
$$

How do we interpret the coefficient $\hat\beta_1$?

We can do the simple interpretation, which is that a one unit increase in $x$ is associated with a $\hat\beta_1$ change in $\log(\hat y_i)$. However, just as we discussed for linear-log regression, $\log(\hat y_i)$ is not very meaningful in terms of interpretation.

Just like linear-log regression, we can use a few properties of logarithms to get a more useful interpretation.

::: callout-note
## Useful Property of Logarithms

A useful property of logarithms is that $e^{\log a} = a$.
:::

So we do not want to interpret in terms of $\log (\hat y_i)$, but in terms of the variable $\hat y_i$ itself. How do we "un-log" $\log (\hat y_i)$? The property above shows that taking $e$ to the $\log (\hat y_i)$ power can isolate $\hat y_i$:

$$
e^{\log (\hat y_i)} = \hat y_i
$$

But, if we do this to $\log (\hat y_i)$, we also have to do it to the other side of our fitted values equation. Thus:

$$
\begin{split}
& \log(\hat y_i) = \hat\beta_0 + \hat\beta_1x_i \\
& e^{\log (\hat y_i)} = e^{\hat\beta_0 + \hat\beta_1 x_i} \\
& \hat y_i = e^{\hat\beta_0}e^{\hat\beta_1x_i}
\end{split}
$$

So now, with this equation, what does a one unit increase in $x$ result in? Well, let us plug in $x_i = x+1$, and $x_i = x$, and find the ratio (multiplicative change):

$$
\begin{split}
\frac{\hat y_{i, \ x_i = x+1}}{ \hat y_{i, \ x_i = x}} & = \frac{e^{\hat\beta_0}e^{\hat\beta_1(x+1)}}{  e^{\hat\beta_0}e^{\hat\beta_1x}} \\
& = \frac{e^{\hat\beta_0}e^{\hat\beta_1x +\hat\beta_1}} {e^{\hat\beta_0}e^{\hat\beta_1x}} \\
& = \frac{e^{\hat\beta_0}e^{\hat\beta_1 x}e^{\hat\beta_1}}{e^{\hat\beta_0}e^{\hat\beta_1x}} \\
& = e^{\hat\beta_1}
\end{split}
$$

Thus, we can see, that with every one unit increase in $x$, our expected $\hat y_i$ is multiplied by $e^{\hat\beta_1}$.

-   So, if $e^{\hat\beta_1} > 1$, then $y$ increases by some percent when $x$ increases by one unit.
-   If $e^{\hat\beta_1} < 1$, then $y$ decreases by some percent when $x$ increases by one unit.
-   And if $e^{\hat\beta_1} = 1$, then $y$ does not change when $x$ increases by one unit.

<br />

### Log-Log Regression

A log-log regression is when both the outcome $y$ and explanatory variable $x$ are logged.

::: callout-tip
## Definition: Log-Log Regression

The log-log regression takes the following form:

$$
\log (y_i) = \beta_0 + \beta_1 \log (x_i) + u_i
$$
:::

Our fitted values will take the following form:

$$
\log (\hat y_i) = \hat\beta_0 + \hat\beta_1 \log(x_i)
$$

Our interpretation will combine the previous log-linear and linear-log interpretations.

When we multiply $x$ by $e$, the expected value of $y$ will multiply by $e^{\hat\beta_1}$. However, this is not that inuitive, since the mathematical constant $e = 2.718$ is not a very meaningful number.

We can also get a proportional change interpretation: For a $p$ percent increase in $x$, the expected value of $y$ is equal to $e^{\hat\beta_1(\log((100+p)/100)}$.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)
