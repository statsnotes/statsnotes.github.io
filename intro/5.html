<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistics for Social Scientists</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="5_files/libs/clipboard/clipboard.min.js"></script>
<script src="5_files/libs/quarto-html/quarto.js"></script>
<script src="5_files/libs/quarto-html/popper.min.js"></script>
<script src="5_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="5_files/libs/quarto-html/anchor.min.js"></script>
<link href="5_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="5_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="5_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="5_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="5_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Statistics for Social Scientists</h1>
            <p class="subtitle lead">Lesson 1.5: Multiple Linear Regression</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Lesson 1.5: Multiple Linear Regression</h2>
   
  <ul class="collapse">
  <li><a href="#the-multiple-linear-regression-model" id="toc-the-multiple-linear-regression-model" class="nav-link active" data-scroll-target="#the-multiple-linear-regression-model">1.5.1: The Multiple Linear Regression Model</a></li>
  <li><a href="#interpretation-of-coefficients" id="toc-interpretation-of-coefficients" class="nav-link" data-scroll-target="#interpretation-of-coefficients">1.5.2: Interpretation of Coefficients</a></li>
  <li><a href="#omitted-variable-bias-and-confounding-variables" id="toc-omitted-variable-bias-and-confounding-variables" class="nav-link" data-scroll-target="#omitted-variable-bias-and-confounding-variables">1.5.3: Omitted Variable Bias and Confounding Variables</a></li>
  <li><a href="#regression-goodness-of-fit-statistics" id="toc-regression-goodness-of-fit-statistics" class="nav-link" data-scroll-target="#regression-goodness-of-fit-statistics">1.5.4: Regression Goodness-of-Fit Statistics</a></li>
  <li><a href="#inference-and-hypothesis-testing" id="toc-inference-and-hypothesis-testing" class="nav-link" data-scroll-target="#inference-and-hypothesis-testing">1.5.5: Inference and Hypothesis Testing</a></li>
  <li><a href="#hypothesis-testing-with-more-than-one-coefficient" id="toc-hypothesis-testing-with-more-than-one-coefficient" class="nav-link" data-scroll-target="#hypothesis-testing-with-more-than-one-coefficient">1.5.6: Hypothesis Testing with More than One Coefficient</a></li>
  <li><a href="#prediction-with-regression" id="toc-prediction-with-regression" class="nav-link" data-scroll-target="#prediction-with-regression">1.5.7: Prediction with Regression</a></li>
  <li><a href="#implementation-in-r" id="toc-implementation-in-r" class="nav-link" data-scroll-target="#implementation-in-r">Implementation in R</a>
  <ul class="collapse">
  <li><a href="#regression-estimation" id="toc-regression-estimation" class="nav-link" data-scroll-target="#regression-estimation">Regression Estimation</a></li>
  <li><a href="#confidence-intervals-1" id="toc-confidence-intervals-1" class="nav-link" data-scroll-target="#confidence-intervals-1">Confidence Intervals</a></li>
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction">Prediction</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the last lesson, we focused on the simple linear regression model. Now, we will expand the number of explanatory variables to get the multiple linear regression model, and we will discuss the advantages of this method over simple linear regression.</p>
<p>This lesson covers the following topics:</p>
<ul>
<li>The basics of multiple linear regression, and how to interpret coefficients.</li>
<li>The motivation behind multiple linear regression - omitted variable bias.</li>
<li>How to conduct statistical inference with multiple linear regression.</li>
<li>Prediction with multiple linear regression.</li>
</ul>
</div>
</div>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
<section id="the-multiple-linear-regression-model" class="level1">
<h1>1.5.1: The Multiple Linear Regression Model</h1>
<p>Multiple linear regression is an extension of simple linear regression, which allows us to include more than one explanatory variable.</p>
<ul>
<li>We will discuss why we would want to include extra explanatory variables later in section 1.5.3. For now, let us first introduce the basics of the model.</li>
</ul>
<p>The <strong>response variable</strong> (outcome variable) is notated <span class="math inline">y</span>, just like in single linear regression.</p>
<p>The <strong>explanatory variable</strong>s are <span class="math inline">x_1, x_2, ..., x_k</span>. We sometimes also denote all explanatory variables as the vector <span class="math inline">\overrightarrow{x}</span>.</p>
<ul>
<li><span class="math inline">k</span> represents the total number of explanatory variables.</li>
<li>Note: if you see the notation <span class="math inline">x_j</span>, that means any explanatory variable <span class="math inline">x_1, \dots , x_k</span>. The variable <span class="math inline">x_j</span> represents any individual coefficient (for generalisation purposes).</li>
</ul>
<p><br></p>
<p>The multiple linear regression takes the following form:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>Take a set of observed data with <span class="math inline">n</span> number of pairs of <span class="math inline">(\overrightarrow{x}_i, y_i)</span> observations. The linear model takes the following form:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + \dots + \beta_k x_{ki} + u_i
</span></p>
<ul>
<li>Where the coefficients (that need to be estimated) are vector<span class="math inline">\overrightarrow{\beta} = \beta_0, \beta_1, ..., \beta_k</span>. That means we have <span class="math inline">k</span> number of variables and <span class="math inline">k+1</span> number of coefficients (with the one not attached to a variable being the intercept).</li>
<li>Where <span class="math inline">u_i</span> is the error term function - that determines the error for each unit <span class="math inline">i</span>. Error <span class="math inline">u_i</span> has a variance of <span class="math inline">\sigma^2</span>, and expectation <span class="math inline">E(u_i) = 0</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>Same as in Simple Linear Regression, once we have estimated <span class="math inline">\overrightarrow\beta</span>, we will have a best-fit <strong>plane</strong>, also called a fitted-values model (see <a href="https://statsnotes.github.io/theory/4.html#fitted-values-and-best-fit-lines">1.4.2</a>). The fitted values model takes the form:</p>
<p><span class="math display">
\hat y_i = \hat\beta_0 + \hat\beta_1 x_{1i} + \dots + \hat\beta_k x_{ki}
</span></p>
<ul>
<li>Where <span class="math inline">\hat y</span> are the predicted values of <span class="math inline">y</span> based on our best-fit plane.</li>
<li>Where <span class="math inline">\hat\beta_0, \dots, \hat\beta_k</span> are our estimates for coefficients <span class="math inline">\beta_0, \dots, \beta_k</span>.</li>
<li>Just like in simple linear regression, the error term <span class="math inline">u_i</span> dispersal because <span class="math inline">E(u_i) = 0</span>.</li>
</ul>
<p>Note how I have been saying best-fit <strong>plane</strong>, not best-fit line. This is because with multiple explanatory variables, we are now no longer in a 2-dimensional space, but a <span class="math inline">k</span>-dimensional space (based on the number of variables).</p>
<ul>
<li>Essentially, each variable has its own axis/dimension.</li>
<li>Mathematically, we are now in a <span class="math inline">\mathbb{R}^k</span> space.</li>
</ul>
<p>Thus, our best-fit line now is a best-fit plane. For example, take this model with 2 explanatory variables <span class="math inline">x_1</span> (years of education), <span class="math inline">x_2</span> (seniority), and <span class="math inline">y</span> (income):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-365376575.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>Any point on this plane is a part of our best-fit plane.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="interpretation-of-coefficients" class="level1">
<h1>1.5.2: Interpretation of Coefficients</h1>
<p>We now have a fitted-values model. But what do these actually mean in the context of the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>? We will see that the coefficients mean slightly different things than in simple linear regression.</p>
<ul>
<li>Note: <span class="math inline">\hat\beta_j</span> refers to any coefficient <span class="math inline">\hat\beta_1, \dots, \hat\beta_k</span> (for generalisation purposes).</li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that the following interpretations only applies to continuous <span class="math inline">x</span> variables and continuous/ordinal <span class="math inline">y</span> variables. We will discuss interpretation with other variables in <a href="https://statsnotes.github.io/metrics/6.html">lesson 1.6</a>.</p>
</div>
</div>
<p><br></p>
<section id="interpretation-of-hatbeta_j" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-hatbeta_j">Interpretation of <span class="math inline">\hat\beta_j</span></h3>
<p>In simple linear regression, we discussed how <span class="math inline">\hat\beta_1</span> is the change in <span class="math inline">y</span> given a one unit increase in <span class="math inline">x</span> (see <a href="https://statsnotes.github.io/intro/4.html#interpretation-and-standardisation">1.4.3</a>).</p>
<p>However, with multiple linear regression, things change a little. <span class="math inline">\hat\beta_j</span> is now the relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>, holding all other explanatory variables <span class="math inline">x_1, \dots, x_k</span> not <span class="math inline">x_j</span> constant.</p>
<p>We can prove this with a partial derivative (see appendix A.3 for mathematical guidance). We know that the rate of change between <span class="math inline">x_j</span> and <span class="math inline">y</span> should be the partial derivative of <span class="math inline">y</span> in respect to <span class="math inline">x_j</span>. Thus, let us find the partial derivative (for simplicity, we will use <span class="math inline">x_1</span>, but this applies to any <span class="math inline">x_j</span>):</p>
<p><span class="math display">
\begin{split}
&amp; \hat y = \hat\beta_0 + \hat\beta_1x_{1i} + \hat\beta_2x_{2i} + \dots + \hat\beta_kx_{ki} \\
&amp; \frac{\partial \hat y}{\partial x_1} = 0 + \hat\beta_1 + 0 + \dots + 0 \\
&amp; \frac{\partial \hat y}{\partial x_1} = \hat\beta_1
\end{split}
</span></p>
<p>We know that by definition, partial derivatives treat other variables as constants. Thus, <span class="math inline">\hat\beta_j</span> is the rate of change between <span class="math inline">x_j</span> and <span class="math inline">y</span>, holding all other explanatory variables constant.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of <span class="math inline">\hat\beta_1</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x_j</span> increases by one unit, there is an expected <span class="math inline">\hat{\beta}_1</span> unit change in <span class="math inline">y</span>, holding all other explanatory variables constant.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how I have been using the word <strong>relationship</strong>, not causal effect.</p>
<p>Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).</p>
</div>
</div>
<p>Note: the “accuracy” of the estimate of <span class="math inline">\hat\beta_1</span> can vary based on if a number of assumptions are met. We will discuss this more advanced topic in <a href="https://statsnotes.github.io/metrics/3.html">lesson 2.3</a>.</p>
<p><br></p>
<p>We know that we are holding all other explanatory variables constant. But what does that even mean?</p>
<p>For example, take this regression:</p>
<p><span class="math display">
\widehat{income}_i = \hat\beta_0 + \hat\beta_1 \text{education}_i + \hat\beta_2 \text{age}_i
</span></p>
<p>In this regression, <span class="math inline">\hat\beta_1</span> is the effect of education on income, holding all other explanatory variables (age) constant.</p>
<ul>
<li>In other words, it means, when holding age constant - at the same age levels, <span class="math inline">\hat\beta_1</span> is the relationship between education and income.</li>
<li>So only looking at 30 year olds, what is the relationship between income and education? What about for 40 year olds?</li>
<li>The regression estimates this for every level of the other explanatory variables (age), and then averages out the effect of education on income to produce <span class="math inline">\hat\beta_1</span>.</li>
</ul>
<p>There is another way to interpret <span class="math inline">\hat\beta_j</span> without discussing “holding other explanatory variables constant”. We can also say that <span class="math inline">\hat\beta_j</span> is the effect of <span class="math inline">x_j</span> on <span class="math inline">y</span> when partialling out the effect of other explanatory variables.</p>
<ul>
<li>This statement is more complex - we will discuss this later in <a href="https://statsnotes.github.io/metrics/2.html">lesson 2.2</a>.</li>
</ul>
<p>We can also standardise our <span class="math inline">\hat\beta_j</span> coefficient in terms of standard errors, in the same way we did in <a href="https://statsnotes.github.io/intro/4.html#interpretation-and-standardisation">1.4.3</a>.</p>
<p><br></p>
</section>
<section id="interpretation-of-hatbeta_0" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-hatbeta_0">Interpretation of <span class="math inline">\hat\beta_0</span></h3>
<p>The intercept term <span class="math inline">\hat\beta_0</span> has a very similar interpretation to that of simple linear regression - the expected value of <span class="math inline">y</span> given all explanatory variables equal 0 (although in the case of simple linear regression, there was only one explanatory variable).</p>
<p>We can prove this mathematically by plugging in <span class="math inline">\overrightarrow{x_i} = 0</span> into our fitted values equation:</p>
<p><span class="math display">
\begin{split}
\hat y_{i, \ \overrightarrow{x_i} = 0} &amp; = \hat\beta_0 + \hat\beta_1 x_{1i} + \dots + \hat\beta_k x_{ki} \\
&amp; = \hat\beta_0 + \hat\beta_1(0) + \dots + \hat\beta_k(0) \\
&amp; = \hat\beta_0
\end{split}
</span></p>
<p>Thus, knowing this, we can interpret <span class="math inline">\hat\beta_0</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of <span class="math inline">\hat\beta_0</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>When all explanatory variables equal 0, the expected value of <span class="math inline">y</span> is <span class="math inline">\hat{\beta}_0</span></p>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="omitted-variable-bias-and-confounding-variables" class="level1">
<h1>1.5.3: Omitted Variable Bias and Confounding Variables</h1>
<p>We have introduced the multiple linear regression model and how we can add more explanatory variables. But why would we want to add more explanatory variables?</p>
<p>Consider two regressions. The first regression, the “short” regression, is a simple linear regression with only explanatory variable <span class="math inline">x</span>. The second regression, the “long” regression, contains an extra variable <span class="math inline">z</span> that is omitted from the first regression:</p>
<p><span class="math display">
\begin{split}y_i &amp; = \beta_0^S + \beta_1^Sx_i + u_i^S \quad \text{short} \\y_i &amp; = \beta_0 + \beta_1x_i + \beta_2z_i + u_i \quad \text{long}\end{split}
</span></p>
<ul>
<li>Note: the <span class="math inline">S</span> in <span class="math inline">\beta_0^S</span> is a subscript representing short. It is not an exponent.</li>
</ul>
<p>Now consider an auxiliary regression, where the omitted variable <span class="math inline">z</span> is the outcome variable, and <span class="math inline">x</span> is the explanatory variable:</p>
<p><span class="math display">
z_i = \delta_0 + \delta_1 x_i + v_i
</span></p>
<ul>
<li>where <span class="math inline">\delta_0, \delta_1</span> are coefficients and <span class="math inline">v_i</span> is the error term</li>
</ul>
<p><br></p>
<p>Now we have <span class="math inline">z</span> in terms of <span class="math inline">x</span>, let us plug <span class="math inline">z</span> into our long regression to “recreate” the short regression:</p>
<p><span class="math display">
\begin{split}y_i &amp; = \beta_0 + \beta_1x_i + \beta_2z_i + u_i \\y_i &amp; = \beta_0 + \beta_1 x_i + \beta_2(\delta_0 + \delta_1x_i + v_i) + u_i \\y_i &amp; = \beta_0 + \beta_1 x_i + \beta_2 \delta_0 + \beta_2 \delta_1 x_i + \beta_2v_i + u_i \\y_i &amp; = \beta_0 + \beta_2 \delta_0 + (\beta_1 + \beta_2 \delta_1)x_i + \beta_2v_i + u_i\end{split}
</span></p>
<p>We have “recreated” the short regression with one variable <span class="math inline">x</span>. Let us see our recreation next to the original short regression:</p>
<p><span class="math display">
\begin{split}
y_i &amp; = \beta_0^S + \beta_1^Sx_i + u_i^S \\
y_i &amp; = \beta_0 + \beta_2 \delta_0 + (\beta_1 + \beta_2 \delta_1)x_i + \beta_2v_i + u_i
\end{split}
</span></p>
<ul>
<li>The short regression coefficient <span class="math inline">\beta_0^S</span> is analogous to the <span class="math inline">\beta_0 + \beta_2 \delta_0</span> in the recreation (both are the intercepts)</li>
<li>The short regression coefficient <span class="math inline">\beta_1^S x_i</span> is analogous to the <span class="math inline">(\beta_1 + \beta_2 \delta_1)x_i</span> in the recreation (both are the slope and variable of interest)</li>
<li>The short regression <span class="math inline">u_i^S</span> is analogous to the <span class="math inline">\beta_2 v_i + u_i</span> in the recreation (both are the error terms).</li>
</ul>
<p>Since the short regression <span class="math inline">\beta_1^S x_i</span> is analogous to the <span class="math inline">(\beta_1 + \beta_2 \delta_1)x_i</span> in the recreation, that means coefficient <span class="math inline">\beta_1^S = \beta_1 + \beta_2 \delta_1</span>.</p>
<p>Thus, the difference between the short regression (simple linear regression) coefficient <span class="math inline">\beta_1^S</span>, and the original long regression coefficient <span class="math inline">\beta_1</span>, is <span class="math inline">\beta_2 \delta_1</span>.</p>
<ul>
<li>If <span class="math inline">\beta_2 = 0</span> (meaning no relationship between omitted <span class="math inline">x</span> and <span class="math inline">y</span>), or <span class="math inline">\delta_1 = 0</span> (meaning no relationship between omitted <span class="math inline">x</span> and <span class="math inline">x</span>), then difference <span class="math inline">\beta_2 \delta_1 = 0</span>, thus there is no difference.</li>
<li>But if either of those facts are not true, then <span class="math inline">\beta_2 \delta_1 ≠ 0</span>, and there is a difference between <span class="math inline">\beta_1^S</span> and <span class="math inline">\beta_1</span>.</li>
</ul>
<p><br></p>
<p>More intuitively, if the omitted variable <span class="math inline">z</span> is both correlated with <span class="math inline">x</span> and <span class="math inline">y</span>, then the two coefficients are different by <span class="math inline">\beta_2 \delta_1</span>.</p>
<ul>
<li>Any variable <span class="math inline">z</span> correlated both with <span class="math inline">x</span> and <span class="math inline">y</span> is called a <strong>confounding variable</strong>.</li>
<li>This <span class="math inline">\beta_2 \delta_1</span> amount is called the <strong>omitted variable bias</strong>.</li>
</ul>
<p>Even more intuitively, take the figure below. There are three variables: the independent variable <span class="math inline">x</span>, the dependent variable <span class="math inline">y</span>, and the confounder <span class="math inline">z</span>. The arrows represent correlation between the variables.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1590374926.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%"></p>
</figure>
</div>
<p>If the confounding variable <span class="math inline">z</span> is correlated with independent variable <span class="math inline">x</span> and dependent variable <span class="math inline">y</span>, our simple linear regression with only explanatory variable <span class="math inline">x</span> (and not <span class="math inline">z</span>) will have a coefficient wrong by <span class="math inline">\beta_2 \delta_1</span>.</p>
<ul>
<li>This is because our simple linear regression (without <span class="math inline">z</span>) is estimating both the relationship between <span class="math inline">x \leftrightarrow y</span> (dependent and independent variable), <u>as well as</u> the relationship <span class="math inline">x \leftrightarrow z \leftrightarrow y</span> (since all three are correlated).</li>
<li>This <span class="math inline">x \leftrightarrow z \leftrightarrow y</span> is called the <strong>backdoor</strong> path. However, this backdoor path is not actually the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>, it is actually the relationship between <span class="math inline">z</span> and <span class="math inline">y</span>.</li>
<li>However, our simple linear regression includes the backdoor path in our coefficient. We do not want that, since we are only interested in how <span class="math inline">x</span> affects <span class="math inline">y</span>, not the backdoor path through <span class="math inline">z</span>.</li>
</ul>
<p>Thus, without including <span class="math inline">z</span> in our regression, the regression with just <span class="math inline">y</span> and <span class="math inline">x</span> will be incorrect by <span class="math inline">\beta_2 \delta_1</span>.</p>
<p><br></p>
<p>Thus, <u>to prevent the incorrect estimation of the simple linear regression model, we must add confounding variables <span class="math inline">z</span> to our regression</u>.</p>
<p>This is where multiple linear regression comes in: Multiple linear regression will allow us to add additional variables to our regressions, thus “controlling” (holding constant) <span class="math inline">z</span>, and allowing us to get an accurate estimate of the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>.</p>
<ul>
<li>We will discuss this idea of “controlling” in more detail later in <a href="https://statsnotes.github.io/metrics/2.html">lesson 2.2</a>, as it is more advanced.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="regression-goodness-of-fit-statistics" class="level1">
<h1>1.5.4: Regression Goodness-of-Fit Statistics</h1>
<p>We have fit a regression model with our estimates of coefficients. But how good is our regression model? We can use two main summary statistics to figure this out.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>The content in this section is almost identical to <a href="https://statsnotes.github.io/intro/4.html#regression-goodness-of-fit-statistics">1.4.6</a>, just slightly adjusted to multiple linear regression. I recommend rereading this section if needed.</p>
</div>
</div>
<p><br></p>
<section id="residual-standard-deviation" class="level3">
<h3 class="anchored" data-anchor-id="residual-standard-deviation">Residual Standard Deviation</h3>
<p>One way to summarise our fit is by considering our residuals/errors (see <a href="https://statsnotes.github.io/intro/4.html#regression-goodness-of-fit-statistics">1.4.6</a>).</p>
<p>For each observation <span class="math inline">i</span>, we have a different residual <span class="math inline">\hat u_i</span>. We can plot the frequency of each error <span class="math inline">\hat u_i</span> into a distribution - which indicates how frequently we get different values of error <span class="math inline">\hat u_i</span>.</p>
<ul>
<li>And since <span class="math inline">\hat u_i</span> is the error/residual, the distribution of <span class="math inline">\hat u_i</span> tells us the probability of different sizes of errors (probability of being how far away from the true value <span class="math inline">y_i</span> in the data).</li>
</ul>
<p>Since <span class="math inline">\hat u_i</span> is a distribution, we can measure its variance <span class="math inline">\hat\sigma^2</span>, and its standard deviation <span class="math inline">\hat\sigma</span>.</p>
<p><span class="math display">
\begin{split}
&amp; \hat\sigma^2 = Var(u_i) \\
&amp; \sigma = \sqrt{Var(u_i)}
\end{split}
</span></p>
<p>This is a little difficult to understand. But consider the figure below - the vertical distributions are the distribution of the residual <span class="math inline">\hat u_i</span>. They represent how likely we are to get a certain error. Higher variance means this distribution is more spread out, and lower variance means this distribution is less spread out.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3513031173.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:95.0%"></p>
</figure>
</div>
<p>Naturally, the better fit our regression is, the smaller the variance of residuals should be.</p>
<p><br></p>
</section>
<section id="r-squared" class="level3">
<h3 class="anchored" data-anchor-id="r-squared">R-Squared</h3>
<p>The R-squared statistic is another statistic commonly used when interpreting the fit of regressions.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: R-Squared
</div>
</div>
<div class="callout-body-container callout-body">
<p>R-Squared measures the proportion of variation in <span class="math inline">y</span>, that is explained by our regression model with <span class="math inline">x</span>.</p>
</div>
</div>
<p>R-squared is always between 0 and 1.</p>
<ul>
<li><p>Higher values indicates that our regression model with <span class="math inline">x</span> explains a large proportion of the variation in <span class="math inline">y</span>, which is good.</p></li>
<li><p>Lower values indicates that our regression model with <span class="math inline">x</span> explains a small proportion of the variation in <span class="math inline">y</span>, which is not as good.</p></li>
</ul>
<p>However, it is important to not be overly concerned with R-squared, as there are several reasons why it can be a misleading metric.</p>
<ul>
<li>The main issue with <span class="math inline">R^2</span> is that adding more explanatory variables never reduces the <span class="math inline">R^2</span> value.</li>
<li>Furthermore, <span class="math inline">R^2</span> often is high just by random chance - the more variables you have, the more likely you are to “accidentally” explain the variation in <span class="math inline">y</span> just through randomness.</li>
<li>That means that if we only focus on <span class="math inline">R^2</span>, we might end up adding very unnecessary variables that are not confounders, and do not improve our model. There are some downsides to including too many variables, that we will discuss in Part II of the course on econometric theory.</li>
</ul>
<p>There is also another way to mathematically derive R-squared (which makes more intuitive sense based on its definition). We will do this derivation later in <a href="https://statsnotes.github.io/metrics/2.html">lesson 2.2</a>.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="inference-and-hypothesis-testing" class="level1">
<h1>1.5.5: Inference and Hypothesis Testing</h1>
<p>Just like in simple linear regression, we will have sampling variation in our estimates of <span class="math inline">\hat\beta_j</span>. We can quantify this variation/uncertainty with the standard error <span class="math inline">\widehat{se}(\hat\beta_j)</span>.</p>
<ul>
<li>Note, every explanatory variable <span class="math inline">\hat\beta_1, \dots, \hat\beta_k</span> will have its own standard error and sampling distribution.</li>
</ul>
<p>With the standard error, we can run confidence intervals and hypothesis testing.</p>
<p><br></p>
<section id="confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h3>
<p>Just like previously discussed in <a href="https://statsnotes.github.io/intro/4.html#standard-errors-and-confidence-intervals">1.4.4</a>, the 95% confidence interval for our estimate of <span class="math inline">\hat\beta_j</span> has the bounds:</p>
<p><span class="math display">
\hat\beta_j - 1.96 \widehat{se}(\hat\beta_j), \ \hat\beta_j + 1.96 \widehat{se}(\hat\beta_j)
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Confidence Intervals
</div>
</div>
<div class="callout-body-container callout-body">
<p>The confidence interval means that under repeated sampling and estimating <span class="math inline">\hat\beta_j</span>, 95% of the confidence intervals we construct will include the true <span class="math inline">\beta_j</span> value in the population.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is very important to note that confidence intervals do not mean a 95% probability that the true <span class="math inline">\beta_j</span> is within any specific confidence interval we calculated.</p>
<p>We cannot know based on one confidence interval, whether it covers or does not cover the true <span class="math inline">\beta_j</span>.</p>
<p>The correct interpretation is that over many samples from the same population, we would expect 95% of our confidence intervals to contain the true <span class="math inline">\beta_j</span> value.</p>
</div>
</div>
<p><br></p>
</section>
<section id="hypothesis-testing" class="level3">
<h3 class="anchored" data-anchor-id="hypothesis-testing">Hypothesis Testing</h3>
<p>Hypothesis testing follows the same procedure as simple linear regression in <a href="https://statsnotes.github.io/intro/4.html#hypothesis-testing">1.4.5</a> (also see <a href="https://statsnotes.github.io/theory/2.html#intuition-of-hypothesis-testing">1.2.5</a> and <a href="https://statsnotes.github.io/theory/2.html#implementing-a-hypothesis-test">1.2.6</a> for more info on hypothesis tests).</p>
<p>In regression, our typical null hypotheses is that there is no relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>, and our alternate hypothesis is that there is a relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>. Thus, our hypotheses are:</p>
<p><span class="math display">
\begin{split}
&amp; H_0 : \beta_j = 0 \\
&amp; H_1: \beta_j ≠ 0
\end{split}
</span></p>
<p>Now, we calculate a t-test statistic:</p>
<p><span class="math display">
t = \frac{\hat\beta_j - 0}{\widehat{se}(\hat\beta_j)}
</span></p>
<ul>
<li>Where the 0 represents the null hypothesis value. If you have any other null hypothesis value, change the 0 to your hypothesis value.</li>
</ul>
<p>Now, we will consult a t-distribution (not a normal distribution) to calculate the p-values.</p>
<ul>
<li>We use a t-distribution, not a normal distribution, even if we have met the central limit theorem. The reason we do this is a little complicated, and will be explained later in <a href="https://statsnotes.github.io/metrics/4.html">lesson 2.4</a>.</li>
</ul>
<p>Once we have obtained our p-values from the t-distribution, we can interpret the p-values as follows:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of p-Values for Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The p-value is the probability of getting a test statistic equally or more extreme than the one we got with our sample estimate <span class="math inline">\hat\beta_j</span>, given the null hypothesis is true.</p>
<ul>
<li><p>If <span class="math inline">p&lt;0.05</span>, we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>), and conclude our alternate hypothesis (that there is a relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>).</p></li>
<li><p>If <span class="math inline">p&gt;0.05</span>, we cannot reject the null hypothesis, and cannot reject that there is no relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how I have been using the word <strong>relationship</strong>, not causal effect.</p>
<p>Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).</p>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="hypothesis-testing-with-more-than-one-coefficient" class="level1">
<h1>1.5.6: Hypothesis Testing with More than One Coefficient</h1>
<p>Sometimes, we want to test more than one coefficient at a time in a hypothesis test.</p>
<ul>
<li>This will be especially obvious why after <a href="https://statsnotes.github.io/metrics/6.html">lesson 1.6</a>.</li>
</ul>
<p>For example, let us say we want to test the statistical significance of <span class="math inline">\hat\beta_2</span> and <span class="math inline">\hat\beta_3</span> at the same time in the following regression model:</p>
<p><span class="math display">
\hat y = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_3
</span></p>
<p>What we can do is create two models - the alternate model <span class="math inline">M_a</span>, and the null model <span class="math inline">M_0</span>. The alternate model <span class="math inline">M_a</span> is the model we have above, and the null modle <span class="math inline">M_0</span> is the model without the two coefficients that we want to test (<span class="math inline">\hat\beta_2</span> and <span class="math inline">\hat\beta_3</span>).</p>
<p><span class="math display">
\begin{split}
&amp; M_0: y = \beta_0 + \beta_1x_1 \\
&amp; M_a: y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3x_3
\end{split}
</span></p>
<p>Our statistical test will be to test if the alternative model <span class="math inline">M_a</span> is significantly “better” than our null model <span class="math inline">M_0</span>. If <span class="math inline">M_a</span> is indeed significantly better, than we know that the coefficients <span class="math inline">\hat\beta_2</span> and <span class="math inline">\hat\beta_3</span> together are statistically significant.</p>
<p>Let us generalise this framework.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: F-Test of Nested Models
</div>
</div>
<div class="callout-body-container callout-body">
<p>The F-test of Nested Models allows us to test multiple coefficients at once. It compares two models: <span class="math inline">M_0</span> and <span class="math inline">M_a</span>.</p>
<p><span class="math display">
\begin{split}
&amp; M_0: y = \beta_0 + \beta_1x_1 + \dots + \beta_g x_g \\
&amp; M_a: y = \beta_0 + \beta_1 x_1 + \dots + \beta_g x_g + \beta_{g+1} x_{g+1} + \dots + \beta_kx_k
\end{split}
</span></p>
<ul>
<li>The model <span class="math inline">M_a</span> contains all of the explanatory variables, including the ones we want to test.</li>
<li>The model <span class="math inline">M_0</span> contains the other explanatory variables that are not a part of our test. Model <span class="math inline">M_0</span> must be “nested” in model <span class="math inline">M_a</span>: i.e.&nbsp;all explanatory variables present in <span class="math inline">M_0</span> must also be in <span class="math inline">M_a</span>.</li>
</ul>
<p>The model tests if <span class="math inline">M_a</span> is significantly better than <span class="math inline">M_0</span>. If this is the case, the extra coefficients in <span class="math inline">M_a</span> that we are testing are statistically significant.</p>
</div>
</div>
<p><br></p>
<p>How do we run a F-test of nested models?</p>
<p>Recall the concept of <span class="math inline">R^2</span> discussed in 1.5.4. <span class="math inline">R^2</span> describes how much of the variation in <span class="math inline">y</span> our explanatory variables explain.</p>
<p>The F-test uses the <span class="math inline">R^2</span> of the two models, and compares them.</p>
<ul>
<li>If the <span class="math inline">M_a</span> model has a statistically significantly higher <span class="math inline">R^2</span> value than the <span class="math inline">M_0</span> model, then <span class="math inline">M_a</span> is considered statistically significant, and we can conclude that the additional explanatory variables in <span class="math inline">M_a</span> are statistically significant.</li>
</ul>
<p><br></p>
<p>As we know from hypothesis testing (see <a href="https://statsnotes.github.io/theory/2.html#intuition-of-hypothesis-testing">1.2.5</a> and <a href="https://statsnotes.github.io/theory/2.html#implementing-a-hypothesis-test">1.2.6</a> for intuition), we need a test statistic and distribution to run a hypothesis test.</p>
<p>The statistic for a F-test is the F-statistic.</p>
<ul>
<li>Let us define <span class="math inline">R^2_a</span> and <span class="math inline">SSR_a</span> as the <span class="math inline">R^2</span> and sum of squared residuals for model <span class="math inline">M_a</span>.</li>
<li>Let us define <span class="math inline">R^2_0</span> and <span class="math inline">SSR_0</span> as the <span class="math inline">R^2</span> and sum of squared residuals for model <span class="math inline">M_0</span>.</li>
<li>The total number of coefficients in model <span class="math inline">M_a</span> is <span class="math inline">k_a</span>, and for model <span class="math inline">M_0</span>, is <span class="math inline">k_0</span>.</li>
<li>Let us define <span class="math inline">n</span> as the number of observations (should be the same for both models).</li>
</ul>
<p>Our F-statistic is mathematically calculated as:</p>
<p><span class="math display">
F = \frac{(SSR_0 - SSE_a)/(k_a - k_0)}{SSR_a / (n - k_a - 1)}
</span></p>
<p>After calculating our F-statistic, we consult an F-distribution with <span class="math inline">k_a - k_0</span> and <span class="math inline">n-k_a - 1</span> degrees of freedom.</p>
<p>With this distribution, we can obtain our p-value.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of p-Values for F-tests
</div>
</div>
<div class="callout-body-container callout-body">
<p>The p-value is the probability of getting a test statistic equally or more extreme than the one we got with our alternate model <span class="math inline">M_a</span>, given the null hypothesis is true.</p>
<ul>
<li><p>If <span class="math inline">p&lt;0.05</span>, we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that <span class="math inline">M_0</span> is a better model), and conclude our alternate hypothesis (that <span class="math inline">M_a</span> is a better model). This also means that the extra coefficients in <span class="math inline">M_a</span> are jointly statistically significant.</p></li>
<li><p>If <span class="math inline">p&gt;0.05</span>, we cannot reject the null hypothesis, and cannot reject that <span class="math inline">M_0</span> is the better model. Thus, the extra coefficients in <span class="math inline">M_a</span> are jointly not statistically significant.</p></li>
</ul>
</div>
</div>
<p><br></p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="prediction-with-regression" class="level1">
<h1>1.5.7: Prediction with Regression</h1>
<p>So far, we have focused on how to interpret the relationship between our explanatory and outcome variables. However, we can also use regression for prediction purposes.</p>
<ul>
<li>Multiple linear regression is typically much better and accurate at prediction than simple linear regression.</li>
<li>This is because we are inputting more information for the predictions. Obviously, it is easier to predict something given more data. For example, it is hard to predict someone’s debt level with just their age. But if you have info on their age, credit score, income, occupation, etc., it becomes easier.</li>
</ul>
<p>Recall our fitted values equation from earlier:</p>
<p><span class="math display">
\hat y_i = \hat\beta_0 + \hat\beta_1 x_{1i} + \dots + \hat\beta_k x_{ki}
</span></p>
<p>We input some values of <span class="math inline">\overrightarrow{x_i}</span>, and we will get a predicted value <span class="math inline">\hat y_i</span>.</p>
<p><br></p>
<p>The best way to really illustrate this is with an example. Let us say that our outcome variable <span class="math inline">y</span> is income, and our explanatory variable <span class="math inline">x_1</span> is years of education, and <span class="math inline">x_2</span> is age. We will get a regression model like this:</p>
<p><span class="math display">
\text{income}_i = \beta_0 + \beta_1 \text{education}_i + \beta_2 \text{age}_i + u_i
</span></p>
<p>Our fitted values equation will take the form of:</p>
<p><span class="math display">
\widehat{\text{income}}_i = \hat\beta_0 + \hat\beta_1 \text{education}_i + \hat\beta_2 \text{age}_i
</span></p>
<p>For simplicity, let us say that our estimate calculates <span class="math inline">\hat\beta_0 = 20000</span>, <span class="math inline">\hat\beta_1 = 3000</span>, and <span class="math inline">\hat\beta_2 = 500</span>. Now, our fitted values equation will be:</p>
<p><span class="math display">
\widehat{\text{income}}_i = 20000 + 3000 \  \text{education}_i + 500 \ \text{age}_i
</span></p>
<p>Using this fitted value equation, we can plug in any education level and age, and get the predicted income for that education and age level.</p>
<p>For example, what would we predict for a 30 year old with 10 years of education to have as an income? Let us plug in 10 for education into our fitted values:</p>
<p><span class="math display">
\begin{split}
\widehat{\text{income}}_i &amp; = 20000 + 3000 \  \text{education}_i + 500 \text{age}_i \\
\widehat{\text{income}}_i &amp; = 20000 + 3000 (10) + 500(30) \\
\widehat{\text{income}}_i &amp; = 20000 + 30000 + 15000 \\
\widehat{\text{income}}_i &amp; = 65000
\end{split}
</span></p>
<ul>
<li>Thus, someone who is 30 years old with 10 years of education has a predicted income of 65000.</li>
</ul>
<p>We can do this for any value of education (or any <span class="math inline">x</span> variable): just plug in <span class="math inline">\overrightarrow x</span> into the fitted values equation, and you will get a prediction of <span class="math inline">y</span>.</p>
<p><br></p>
<p>Remember in the past few sections, how we discussed statistical inference with <span class="math inline">\hat\beta_j</span>, and how in different samples, the value of <span class="math inline">\hat\beta_j</span> will change?</p>
<p>Well, what implication does this have on our prediction task? After all, if <span class="math inline">\hat\beta_j</span> (and <span class="math inline">\hat\beta_0</span>) change in different samples, our estimates will also change in different samples.</p>
<p>To address this, we can also create <strong>confidence intervals for predictions</strong>.</p>
<p>The variance of the residuals <span class="math inline">\hat u_i</span>, <span class="math inline">Var(\hat u_i)</span>, is what we are interested in (discussed in the previous section). The standard deviation of the residuals is just the square root of variance. Let us call this <span class="math inline">sd(\hat u_i)</span>.</p>
<p>Using this concept, we can create 95% confidence intervals for our predictions <span class="math inline">\hat y_i</span> (just like for estimates of <span class="math inline">\hat\beta_1</span>).</p>
<p><span class="math display">
\hat y_i - 1.96 \ sd(\hat u_i), \ \ \hat y_i + 1.96 \ sd(\hat u_i)
</span></p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="implementation-in-r" class="level1">
<h1>Implementation in R</h1>
<section id="regression-estimation" class="level2">
<h2 class="anchored" data-anchor-id="regression-estimation">Regression Estimation</h2>
<p>To estimate a regression, we can use the <em>feols()</em> function from the package <em>fixest</em>.</p>
<p>For the <em>feols()</em> function, we will need the <em>fixest</em> package. Make sure to install it if you have not previously (see appendix B.1).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fixest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
<p><strong>Syntax:</strong></p>
<p>For the <em>feols()</em> function, the syntax is as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">feols</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2<span class="sc">+</span> x3, <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Replace <em>model</em> with any name you want to store your regression model in.</li>
<li>Replace <em>y</em> with your outcome variable name, and <em>x1, x2, x3</em> with your explanatory variables name. You can add more after a + sign, and remove down to one explanatory variable.</li>
<li>Replace <em>mydata</em> with the name of your dataframe.</li>
</ul>
<p>The <em>se = “hetero”</em> tells R to use heteroscedasticity-robust standard errors (which we will discuss later in <a href="https://statsnotes.github.io/metrics/4.html">lesson 2.4</a>).</p>
<ul>
<li>Just know it is standard to use robust standard errors nowadays.</li>
</ul>
<p>Note: you can also use the <em>lm()</em> function with the same syntax, excluding the <em>se = “hetero”</em> section. However, this will not include robust standard errors.</p>
<p><br></p>
<p><strong>Example:</strong></p>
<p>Let us run a regression with outcome variable <em>immatt</em> (attitude towards immigrants), explanatory variable <em>age</em> and <em>educ</em> (education), from the dataframe called <em>dta</em>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>my_model <span class="ot">&lt;-</span> <span class="fu">feols</span>(immatt <span class="sc">~</span> age <span class="sc">+</span> educ, <span class="at">data =</span> dta)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(my_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OLS estimation, Dep. Var.: immatt
Observations: 33,706
Standard-errors: IID 
             Estimate Std. Error  t value  Pr(&gt;|t|)    
(Intercept) -0.529355   0.026638 -19.8720 &lt; 2.2e-16 ***
age         -0.005175   0.000291 -17.8117 &lt; 2.2e-16 ***
educ         0.063833   0.001381  46.2108 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
RMSE: 0.932834   Adj. R2: 0.08225</code></pre>
</div>
</div>
<p>We can see our output:</p>
<ul>
<li>In the estimate column, we get our intercept estimate <span class="math inline">\hat\beta_0</span>, and our explanatory variable coefficient estimates <span class="math inline">\hat\beta_1</span> and <span class="math inline">\hat\beta_2</span>.</li>
<li>In the standard error column, we can see R calculates the standard errors for us.</li>
<li>In the t-value column, we can see R calculates the t-test statistic for us.</li>
<li>In the p-value column, we can see R calculates the p-value for us, and puts stars * if the coefficient is statistically significant.</li>
</ul>
<p><br></p>
</section>
<section id="confidence-intervals-1" class="level2">
<h2 class="anchored" data-anchor-id="confidence-intervals-1">Confidence Intervals</h2>
<p>To create confidence intervals for our coefficients, we can use the <em>confint()</em> function.</p>
<ul>
<li>Note: you must fit a regression model before doing this, see above.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Replace <em>model</em> with the variable name that you stored your regression model to.</li>
</ul>
<p><br></p>
<p><strong>Example:</strong></p>
<p>Let us find the confidence intervals for the regression we fit above:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(my_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                  2.5 %       97.5 %
(Intercept) -0.58156640 -0.477142689
age         -0.00574481 -0.004605809
educ         0.06112520  0.066540126</code></pre>
</div>
</div>
<p>We can see that R outputs the lower and upper bound of our confidence intervals for our coefficients.</p>
<p><br></p>
</section>
<section id="prediction" class="level2">
<h2 class="anchored" data-anchor-id="prediction">Prediction</h2>
<p>For prediction, we can use the <em>predict()</em> function. You must fit a regression model before doing this, and also must have new <span class="math inline">x</span> values in which you want to predict.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predict</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, <span class="at">newdata =</span> x_values)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>predictions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Replace <em>predictions</em> with any variable name in which to store your predictions.</li>
<li>Replace <em>model</em> with your regression model variable name that you fit earlier.</li>
<li>Replace <em>x_values</em> with any vector of x-values you want to predict.</li>
</ul>
<p>The final <em>predictions</em> prints out our predictions.</p>
<p><br></p>
<p><strong>Example:</strong></p>
<p>Let us use the previous regression example, with outcome variable <em>immatt</em> (attitude towards immigrants), explanatory variable <em>age</em>.</p>
<p>Let us say I want to predict the immigration attitude of a 20 and a 60 year old, both at education years equalling 12. I will first create a vector with these explanatory variable values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># my values for x1: age</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x1_values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">20</span>, <span class="dv">60</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># my values for x2: educ</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>x2_values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">12</span>,<span class="dv">12</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># bind both variables into dataframe</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>x_values <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(x1_values, x2_values))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">#name the columns so R knows what variables they are</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(x_values) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"age"</span>, <span class="st">"educ"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, let us predict:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>my_prediction <span class="ot">&lt;-</span> <span class="fu">predict</span>(my_model, <span class="at">newdata =</span> x_values)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>my_prediction</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  0.13313121 -0.07388117</code></pre>
</div>
</div>
<p>We can see that our output includes predictions.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>