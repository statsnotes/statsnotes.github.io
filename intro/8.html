<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistics for Social Scientists</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="8_files/libs/clipboard/clipboard.min.js"></script>
<script src="8_files/libs/quarto-html/quarto.js"></script>
<script src="8_files/libs/quarto-html/popper.min.js"></script>
<script src="8_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="8_files/libs/quarto-html/anchor.min.js"></script>
<link href="8_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="8_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="8_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="8_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="8_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Statistics for Social Scientists</h1>
            <p class="subtitle lead">Lesson 1.8: Logistic Regression Model</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Lesson 1.8: Logistic Regression Model</h2>
   
  <ul class="collapse">
  <li><a href="#motivation---limitations-of-the-linear-probability-model" id="toc-motivation---limitations-of-the-linear-probability-model" class="nav-link active" data-scroll-target="#motivation---limitations-of-the-linear-probability-model">1.8.1: Motivation - Limitations of the Linear Probability Model</a></li>
  <li><a href="#logistic-regression-model" id="toc-logistic-regression-model" class="nav-link" data-scroll-target="#logistic-regression-model">1.8.2: Logistic Regression Model</a></li>
  <li><a href="#interpretation-of-coefficients-and-limitations" id="toc-interpretation-of-coefficients-and-limitations" class="nav-link" data-scroll-target="#interpretation-of-coefficients-and-limitations">1.8.3: Interpretation of Coefficients and Limitations</a></li>
  <li><a href="#odds-ratios-and-interpretation" id="toc-odds-ratios-and-interpretation" class="nav-link" data-scroll-target="#odds-ratios-and-interpretation">1.8.4: Odds Ratios and Interpretation</a></li>
  <li><a href="#inference-and-hypothesis-testing" id="toc-inference-and-hypothesis-testing" class="nav-link" data-scroll-target="#inference-and-hypothesis-testing">1.8.5: Inference and Hypothesis Testing</a></li>
  <li><a href="#likelihood-ratio-tests" id="toc-likelihood-ratio-tests" class="nav-link" data-scroll-target="#likelihood-ratio-tests">1.8.6: Likelihood Ratio Tests</a></li>
  <li><a href="#prediction-and-classification-with-logistic-regression" id="toc-prediction-and-classification-with-logistic-regression" class="nav-link" data-scroll-target="#prediction-and-classification-with-logistic-regression">1.8.7: Prediction and Classification with Logistic Regression</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p>Up to this point, we have focused on the linear regression model. However, in the case of binary outcome variables, the linear regression model may not be appropriate. In this lesson, we introduce the logistic regression model as a way to model binary outcome variables.</p>
<p>This lesson covers the following topics:</p>
<ul>
<li>The limitations of the linear probability model</li>
<li>The logistic regression model, and how to interpret the coefficients of the model (including with odds ratios).</li>
<li>How we can conduct statistical inference with logistic regression, including likelihood ratio tests.</li>
<li>How logistic regression can be used for prediction and classification.</li>
</ul>
</div>
</div>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
<section id="motivation---limitations-of-the-linear-probability-model" class="level1">
<h1>1.8.1: Motivation - Limitations of the Linear Probability Model</h1>
<p>In <a href="https://statsnotes.github.io/intro/6.html#binary-outcomes-and-the-linear-probability-model">1.9.6</a>, we introduced the linear probability model - a way to adapt the linear regression model to deal with binary outcome variables.</p>
<ul>
<li>Remember, binary outcome variables are <span class="math inline">y</span> variables that have two categories: <span class="math inline">y=0</span> and <span class="math inline">y=1</span>.</li>
</ul>
<p>The linear probability model, instead of having the outcome of <span class="math inline">y_i</span>, instead has the outcome of <span class="math inline">\pi_i</span>: the probability of an observation <span class="math inline">i</span> being in category <span class="math inline">y = 1</span>:</p>
<p><span class="math display">
\pi_i = \beta_0 + \beta_1x_i + \dots + \beta_kx_{ki} + u_i
</span></p>
<p>The outcome <span class="math inline">\pi_i</span> of the linear probability model is the probability of an observation <span class="math inline">i</span> being in category <span class="math inline">y = 1</span>:</p>
<p><span class="math display">
\pi_i = Pr(y_i = 1)
</span></p>
<p>This also implies that the probability of an observation <span class="math inline">i</span> being in category <span class="math inline">y=1</span> is <span class="math inline">1-\pi_i</span>, due to the rules of probability:</p>
<p><span class="math display">
1 - \pi_i = Pr(y_i = 0)
</span></p>
<p><br></p>
<p>However, as we briefly discussed in <a href="https://statsnotes.github.io/intro/6.html#limitations-of-linear-regression-for-non-continuous-outcomes">1.9.7</a>, there are a few major weaknesses of the linear probability model.</p>
<p>First of all, the linear probability model is, of course, linear. A linear function <span class="math inline">f(x)=mx+b</span> has a range of <span class="math inline">(-∞, ∞)</span>. That means the linear probability model’s potential <span class="math inline">\pi_i</span> values are <span class="math inline">\pi_i \in (-∞, ∞)</span>.</p>
<p>However, we know by the axioms of probability, that a probability must be between 0 and 1: <span class="math inline">\pi \in [0, 1]</span>. A probability of 0 means an event never occurs, and a probability of 1 means an event always occurs.</p>
<p>This means that if we use the linear probability model, we will often get nonsensical predictions of <span class="math inline">\pi_i</span> outside of the range between 0 and 1. This is a pretty big concern if our goal is to make accurate predictions.</p>
<p><br></p>
<p>Another limitation of the linear probability model is that the assumption of homoscedasticity is not met in binary outcome variables. We have not yet discussed homoscedasticity (discussed in <a href="https://statsnotes.github.io/metrics/4.html#gauss-markov-and-homoscedasticity">2.4.1</a>), and we will not be too concerned about this for now. Just know that using the linear probability model will result in larger standard errors, making it harder to reject our null hypothesis during statistical inference.</p>
<p>Finally, the assumption of normality (of the error term <span class="math inline">u</span>) in the linear probability model is not met. This is not a huge issue, since as we covered in <a href="https://statsnotes.github.io/intro/2.html#central-limit-theorem">1.2.4</a>, the central limit theorem ensures normality of our sampling distributions without the underlying distributions being normal. However, it can be a limitation if our sample sizes are very small.</p>
<p><br></p>
<p>So, how do we address these issues? The most common answer is with a <strong>link function</strong>. A link function takes our linear probability model outcome <span class="math inline">\pi_i</span> as an input, and transforms it to meet the requirements of our data.</p>
<ul>
<li>For example, with binary outcome variables, a link function can input <span class="math inline">\pi_i</span>, and output a value that is always between 0 and 1.</li>
</ul>
<p>The logistic regression is an extension of the linear regression, which applies a link function to ensure probabilities <span class="math inline">\pi_i</span> are always between 0 and 1.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="logistic-regression-model" class="level1">
<h1>1.8.2: Logistic Regression Model</h1>
<p>The logistic regression model transforms the linear probability model, such that the probability <span class="math inline">\pi_i</span> is always between 0 and 1, thus meeting the rules of probability, and solving the issues with the linear probability model.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Logistic Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The logistic regression takes the following form:</p>
<p><span class="math display">
\log\left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki}
</span></p>
<ul>
<li>Where <span class="math inline">\beta_0</span> (intercept) and coefficients <span class="math inline">\beta_1, \dots, \beta_k</span> need to be estimated.</li>
</ul>
</div>
</div>
<p><br></p>
<p>As we can see, the logistic regression is a model of <span class="math inline">\pi_i</span> through a link function: <span class="math inline">\log[\pi_i / (1- \pi_i)]</span> (this is also called the log-odds). However, this obviously is not too useful for us - we want to know <span class="math inline">\pi_i</span>, not <span class="math inline">\log[\pi_i / (1- \pi_i)]</span>.</p>
<p>Luckily, through rules of logarithms, we can isolate <span class="math inline">\pi_i</span> to also get logistic regression as a model for probabilities.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Property of Logarithms
</div>
</div>
<div class="callout-body-container callout-body">
<p>A useful property of logarithms is:</p>
<p><span class="math display">
e^{\log a} = a
</span></p>
</div>
</div>
<p>Using this property, we can isolate <span class="math inline">\pi_i</span> to get the logistic regression as a model for probabilities <span class="math inline">\pi_i</span>:</p>
<p><span class="math display">
\begin{split}
\log\left( \frac{\pi_i}{1 - \pi_i} \right) &amp; = \beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} \\
e^{\log\left( \frac{\pi_i}{1 - \pi_i} \right)} &amp; = e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} } \\
\frac{\pi_i}{1 - \pi_i} &amp; = e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} } \\
\pi_i &amp; = (1- \pi_i) e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} } \\
\pi_i &amp; = e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} } - \pi_i e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} } \\
\pi_i + \pi_i e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} } &amp; = e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} } \\
\pi_i(1 + e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} }) &amp; = e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} } \\
\pi_i &amp; = \frac{e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} }}{1+ e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} }}
\end{split}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Logistic Regression for Probabilities
</div>
</div>
<div class="callout-body-container callout-body">
<p>The logistic regression can be rewritten as follows in terms of probabilities <span class="math inline">\pi_i</span>:</p>
<p><span class="math display">
\pi_i = \frac{e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} }}{1+ e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} }}
</span></p>
</div>
</div>
<p>How do we estimate the parameters <span class="math inline">\beta_0, \dots, \beta_k</span>? We use a procedure called Maximum Likelihood Estimation. This is quite complex, and we will not delve into the estimation process in this lesson (however, it is covered in <a href="https://statsnotes.github.io/metrics/6.html">lesson 2.6</a>).</p>
<p>Naturally, our fitted probabilities <span class="math inline">\hat\pi_i</span> (fitted values for probabilities) will be the same regression, but with our estimated parameters <span class="math inline">\hat\beta_0, \dots, \hat\beta_k</span>:</p>
<p><span class="math display">
\hat\pi_i = \frac{e^{\hat\beta_0 + \hat\beta_1 x_{1i} + \dots + \hat\beta_kx_{ki} }}{1+ e^{\hat\beta_0 + \hat\beta_1 x_{1i} + \dots + \hat\beta_kx_{ki} }}
</span></p>
<p><br></p>
<p>Now that we know the function for our fitted probabilities <span class="math inline">\hat\pi_i</span>, what do these functions actually look like graphically? In the figure below are different logistic regression fitted probabilities graphed, while altering the parameters.</p>
<ul>
<li>Note: In the figure below, alpha <span class="math inline">\alpha</span> means intercept <span class="math inline">\beta_0</span>. This is a common alternative notation.</li>
</ul>
<p><img src="images/clipboard-1472632512.png" class="img-fluid" style="width:100.0%"></p>
<p>We can see through these graphs, that the fitted probabilities of <span class="math inline">\hat\pi_i</span> never exceed 1, or go below 0. Thus, the logistic regression solves this issue of the linear probability model.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="interpretation-of-coefficients-and-limitations" class="level1">
<h1>1.8.3: Interpretation of Coefficients and Limitations</h1>
<p>Once we have estimated our parameters <span class="math inline">\beta_0, \dots, \beta_k</span>, how do we actually interpret these in our fitted probabilities equation?</p>
<ul>
<li>First, let us focus on the qualitative/intuitive interpretations.</li>
<li>Then, we will talk about the numerical interpretations, and the difficulties with those.</li>
</ul>
<p><br></p>
<section id="qualitativeintuitive-interpretations" class="level3">
<h3 class="anchored" data-anchor-id="qualitativeintuitive-interpretations">Qualitative/Intuitive Interpretations</h3>
<p>The intercept <span class="math inline">\hat\beta_0</span> “shifts” the probabilities up and down. Thus, a higher <span class="math inline">\hat\beta_0</span> value means a higher probability <span class="math inline">\pi_i</span> for all values of <span class="math inline">x</span>, and a lower <span class="math inline">\hat\beta_0</span> value means a lower probability <span class="math inline">\pi_i</span> for all values.</p>
<p>For example the figure below (the left side) shows how the probabilities shift upwards and downwards when we changer <span class="math inline">\beta_0</span> (labelled alpha in the graph).</p>
<p><img src="images/clipboard-1472632512.png" class="img-fluid" style="width:100.0%"></p>
<p>The coefficient <span class="math inline">\hat\beta_j</span> (any <span class="math inline">\hat\beta_1, \dots, \hat\beta_k</span>) describes the association between <span class="math inline">x</span> and the probability of being in category <span class="math inline">y=1</span>.</p>
<ul>
<li>When <span class="math inline">\hat\beta_j</span> is positive, higher values of <span class="math inline">x</span> <strong>increases</strong> the chances of an observation being in cateogry <span class="math inline">y=1</span>.</li>
<li>When <span class="math inline">\hat\beta_j</span> is negative, higher values of <span class="math inline">x</span> <strong>reduces</strong> the chances of an observation being in category <span class="math inline">y=1</span>.</li>
<li>When <span class="math inline">\hat\beta_j = 0</span>, increasing/reducing <span class="math inline">x</span> has <strong>no relationship</strong> with the chances of an observation being in category <span class="math inline">y=1</span>.</li>
</ul>
<p>The figure above shows this in graphical form (on the right side).</p>
<p><br></p>
</section>
<section id="numerical-interpretations" class="level3">
<h3 class="anchored" data-anchor-id="numerical-interpretations">Numerical Interpretations</h3>
<p>We have discussed how the relationship between <span class="math inline">x</span> and <span class="math inline">y</span> given when <span class="math inline">\hat\beta_j</span> is positive or negative. However, what about the magnitude of the relationships? What do the actual values of <span class="math inline">\hat\beta_j</span> tell us.</p>
<p>Unfortunately, these are less self explanatory that the linear regression coefficients.</p>
<p>One way we could interpret the values is with the log-odds version of the logistic regression:</p>
<p><span class="math display">
\log\left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki}
</span></p>
<ul>
<li>Where <span class="math inline">\log[\pi_i / (1- \pi_i)]</span> is called the log-odds.</li>
</ul>
<p>Here, we can use a linear regression interpretation of <span class="math inline">\hat\beta_j</span>: As <span class="math inline">x</span> increases by one unit, the expected value of the log-odds <span class="math inline">\log[\pi_i / (1- \pi_i)]</span> changes by <span class="math inline">\hat\beta_j</span>.</p>
<p>The issue with this interpretation is that the log-odds is very unintuitive - what even is a log-odd? <span class="math inline">\log[\pi_i / (1- \pi_i)]</span> is not a very understandable metric in terms of interpretation.</p>
<p>We want to interpret in terms of probabilities or odds. However, recall the logistic regression for probabilities:</p>
<p><span class="math display">
\hat\pi_i = \frac{e^{\hat\beta_0 + \hat\beta_1 x_{1i} + \dots + \hat\beta_kx_{ki} }}{1+ e^{\hat\beta_0 + \hat\beta_1 x_{1i} + \dots + \hat\beta_kx_{ki} }}
</span></p>
<p>It is really not easy to interpret how changing <span class="math inline">x</span> by one unit will affect <span class="math inline">\hat\pi_i</span>.</p>
<ul>
<li>We can generally say that larger values of <span class="math inline">\hat\beta_j</span> mean a stronger positive relationship, and more negative values of <span class="math inline">\hat\beta_j</span> mean a stronger negative relationship.</li>
<li>However, we cannot express the actual magnitude/value of change when increasing <span class="math inline">x</span> by one unit.</li>
</ul>
<p>Luckily for us, there is a way to somewhat interpret logistic regression coefficients and their magnitudes: the odds ratios. We will cover this in the next section.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="odds-ratios-and-interpretation" class="level1">
<h1>1.8.4: Odds Ratios and Interpretation</h1>
<p>Odds ratios are a way to interpret the magnitude of our coefficients <span class="math inline">\hat\beta_j</span>. But to interpret odds ratios, we must first understand what odds are.</p>
<p><br></p>
<section id="the-odds-of-an-event" class="level3">
<h3 class="anchored" data-anchor-id="the-odds-of-an-event">The Odds of an Event</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Odds of an Event
</div>
</div>
<div class="callout-body-container callout-body">
<p>The odds of an event, is the probability of that event, divided by the probability of that event not happening.</p>
<p>For example, assume we have some event <span class="math inline">A</span>. The offs of event <span class="math inline">A</span> occuring are:</p>
<p><span class="math display">
\text{odds}_A = \frac{Pr(A)}{1 - Pr(A)}
</span></p>
</div>
</div>
<p>In terms of logistic regression, the event we are interested in is an observation being in category <span class="math inline">y=1</span>. The probability of that event occurring, as we have explained before, is <span class="math inline">\pi_i</span>. Thus, the odds of <span class="math inline">\pi_i</span> are as follows:</p>
<p><span class="math display">
\text{odds} = \frac{\pi_i}{1- \pi_i}
</span></p>
<p>You might note that the logistic regression’s link function looks very similar to this:</p>
<p><span class="math display">
\log\left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki}
</span></p>
<p>This is because the logistic regression is model of the <strong>log-odds</strong> of the event of an observation being in category <span class="math inline">y=1</span>.</p>
<ul>
<li>They are the “log”-odds because the logistic regression takes the log of the odds.</li>
</ul>
<p>We can solve for the odds of an observation in category <span class="math inline">y=1</span>, which is <span class="math inline">\pi_i/(1 - \pi_i)</span>:</p>
<p><span class="math display">
\text{odds} = \frac{\pi_i}{1 - \pi_i}  = e^{\beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} } \\
</span></p>
<p><br></p>
</section>
<section id="odds-ratios" class="level3">
<h3 class="anchored" data-anchor-id="odds-ratios">Odds Ratios</h3>
<p>Odds ratios, as the name suggests, are the ratio of two odds.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Odds Ratio
</div>
</div>
<div class="callout-body-container callout-body">
<p>The odds ratio is the ratio of two odds. Let us say we have two odds: <span class="math inline">\text{odds}_0</span> and <span class="math inline">\text{odds}_1</span>. The odds ratio takes the form:</p>
<p><span class="math display">
OR = \frac{\text{odds}_1}{\text{odds}_0} = \frac{\pi_1 / (1- \pi_1)}{\pi_0 /(1-\pi_0)}
</span></p>
</div>
</div>
<p>The odds ratio allows us to compare two different odds of <span class="math inline">\pi_i</span>. Why is this useful?</p>
<ul>
<li>We could calculate the odds of <span class="math inline">\pi_i</span> given <span class="math inline">x_i = x</span> (any value of <span class="math inline">x</span>).</li>
<li>Then, we could calculate the odds of <span class="math inline">\pi_i</span> given <span class="math inline">x_i = x + 1</span> (an increase of one unit compared to the previous value of <span class="math inline">x</span>).</li>
<li>Then, by comparing the odds ratio, we can compare the multiplicative change between <span class="math inline">x</span> and <span class="math inline">x+1</span>, which allows us to understand how increasing <span class="math inline">x</span> by one unit affect the outcome.</li>
</ul>
<p>How do we calculate the odds ratio between the odds of <span class="math inline">\pi_i</span> when <span class="math inline">x_i = x</span> and <span class="math inline">x_i = x+1</span>?</p>
<p>First, let us find the odds of each individual scenario: The odds of <span class="math inline">x_i = x</span> are (using the formula for odds introduced above). Let us assume that we only have one explanatory variable for simplicity.</p>
<p><span class="math display">
\begin{split}
\text{odds}_{x_i = x} = \frac{\pi_i}{1 - \pi_i} &amp; =  e^{\beta_0 + \beta_1 x_i} \\
&amp; = e^{\beta_0 + \beta_1 x} \\
&amp; =e^{\beta_0}e^{\beta_1 x}
\end{split}
</span></p>
<p>Now the odds of <span class="math inline">x_i = x+1</span> are as follows:</p>
<p><span class="math display">
\begin{split}
\text{odds}_{x_i = x+1} = \frac{\pi_i}{1 - \pi_i} &amp; =  e^{\beta_0 + \beta_1 x_i} \\
&amp; = e^{\beta_0 + \beta_1 (x+1)} \\
&amp; = e^{\beta_0 + \beta_1x + \beta_1} \\
&amp; = e^{\beta_0}e^{\beta_1x}e^{\beta_1}
\end{split}
</span></p>
<p>Now, let us find the odds ratio of these two odds:</p>
<p><span class="math display">
\begin{split}
OR &amp; = \frac{\text{odds}_{x_i = x+1}}{\text{odds}_{x_i = x}} \\
&amp;= \frac{e^{\beta_0}e^{\beta_1x}e^{\beta_1}}{e^{\beta_0}e^{\beta_1x}} \\
&amp; = e^{\beta_1}
\end{split}
</span></p>
<p>Thus, our odds ratio when increasing <span class="math inline">x</span> by one unit is <span class="math inline">e^{\beta_1}</span>.</p>
<p><br></p>
</section>
<section id="interpreting-odds-ratios" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-odds-ratios">Interpreting Odds Ratios</h3>
<p>We know that an odds ratio when increasing <span class="math inline">x</span> by one in a logistic regression is <span class="math inline">e^{\beta_1}</span>. But what does this mean?</p>
<p>What is a ratio? It is the multiplicative change to go from one value to another. This is a little complex to understand, so think about it this way: When you take the odds for <span class="math inline">x_i = x</span>, and multiply by the odds ratio, you get the odds for <span class="math inline">x_i = x+1</span>:</p>
<p><span class="math display">
\text{odds}_{x_i = x} \times \frac{\text{odds}_{x_i = x+1}}{\text{odds}_{x_i = x}} = \text{odds}_{x_i = x+1}
</span></p>
<p>So essentially, <u>when we increase <span class="math inline">x</span> by one, the odds of an observation being in category <span class="math inline">y=1</span> is multiplied by the odds ratio.</u></p>
<p><br></p>
<p>This intuition of multiplicative change allows us to interpret odds ratios.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpreting Odds Ratios
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x</span> increases by one unit, there is an associated <span class="math inline">e^{\beta_1}</span> (odds ratio) multiplicative change in the odds of an observation being in category <span class="math inline">y=1</span>.</p>
<p>So, if <span class="math inline">e^{\beta_1}&gt;1</span>, then a one unit increase in <span class="math inline">x</span> is associated with an increase in the odds of a unit being in category <span class="math inline">y=1</span> (since multiplying by anything greater than 1 increases the original value).</p>
<ul>
<li>More specifically, when <span class="math inline">e^{\beta_1} &gt; 1</span>, a one unit increase in <span class="math inline">x</span> is associated with a <span class="math inline">(e^{\beta_1} &gt; 1 - 1) \times 100</span> percent increase in the odds of a unit being in category <span class="math inline">y=1</span>.</li>
<li>For example, if the odds ratio <span class="math inline">e^{\beta_1} = 1.28</span>, a one unit increase in <span class="math inline">x</span> is associated with a 28% increase in the odds of a unit being in category <span class="math inline">y=1</span>.</li>
</ul>
<p>If <span class="math inline">e^{\beta_1} &lt; 1</span>, then a one unit increase in <span class="math inline">x</span> is associated with a decrease in the odds of a unit being in category <span class="math inline">y=1</span> (since multiplying by anything less than 1 decreases the original value).</p>
<ul>
<li>More specifically, when <span class="math inline">e^{\beta_1}&lt; 1</span>, a one unit increase in <span class="math inline">x</span> is associated with a <span class="math inline">(1 - e^{\beta_1}) \times 100</span> percent decrease in the odds of a unit being in category <span class="math inline">y=1</span>.</li>
<li>For example, if odds ratio <span class="math inline">e^{\beta_1} = 0.7</span>, a one unit increase in <span class="math inline">x</span> is associated with a 30% decrease in the odds of a unit being in category <span class="math inline">y=1</span>.</li>
</ul>
<p>If <span class="math inline">e^{\beta_1} = 1</span>, a one unit increase in <span class="math inline">x</span> is associated with no change in the odds of a unit being in category <span class="math inline">y=1</span> (because multiplying by 1 gets you the same result).</p>
</div>
</div>
<p>A key warning when interpreting odds ratios is that odds are not probabilities - consult the definition of odds above.</p>
<ul>
<li>For example, a doubling of the odds is not equal to a doubling of the probability of an event.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="inference-and-hypothesis-testing" class="level1">
<h1>1.8.5: Inference and Hypothesis Testing</h1>
<p>Just like with linear regression, we can conduct hypothesis tests on our coefficients (and odds ratios).</p>
<p>Just as with linear regression, we will have sampling variation in our estimates of our coefficients <span class="math inline">\hat\beta_j</span>. We can quantify this variation/uncertainty with the standard error <span class="math inline">\widehat{se}(\hat\beta_j)</span>.</p>
<ul>
<li>Note, every explanatory variable coefficient <span class="math inline">\hat\beta_1, \dots, \hat\beta_k</span> will have its own standard error and sampling distribution.</li>
</ul>
<p>With the standard error, we can run confidence intervals and hypothesis testing.</p>
<p><br></p>
<section id="confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h3>
<p>Just like previously discussed for linear regression in <a href="https://statsnotes.github.io/intro/4.html#standard-errors-and-confidence-intervals">1.4.4</a>, the 95% confidence intervals of our estimate <span class="math inline">\hat\beta_j</span> has the bounds:</p>
<p><span class="math display">
\hat\beta_j - 1.96 \widehat{se}(\hat\beta_j), \ \hat\beta_j + 1.96 \widehat{se}(\hat\beta_j)
</span></p>
<p>We can also calculate confidence intervals of the odds ratios. This is done by first calculating the interval for <span class="math inline">\hat\beta_j</span> first (as shown above), then taking the exponentials of the end points. Thus, the 95% confidence interval for the odds ratios has the bounds:</p>
<p><span class="math display">
e^{\hat\beta_j - 1.96 \widehat{se}(\hat\beta_j)}, \ e^{\hat\beta_j + 1.96 \widehat{se}(\hat\beta_j)}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Confidence Intervals
</div>
</div>
<div class="callout-body-container callout-body">
<p>The confidence interval means that under repeated sampling and estimating <span class="math inline">\hat\beta_j</span> (or the odds ratios)​, 95% of the confidence intervals we construct will include the true <span class="math inline">\hat\beta_j</span> (or odds ratio) value in the population.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is very important to note that confidence intervals do not mean a 95% probability that the true <span class="math inline">\hat\beta_j</span> (or odds ratio) is within any specific confidence interval we calculated.</p>
<p>We cannot know based on one confidence interval, whether it covers or does not cover the true <span class="math inline">\hat\beta_j</span> (or odds ratio).</p>
<p>The correct interpretation is that over many samples from the same population, we would expect 95% of our confidence intervals to contain the true <span class="math inline">\hat\beta_j</span> (or odds ratio) value.</p>
</div>
</div>
<p><br></p>
</section>
<section id="hypothesis-testing" class="level3">
<h3 class="anchored" data-anchor-id="hypothesis-testing">Hypothesis Testing</h3>
<p>Hypothesis testing follows the same procedure as simple linear regression in <a href="https://statsnotes.github.io/intro/4.html#hypothesis-testing">1.4.5</a> (also see <a href="https://statsnotes.github.io/theory/2.html#intuition-of-hypothesis-testing">1.2.5</a>, <a href="https://statsnotes.github.io/theory/2.html#implementing-a-hypothesis-test">1.2.6</a>, and <a href="https://statsnotes.github.io/intro/5.html#inference-and-hypothesis-testing">1.5.5</a> on more for hypothesis testing). This allows us to test if there is a statistically significant relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>.</p>
<p>In regression our typical null hypotheses is that there is no relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>, and our alternate hypothesis is that there is a relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>. Thus, our hypotheses are:</p>
<p><span class="math display">
\begin{split}
H_0 &amp; : \beta_j = 0 \\
H_1 &amp; : \beta_j ≠ 0
\end{split}
</span></p>
<p>Now we calculate a test statistic. However, instead of the t-test statistic (as we used in linear regression), we will use a z-test statistic. The mathematics is the same, we just use a different distribution.</p>
<p><span class="math display">
z = \frac{\hat\beta_j - 0}{\widehat{se}(\hat\beta_j)}
</span></p>
<ul>
<li>Where the 0 represents the null hypothesis value. If you have any other null hypothesis value, change the 0 to your hypothesis value.</li>
</ul>
<p>Now, we will consult a standard normal distribution (see <a href="https://statsnotes.github.io/intro/1.html#the-standard-normal-distribution">1.1.7</a>) to calculate the p-values.</p>
<ul>
<li>Note that we are using a standard normal distribution, not a t-distribution like in regressions.</li>
<li>The reason for this is because of the central limit theorem (see <a href="https://statsnotes.github.io/intro/2.html#central-limit-theorem">1.2.4</a>). The linear regression is actually an outlier - there is a technical reason for why we use the t-distribution and not the normal for linear regression, explained in <a href="https://statsnotes.github.io/metrics/4.html">lesson 2.4</a>.</li>
</ul>
<p>Once we have obtained our p-values from the t-distribution, we can interpret the p-values as follows:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of p-Values for Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The p-value is the probability of getting a test statistic equally or more extreme than the one we got with our sample estimate <span class="math inline">\hat\beta_j</span>, given the null hypothesis is true.</p>
<ul>
<li><p>If <span class="math inline">p&lt;0.05</span>, we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>), and conclude our alternate hypothesis (that there is a relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>).</p></li>
<li><p>If <span class="math inline">p&gt;0.05</span>, we cannot reject the null hypothesis, and cannot reject that there is no relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how I have been using the word <strong>relationship</strong>, not causal effect.</p>
<p>Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).</p>
</div>
</div>
<p><br></p>
</section>
<section id="extension-wald-test" class="level3">
<h3 class="anchored" data-anchor-id="extension-wald-test">Extension: Wald Test</h3>
<p>In the above hypothesis test, we used the <span class="math inline">z</span>-test statistic to conduct our hypothesis test. We can use another test statistic, the <strong>wald test statistic</strong>:</p>
<p><span class="math display">
W = \left( \frac{\hat\beta_j -0 }{\widehat{se}(\hat\beta_j)} \right)^2
</span></p>
<ul>
<li>The wald test statistic is also just the <span class="math inline">z</span>-test statistic squared.</li>
</ul>
<p>The wald test statistic is then compared to a <span class="math inline">\chi^2</span> distribution with 1 degree of freedom, to obtain the p-value.</p>
<p>The wald test produces the exact same p-value as the z-test, so there is no real reason to use one or the other. The reason I am mentioning this is because some statistical software and papers will use the wald test, so it is useful to know what is going on.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="likelihood-ratio-tests" class="level1">
<h1>1.8.6: Likelihood Ratio Tests</h1>
<p>Just like in linear regression, sometimes we will want to test more than one coefficient at a time in a hypothesis test. This is especially the case when we have categorical variables or polynomial transformations.</p>
<p>In linear regression, we talked about how we could use the F-test to run tests on multiple coefficients.</p>
<p>However, F-tests no longer work in the case of logistic regression. This is because the F-tests compare the <span class="math inline">R^2</span> metric between two models, where <span class="math inline">R^2</span> measures the percent of variation of the outcome variable that the explanatory variables explain. However, in logistic regression with a binary <span class="math inline">y</span> outcome, there isn’t really such thing as the “variation” of the outcome - it is always <span class="math inline">y=0</span> or <span class="math inline">y=1</span>.</p>
<p>Thus, we need some alternative test to use on logistic regression for testing multiple coefficients.</p>
<p><br></p>
<p>The <strong>likelihood ratio test</strong> is the test we use to test multiple coefficients in logistic regression.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Likelihood Ratio Test
</div>
</div>
<div class="callout-body-container callout-body">
<p>The likelihood ratio test allows us to test multiple coefficients at once. Just like the F-test for linear regression, the likelihood ratio test compares two models: our null model <span class="math inline">M_0</span>, and our alternate model <span class="math inline">M_a</span>:</p>
<p><span class="math display">
\begin{split}
&amp; M_0 : \log \left( \frac{\pi_i}{1-\pi_i} \right) = \beta_0 + \beta_1 x_1 + \dots + \beta_g x_g \\
&amp; M_a : \log \left( \frac{\pi_i}{1-\pi_i} \right) = \beta_0 + \beta_1 x_1 + \dots + \beta_g x_g + \beta_{g+1} x_{g+1} + \dots + \beta_k x_k \\
\end{split}
</span></p>
<ul>
<li><p>The model <span class="math inline">M_a</span> contains all of the explanatory variables, including the ones we want to test.</p></li>
<li><p>The model <span class="math inline">M_0</span> contains the other explanatory variables that are not a part of our test. Model <span class="math inline">M_0</span> must be “nested” in model <span class="math inline">M_a</span>: i.e.&nbsp;all explanatory variables present in <span class="math inline">M_0</span> must also be in <span class="math inline">M_a</span>.</p></li>
</ul>
<p>The model tests if <span class="math inline">M_a</span> is significant better than <span class="math inline">M_0</span>. If this is the case, the extra coefficients in <span class="math inline">M_a</span> that we are testing are statistically significant.</p>
</div>
</div>
<p>However, we just said we cannot use <span class="math inline">R^2</span> for logistic regression. Then what does the likelihood ratio test use?</p>
<p>The answer is the <strong>likelihood</strong> <strong>function</strong>.</p>
<ul>
<li>The likelihood function determines how “likely” we are to observe the data we actually observe, given our estimated coefficients <span class="math inline">\hat\beta_0, \dots \hat\beta_k</span> and our model’s explanatory variables.</li>
<li>A higher likelihood function means it is more likely our estimated coefficients are the true values in the population, since those true values in the population “produced” the data we have.</li>
<li>If one model’s estimated coefficients is more likely to produce the data we actually observe, it is considered more likely, and is considered the better model.</li>
</ul>
<p>We will discuss the intricacies of likelihood functions in <a href="https://statsnotes.github.io/metrics/6.html">lesson 2.6</a>, as they can be very complex and hard to understand. For now, just know that the higher likelihood our model is, the better it is considered to be.</p>
<p><br></p>
<p>Consider two models. Our null hypothesis model <span class="math inline">M_0</span> has some likelihood, that we will label <span class="math inline">L_0</span>. The alternate hypothesis model <span class="math inline">M_a</span> also has some likelihood, that we will label <span class="math inline">L_a</span>.</p>
<p>If <span class="math inline">L_a</span> is statistically significantly larger than <span class="math inline">L_0</span>, then we will consider <span class="math inline">M_a</span> to be the statistically significantly better model.</p>
<p>Our test statistic for the test will be the <strong>Likelihood Ratio Test Statistic</strong>, labelled <span class="math inline">L^2</span>.</p>
<p><span class="math display">
L^2 = -2 \log\left( \frac{L_0}{L_1} \right) = 2 \log (L_1) - 2 \log (L_0)
</span></p>
<ul>
<li>Essentially, this is the difference between log likelihoods. The reason for the log-likelihoods, and not just likelihoods, will be explained in <a href="https://statsnotes.github.io/metrics/6.html">lesson 2.6</a> where we cover the statistical theory behind this test.</li>
</ul>
<p>Once we get our test statistic, we will consult a <span class="math inline">\chi^2</span> distribution with degrees of freedom equal to the number of extra coefficients in the larger model <span class="math inline">M_a</span>. The p-value will be obtained from this distribution.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of p-values for Likelihood Ratio Tests
</div>
</div>
<div class="callout-body-container callout-body">
<p>The p-value is the probability of getting a test statistic equally or more extreme than the one we got with our alternate model <span class="math inline">M_a</span>, given the null hypothesis is true.</p>
<ul>
<li>If <span class="math inline">p &lt; 0.05</span>, we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that <span class="math inline">M_0</span> is a better model), and conclude that our alternate hypothesis (that <span class="math inline">M_a</span> is a better model). This also means that the extra coefficients in <span class="math inline">M_a</span> are jointly statistically significant.</li>
<li>If <span class="math inline">p&gt; 0.05</span>, we cannot reject the null hypothesis, and cannot reject that <span class="math inline">M_0</span> is the better model. Thus, the extra coefficients in <span class="math inline">M_a</span> are jointly not statistically significant.</li>
</ul>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="prediction-and-classification-with-logistic-regression" class="level1">
<h1>1.8.7: Prediction and Classification with Logistic Regression</h1>
<p>Just like with linear regression, the logistic regression can be used for more than just interpreting the relationship between variables. The logistic model can also be used for prediction.</p>
<p>We can do two types of prediction with logistic regression:</p>
<ol type="1">
<li>We can predict the probabilities of an unobserved observation being in category <span class="math inline">y=1</span>.</li>
<li>We can classify unobserved observations into either category <span class="math inline">y=1</span> or category <span class="math inline">y=0</span>.</li>
</ol>
<p>Let us explore both of these prediction methods.</p>
<p><br></p>
<section id="predicting-probabilities" class="level3">
<h3 class="anchored" data-anchor-id="predicting-probabilities">Predicting Probabilities</h3>
<p>We can predict the probability of an unobserved observation being in category <span class="math inline">y=1</span> (or category <span class="math inline">y=0</span> with some quick mathematics). To do this, we simply use the fitted probabilities:</p>
<p><span class="math display">
\hat\pi_i = \frac{e^{\hat\beta_0 + \hat\beta_1 x_{1i} + \dots + \hat\beta_kx_{ki} }}{1+ e^{\hat\beta_0 + \hat\beta_1 x_{1i} + \dots + \hat\beta_kx_{ki} }}
</span></p>
<p>We simply plug in our unobserved observations <span class="math inline">x_1, \dots, x_k</span> values, and the fitted probabilities will spit out a probability of that observation being in category <span class="math inline">y=1</span>.</p>
<p>If we are interested in finding the probability of an observation being in category <span class="math inline">y=0</span>, we can simply first find the probability <span class="math inline">\pi_i</span> of an observation being in category <span class="math inline">y=1</span>, then doing <span class="math inline">1 - \pi_i</span> to get the probability of an observation being in category <span class="math inline">y=1</span>.</p>
<p><br></p>
<p>For example, let us consider a very simple model with one explanatory variable</p>
<ul>
<li>Our outcome variable <span class="math inline">y</span> will be which party an individual voted for, with <span class="math inline">y=0</span> being the Democrats, and <span class="math inline">y=1</span> being the republicans.</li>
<li>Our explanatory variable <span class="math inline">x</span> will be the age of the individual.</li>
</ul>
<p>Our fitted probabilities will take the form:</p>
<p><span class="math display">
\hat\pi_i = \frac{e^{\hat\beta_0 + \hat\beta_1\text{age}_i}}{1 + e^{\hat\beta_0 + \hat\beta_1\text{age}_i}}
</span></p>
<ul>
<li>Where <span class="math inline">\pi_i</span> is the probability that the observation <span class="math inline">i</span> voted republican.</li>
</ul>
<p>Let us pretend that we estimated the model to get <span class="math inline">\hat\beta_0 = -3</span> and <span class="math inline">\hat\beta_1 = 0.1</span>. Thus, our fitted probabilities will be:</p>
<p><span class="math display">
\hat\pi_i = \frac{e^{-3 + 0.1\text{age}_i}}{1 + e^{-3 + 0.1\text{age}_i}}
</span></p>
<p>With this fitted model, we can predict the probability of any observation <span class="math inline">i</span> voted for republicans, given their age. For example, what is the probability a 20 year old voted republican?</p>
<p><span class="math display">
\hat\pi_i = \frac{e^{-3 + 0.1(20)}}{1 + e^{-3 + 0.1(20)}} = 0.2689
</span></p>
<ul>
<li>Thus, a 20 year old has a 26.89 percent chance of voting republican.</li>
</ul>
<p>What about the probability of a 60 year old voting republican?</p>
<p><span class="math display">
\hat\pi_i = \frac{e^{-3 + 0.1(60)}}{1 + e^{-3 + 0.1(60)}} = 0.9525
</span></p>
<ul>
<li>Thus, a 60 year old has a 95.25 percent chance of voting republican</li>
</ul>
<p><br></p>
</section>
<section id="classification" class="level3">
<h3 class="anchored" data-anchor-id="classification">Classification</h3>
<p>What if we do not care about the probabilities of an observation being in category <span class="math inline">y=1</span>? Instead, we care about what category an observation would be in?</p>
<ul>
<li><p>For example, let us say google wants to create an email spam filter. Google does not necessarily care about the exact probability of an email being spam or not. What google cares about is if it should classify that email as spam or not.</p></li>
<li><p>Classification is about assigning categories to observations, not about predicting the probabilities.</p></li>
</ul>
<p>So in other words, instead of predicting probability <span class="math inline">\hat\pi_i</span>, we want to find the actual value of <span class="math inline">\hat y_i</span> - is it category <span class="math inline">y=0</span> or <span class="math inline">y=1</span>.</p>
<p>How do we do classification?</p>
<ol type="1">
<li>First, we want to calculate predicted probabilities <span class="math inline">\hat\pi_i</span>, like we did above.</li>
<li>Then, given the probability of an observation, we either assign that observation to <span class="math inline">\hat y_i = 0</span> or <span class="math inline">\hat y = 1</span> based on some threshold.</li>
</ol>
<p>For example, Google would first calculate the probability that an email is spam, based on a set of explanatory variables. Then, if google believes that a specific email has a more than a 50% chance of being spam, it assigns it to the spam category, and if it has less than a 50% chance of being spam, it assigns it to the non-spam category.</p>
<p>50% is the most common probability threshold for assigning observations into either category. However, you can choose a different threshold, based on your risk-tolerance.</p>
<ul>
<li>For example, if you are conducting a Cancer Test, you would prefer to flag people without cancer accidentally as having cancer, rather than miss someone who has cancer (and possibly cause their death). So, you might assign cases with even a 10% chance of cancer to the cancer positive side, to ensure no one who actually has cancer falls through the cracks.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>