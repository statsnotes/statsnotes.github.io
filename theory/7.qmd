---
title: "Statistics for Social Scientists"
subtitle: "Lesson 1.7: Properties of the OLS Estimator"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Lesson 1.7: Properties of the OLS Estimator"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

This lesson covers the following topics:

-   Some simple algebraic properties of our best-fit plane through OLS estimation.
-   The conditions on which OLS estimation is an unbiased estimator for multiple regression (Gauss-Markov conditions), and how violations of these conditions will lead to biased estimates.
-   Omitted Variable Bias, and how it causes violations of the Gauss-Markov conditions.
-   The method of moments estimator, how OLS is a method of moments estimator, and how this shows another reason endogeneity is a concern.

::: callout-note
This entire lesson will mirror the first half of [lesson 1.5](https://statsnotes.github.io/theory/5.html), but will adjust the properties of bivariate regression to multiple regression. I recommend a strong understanding of those topics before starting this chapter.
:::

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.1: Residuals and Algebraic Properties of OLS

Once we have our OLS estimates of parameters $\hat\beta_0 , \dots \hat\beta_k$ (estimated in [1.6.4](https://statsnotes.github.io/theory/6.html#mathematics-of-the-ordinary-least-squares-estimator)), we can obtain the OLS residuals $\hat u_i$:

$$
\begin{split}
\hat u_i & = y_i - \hat y_i \\
& = y_i - (\hat \beta_0 + \hat\beta_1x_{1i} + \dots + \hat\beta_k x_{ki}) \\
& = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}
\end{split}
$$

-   If you do not know what residuals $\hat u_i$ are, and why they are different from $u_i$ error term, see [1.5.1](https://statsnotes.github.io/theory/5.html#residuals-and-algebraic-properties-of-ols).

<br />

Recall that the OLS estimates of $\hat\beta_0 , \dots \hat\beta_k$ are chosen to satisfy the following first order conditions (see [1.6.4](https://statsnotes.github.io/theory/6.html#mathematics-of-the-ordinary-least-squares-estimator)):

$$
\begin{split}& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\& \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
& \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
$$

-   and so on for $x_{3i}, ..., x_{ki}$.

Using these properties, we can plug in $\hat u_i = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}$ to get the above conditions:

$$
\begin{split}
& \sum\limits_{i=1}^n \hat u_i = 0 \\
& \sum\limits_{i=1}^n x_{1i} \hat u_i = 0 \\
& \sum\limits_{i=1}^n x_{2i} \hat u_i = 0 
\end{split}
$$

-   and so on for $x_{3i}, ..., x_{ki}$.

These conditions can be used to find a few properties of OLS in multiple linear regression.

<br />

### Property 1: Sum of Residuals is 0

OLS residuals always add up to zero, since:

$$
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = \sum\limits_{i=1}^n \hat u_i = 0
$$

-   This property ensures that the average value of $y_i$ in our data is the same as the average value of our predictions $\hat y_i$.

<br />

### Property 2: No Covariance Between Any $x$ and Residual.

Using the first order conditions of OLS, and the definition of $\hat u_i$ from above, we know the following to be true:

$$
\begin{split}
& \sum\limits_{i=1}^n x_{1i} \hat u_i = 0 \\
& \sum\limits_{i=1}^n x_{2i} \hat u_i = 0 
\end{split}
$$

-   and so on for $x_{3i}, ..., x_{ki}$.

Using the same proof from [1.5.1](https://statsnotes.github.io/theory/5.html#residuals-and-algebraic-properties-of-ols), we know the following are true:

$$
\begin{split}
& Cov(x_1, \hat u) = 0 \\
& Cov(x_2, \hat u) = 0
\end{split}
$$

-   and so on for $x_{3i}, ..., x_{ki}$.

Thus, the covariances (and thus correlations) between all explanatory variables $x_1, \dots, x_k$ and $\hat u$ must be zero.

<br />

### Property 3: Regression Line Passes Through Means

For a similar proof in [1.5.1](https://statsnotes.github.io/theory/5.html#residuals-and-algebraic-properties-of-ols), we know that the point $(\bar y, \bar x_1, \dots, \bar x_k)$ is on the regression plane.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.2: Unbiasedness of OLS Under the Gauss-Markov Theorem

Unbiasedness, as we have discussed in both [1.2.8](https://statsnotes.github.io/theory/2.html#properties-of-estimators) and [1.5.2](https://statsnotes.github.io/theory/5.html#unbiasedness-of-ols-under-the-gauss-markov-theorem), is a desirable property of estimators.

Just like for simple linear regression, the Gauss-Markov Theorem proves that OLS is unbiased under 4 conditions.

-   Notation note: $\beta_j$ refers to any one of $\beta_1, \dots, \beta_k$. It can be generalised to any of the coefficients besides the intercept.

::: callout-tip
## Definition: Gauss-Markov Theorem

The Gauss-Markov Theorem states that under 4 conditions, the OLS estimator is an unbiased estimator for $\beta_j$.

-   **MLR.1 (Linearity in Parameters)**: The parameters of the model are linear.
-   **MLR.2 (Random Sampling)**: All observations in our sample are randomly sampled from the population.
-   **MLR.3 (No Perfect Mutlicollinearity)**: There are no exact linear relationships among variables (where correlation coefficient equals 1).
-   **MLR.4 (Zero Conditional Mean)**. The error term $u$ has an expectation of 0, given any value of $x_j$ for any explanatory variable.

[Note specifically that MLR.3 is different than the equivalent condition for simple linear regression]{.underline}. We will explore each condition in detail.
:::

<br />

### Assumption MLR.1: Linearity

Assumption MLR.1 states that a model must be **linear in parameters**.

This means that the parameters of the model $\beta_0$ and $\beta_1$ must not be multiplied together - they must be added together.

For example, the following is linear:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u
$$

While the following is not linear (since parameters $\beta_1$ and $\beta_2$ are multiplied):

$$
y = \beta_0 + \beta_1\beta_2x_1 + \beta_3 x_2 + u
$$

Note: This does not mean that the actual regression line must be linear - only the parameters/coefficients must not be multiplied, the variables can be. For example, the following model is still linear in parameters:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + u
$$

<br />

### Assumption MLR.2: Random Sampling

This assumption says that all observations in our sample are randomly sampled from the same population.

Why is this useful? Take the standard linear model:

$$
y = \beta_0 + \beta_1x_1 + \dots + \beta_kx_k + u
$$

The error term $u$ is some random variable (with its own probability distribution), that can be defined by its expectation $E(u)$.

-   If we randomly select one observation $i$ from the data, each observation $i$ has an equal chance of being selected.
-   The error term for that observation, $u_i$ should also have the same expectation as the random variable $u$, since each observation $i$ within $u$ has the same chance of being selected.

Thus, random sampling allows us to say $E(u) = E(u_i)$.

Why is this not the case when random sampling is not present?

-   Imagine you are not random sampling. Perhaps, some observations $i$ with lower $u_i$ values are more likely to be sampled.
-   If there is no random sampling, then $E(u_i)$ may not equal $E(u)$.

<br />

### Assumption MLR.3: No Perfect Multicollinearity

Recall that in [1.6.6](https://statsnotes.github.io/theory/6.html#regression-anatomy-formula-for-ols-estimation), we derived the OLS estimator in terms of regression anatomy as:

$$
\hat\beta_j = \frac{\sum_{i=1}^n \widetilde{r_{ji}} \ y_i}{\sum_{i=1}^n \widetilde{r_{ji}}^2}
$$

Where $\widetilde{r_{ij}}$ is the part of $x_j$ that is uncorrelated with any other explanatory variables.

Obviously, as we know from mathematics, the denominator cannot be zero, or else, $\hat\beta_j$ will not exists. This implies that we cannot derive $\hat\beta_j$ if $\sum \widetilde{r_{ij}}^2 = 0$.

-   What causes $\sum\widetilde{r_{ij}}^2 = 0$?
-   If $\sum\widetilde{r_{ij}}^2 = 0$, that means there is no part of $x_j$ that is completely uncorrelated with any other explanatory variable.
-   In other words, that means $x_j$ is **perfectly correlated** to at least one other explanatory variable.

Thus, if $x_j$ is perfectly correlated with any other explanatory variable, coefficient $\hat\beta_j$ is not calculable.

<br />

But what does perfectly correlated mean? Mathematically, it means the correlation coefficient between $x_j$ and any other explanatory variable $x_m$ is equal to 1 or -1.

$$
|Corr(x_j, x_m)| = 1
$$

But practically, what does that mean when choosing our explanatory variables?

-   It means that we cannot choose measurements of the same concept in different units.
-   For example, you cannot include the two variables *height in inches* and *height in cm* in the same regression. This is because both variables are perfectly correlated, just differing only by a common factor.

This only applies if variables are **perfectly** correlated.

-   However, even if there is no perfect correlation, but a high correlation, this can cause some issues (not related to unbiasedness). We will discuss this later in lesson 1.8.

<br />

### Assumption MLR.4: Zero-Conditional Mean

In the population, the error term $u$ must have an expecation of 0, given all values of $x$. Mathematically:

$$
E(u|x_1, \dots x_k) = 0 \text{ for all } (x_1, \dots , x_k)
$$

And using condition MLR.2 Random Sampling (from above), we also know that:

$$
E(u|x_1, \dots ,x_k) = E(u_i|x_{1i}, \dots, x_{ki}) = 0
$$

This also implies that the error term $u$ and any $x_1, \dots , x_k$ are uncorrelated with each other.

-   This is because if $u$ was correlated with $x_j$, it would not always be equal to 0 no matter the value of $x$.

Since the correlation is 0, the covariance between $u$ and any $x_1, \dots, x_k$ must also be 0:

$$
\begin{split}
& Cov(x_1,u) = 0 \\
& Cov(x_2, u) = 0
\end{split}
$$

-   and so on for $x_{3i}, ..., x_{ki}$.

<br />

This alternative statement relating to correlation/covariance is called **exogeneity**.

-   If this assumption is violated (where $x_j$ is correlated with error $u$), then we have **endogeneity**, and $x_j$ is considered an **endogeneous regressor**.
-   We will discuss endogeneity in more detail in lesson 1.7.

This is the most frequently violated of assumptions, yet arguable the most important of assumptions to ensure OLS is unbiased.

-   We will mathematically show this in the next lesson.
-   Because it is so frequently violated, we will need to introduce further methods beyond simple linear regression for accurate estimation.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.3: Proof of Unbiasedness of OLS under Gauss-Markov

In the last section, we covered the Gauss-Markov assumptions, and how when they are met, OLS is unbiased. In this section, we will use those assumptions to prove the unbiasedness of OLS.

-   Or in other words, we want to show $E(\hat\beta_j) = \beta_j$.

For simplicity, let us focus on $\hat\beta_1$. However, this can be generalised to any $\hat\beta_2, \dots, \hat\beta_k$.

::: callout-note
## Useful Properties of Summations

Before we start, here are a few key properties of summation

**Property 1:**

$$
\sum\limits_{i=1}^n \widetilde{r_{1i}} = 0
$$

-   This is because $\widetilde{r_{1i}}$ is a residual term of a OLS regression of outcome $x_1$ and explanatory variables $x_2, \dots, x_k$, and we know OLS residuals sum to 0 (algebraic properties discussed in [1.7.1](https://statsnotes.github.io/theory/7.html#residuals-and-algebraic-properties-of-ols)).

**Property 2:**

$$
\sum\limits_{i=1}^n \widetilde{r_{1i}} x_{ji} = 0, \text{ given } j=2, \dots, k 
$$

-   Because for OLS, $\sum x_i \hat u_i = 0$ ([1.7.1](https://statsnotes.github.io/theory/7.html#residuals-and-algebraic-properties-of-ols)), and we know $\widetilde{r_{1i}}$ is the residual $\hat u_i$ in a regression with explanatory variables $x_2, \dots, x_k$ and outcome variable $x_1$.

**Property 3:**

$$
\sum\limits_{i=1}^n \widetilde{r_{1i}} x_{1i} = \sum\limits_{i=1}^n \widetilde{r_{1i}}(\hat x_{1i} + \sum\limits_{i=1}^n \widetilde{r_{1i}}) = \sum\limits_{i=1}^n \widetilde{r_{1i}}^2
$$

-   Because we have the regression fitted values $\hat x_{1i} = \hat \gamma_0 + \hat\gamma_1 x_2 + \dots + \hat\gamma_{k-1} x_{k} + \widetilde{r_{1i}}$ from [1.6.6](https://statsnotes.github.io/theory/6.html#regression-anatomy-formula-for-ols-estimation).
-   And we know with regression, actual values are the predicted plus residual: $y_i = \hat y_i + \hat u_i$. Thus, $x_i = \hat x_i + \widetilde{r_{1i}}$.
:::

<br />

Recall the regression anatomy solution of OLS for $\hat\beta_1$ (derived in [1.6.6](https://statsnotes.github.io/theory/6.html#regression-anatomy-formula-for-ols-estimation)):

$$
\hat\beta_1 = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ y_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
$$

-   Where $\widetilde{r_{1i}}$ is the part of $x_1$ uncorrelated with $x_2, \dots, x_k$.

::: callout-note
## MLR.3 No Perfect Multicollinearity

The existence of $\hat\beta_1$ is guaranteed by condition 3 from above, since the denominator $\sum\widetilde{r_{1i}}^2$ must not be 0.
:::

Now, let us plug in $y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i$ into our regression anatomy formula:

$$
\begin{split}
\hat\beta_1 & = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ y_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
& = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ (\beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i)}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
\end{split}
$$

Now, focusing on the numerator, and using the summation properties above, let us simplify:

$$
\begin{split}
& \sum\limits_{i=1}^n \widetilde{r_{1i}} \ (\beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i) \\
& = \sum\limits_{i=1}^n (\widetilde{r_{1i}}\beta_0 + \widetilde{r_{1i}}\beta_1x_{1i} + \dots + \widetilde{r_{1i}}\beta_k x_{ki} + \widetilde{r_{1i}} u_i) \\
& = \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_0 + \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_1 x_{1i} + \dots + \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_k x_{ki} + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
& = \beta_0 \sum\limits_{i=1}^n \widetilde{r_{1i}} + \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}} x_{1i} + \dots + \beta_k \sum\limits_{i=1}^n \widetilde{r_{1i}} x_{ki} + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
& = \beta_0 (0) + \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}}^2 + \beta_2 (0) + \dots + \beta_k (0) + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
& = \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}}^2 + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i
\end{split}
$$

Now, putting the numerator back in, we can simplify:

$$
\begin{split}
\hat\beta_1 & = \frac{\beta_1 \sum_{i=1}^n \widetilde{r_{1i}}^2 + \sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
& \\
& = \frac{\beta_1 \sum_{i=1}^n \widetilde{r_{1i}}^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2} + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
& \\
& = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
$$

<br />

Now, we want to find $E(\hat\beta_1)$. Note that the second part of the equation is a function of $u_i$, of which itself is a function of all explanatory variables $x_{1i}, \dots, x_{ki}$. Thus, we know:

$$
\begin{split}
E(\hat\beta_1 | x_{1i}, \dots x_{ki}) & = \beta_1 + E\left( \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \biggr|x_{1i},\dots, x_{ki} \right) \\
& = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ E(u_i|x_{1i}, \dots,x_{ki})}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
$$

But what is $E(u_i|x_{1i}, \dots , x_{ki})$? We can use two Gauss-Markov conditions to evaluate this:

::: callout-note
## MLR.4 Zero Conditional Mean

The Zero-Conditional Mean assumption says $E(u|x_1, \dots, x_k) = 0$.
:::

::: callout-note
## MLR.2 Random Sampling

Random sampling allows us to say that $E(u|x_1, \dots, x_k)$ is the same as $E(u_i|x_{1i}, \dots, x_{ki})$, where both equal 0.
:::

Thus, with these two assumptions, we know that:

$$
E(u_i|x_{1i}, \dots, x_{ki}) = 0
$$

Which means that:

$$
\begin{split}
E(\hat\beta_1 | x_{1i}, \dots x_{ki}) 
& = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ E(u_i|x_{1i}, \dots,x_{ki})}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
& = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ (0)}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
& = \beta_1 + 0 \\
& = \beta_1
\end{split}
$$

Now, just like in simple linear regression (see [1.5.3](https://statsnotes.github.io/theory/5.html#proof-of-unbiasedness-of-ols-under-gauss-markov)), we use the **law of iterated expectations** to conclude this proof:

$$
\begin{split}
E(\hat\beta_1) & = E[E(\hat\beta_1|x_{1i}, \dots, x_{ki})] \\
& = E(\beta_1) \\
& = \beta_1
\end{split}
$$

Thus, OLS is unbiased under the Gauss-Markov conditions.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.4: Omitted Variable Bias

In the previous section (and in simple linear regression section [1.5.2](https://statsnotes.github.io/theory/5.html#unbiasedness-of-ols-under-the-gauss-markov-theorem)), we discussed how the assumption of **zero-conditional mean** $E(u | x_1, \dots, x_k) = 0$ is critical to the unbiasdness of OLS.

-   This assumption is also called **exogeneity**.

However, this assumption is frequently violated. The most common reason for this is because of **omitted variable bias**.

<br />

Consider two regressions, one with only our exlanatory variable of interest $D$, and, and one with an extra control variable $x$ that is omitted from the first regression:

$$
\begin{split}y_i & = \beta_0^S + \beta_1^SD_i + u_i^S \quad \text{short} \\y_i & = \beta_0 + \beta_1D_i + \beta_2x_i + u_i \quad \text{long}\end{split}
$$

Now consider an auxiliary regression, where the omitted variable $x$ is the outcome variable, and $D_i$ is the explanatory variable:

$$
x_i = \delta_0 + \delta_1 D_i + v_i
$$

-   where $\delta_0, \delta_1$ are coefficients and $v_i$ is the error term

<br />

Now we have $x_i$ in terms of $D_i$, let us plug $x_i$ into our long regression to "recreate" the short regression:

$$
\begin{split}y_i & = \beta_0 + \beta_1D_i + \beta_2x_i + u_i \\y_i & = \beta_0 + \beta_1 D_i + \beta_2(\delta_0 + \delta_1D_i + v_i) + u_i \\y_i & = \beta_0 + \beta_1 D_i + \beta_2 \delta_0 + \beta_2 \delta_1 D_i + \beta_2v_i + u_i \\y_i & = \beta_0 + \beta_2 \delta_0 + (\beta_1 + \beta_2 \delta_1)D_i + \beta_2v_i + u_i\end{split}
$$

We have "recreated" the short regression with one variable $D$. Let us see our recreation next to the original short regression:

$$
\begin{split}
y_i & = \beta_0^S + \beta_1^SD_i + u_i^S \\
y_i & = \beta_0 + \beta_2 \delta_0 + (\beta_1 + \beta_2 \delta_1)D_i + \beta_2v_i + u_i
\end{split}
$$

-   The short regression $\beta_0^S$ is analogous to the $\beta_0 + \beta_2 \delta_0$ in the recreation (both are the intercepts)
-   The short regression $\beta_1^S D_i$ is analogous to the $(\beta_1 + \beta_2 \delta_1)D_i$ in the recreation (both are the slope and variable of interest)
-   The short regression $u_i^S$ is analogous to the $\beta_2 v_i + u_i$ in the recreation (both are the error terms).

Since the short regression $\beta_1^S D_i$ is analogous to the $(\beta_1 + \beta_2 \delta_1)D_i$ in the recreation, that means coefficient $\beta_1^S = \beta_1 + \beta_2 \delta_1$.

<br />

The difference between the short regression coefficient $\beta_1^S$, and the original long regression coefficient $\beta_1$, is $\beta_2 \delta_1$.

-   Or in other words, $\beta_2 \delta_1$, which is actually some of the effect of omitted $x$ on $y$, is being "tangled up" into the coefficient of the short regression $\beta_1^S$ (which is only supposed to describe the relationship between $D$ and $y$).

If $\beta_2 = 0$ (meaning no relationship between omitted $x_i$ and $y$), or $\delta_1 = 0$ (meaning no relationship between omitted $x_i$ and $D_i$), then difference $\beta_2 \delta_1 = 0$, thus there is no issue.

-   But if either of those facts are not true, then $\beta_2 \delta_1 ≠ 0$, and omitted variable bias is non-zero.

In the next section, we will talk about the implications of this.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.5: Endogeneity and Violation of Gauss-Markov

In the last section, we discussed omitted variable bias. But why is omitted variable bias (if non-zero) an issue?

-   The omitted variable $x$'s effect is mostly subsumed into the error term $u_i^S$ of the short regression.
-   But some bit of $x$ (that is correlated with $D$) is included in our coefficient (specifically, $\beta_2 \delta_1$).
-   That means our treatment variable $D$ will be correlated with the error term, [violating the Gauss-Markov assumption of zero-conditional mean, and producing biased OLS results]{.underline}.

::: callout-tip
## Definition: Endogeneity

Endogeneity is when a regressor/explanatory variable $x_j$ is correlated with the error term $u_i$.

-   This frequently occurs due to **omitted variable bias**.
-   The explanatory variable that is correlated with the error term is called an **endogenous variable**.

Mathematically, endogeneity is defined as when:

$$
Cov(x_j, u) ≠ 0
$$

When endogeneity exists, the assumption of zero-conditional mean/exogeneity $E(u|x_1, \dots, x_k) = 0$ is violated (this assumption was introduced in [1.7.2](https://statsnotes.github.io/theory/7.html#unbiasedness-of-ols-under-the-gauss-markov-theorem)).

-   This means that OLS estimates are no longer unbiased if endogeneity/an endogenous variable are present.
:::

<br />

[Thus, the violation of the MLR.4 Zero-Conditional Mean Assumption frequently results from omitting confounding variables that are correlated with both our explanatory variable of interest and the outcome variable.]{.underline}

-   Endogeneity can also be caused by other factors, such as measurement error, or simultaneity (when both the explanatory variable and outcome variable explain each other). However, the primary concern that we worry about in econometrics is omitted variable bias.

How can we solve this problem?

-   The easiest way is to add more control variables.
-   If we include every single possible confounder that is correlated both with our explanatory variable of interest, and the outcome variable, then, there will be no more omitted variable bias, and no more endogeneity.

However, as we will discuss a lot more in Part II on causal inference, including all confounding variables is often impossible, as many social science situations will have thousands of them, many of them **unobservable** or impossible to measure.

-   Thus, there are also other techniques to "unbias" OLS without including all possible confounders. We will discuss an example of this in lesson 1.11 on instrumental variables.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.6: OLS as a Method of Moments Estimator

So far, we have discussed why endogeneity is an issue through omitted variable bias. However, there is another way to show the issue with endogeneity and why it causes bias - by describing the OLS estimator as a method of moments estimator.

::: callout-tip
## Definition: Method of Moments Estimator

The Method of Moments Estimator is another estimator of the true value of populations in the parameter.

The estimator defines key population **moments** of interest - which are the population parameters written in terms of expected values.

Then, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter.

-   For example, to estimate the population mean, the Method of Moments uses the sample mean.

We should have **at least** as many population moments, as we have estimands.
:::

<br />

This is a little hard to grasp. However, a simple example of the Method of Moments estimator can help us understand this.

Let us say that we have some random variable $y$, with a true population mean $\mu$. We want to estimate $\mu$, but we only have a sample of the population

First, let us define the moment of interest $\mu$ in terms of expectation:

$$
\mu = E(y)
$$

The method of moments says we should use the sample equivalent of the population parameter. The sample equivalent of $\mu$ (the true mean of the population), is of course, the sample mean $\bar y$.

With the method of moments estimator, our "moment" estimate of the true mean $\mu$ of $y$ would be:

$$
\hat\mu = \bar y = \frac{1}{n}\sum\limits_{i=1}^ny_i
$$

<br />

Another example of a Method of Moments estimator is if we have some random variable $y$, and we want to estimate the population mean $\mu$ and variance $\sigma^2$.

Here, we have two population moments $\mu$ and $\sigma^2$, that can be expressed in expected value:

$$
\begin{split}
& \mu = E(y) \\
& \sigma^2 = E((y - \mu)^2)
\end{split}
$$

The method of moments estimator uses the sample equivalents of the population parameters to estimate it. Thus, our "moment" estimates are:

$$
\begin{split}
& \hat\mu = \bar y \\
& \hat\sigma^2 = S.Var(y)
\end{split}
$$

-   Where $S.Var(y)$ is the variance of the sample $y$.

<br />

[OLS can also be considered a Method of Moments estimator]{.underline}. We will have moments for each parameter we are trying to estimate.

-   So for bivariate regression, we will have 2 moments for $\beta_0$ and $\beta_1$.
-   For multiple regression, we will have $k$ moments for $\beta_0, \beta_1, \dots, \beta_k$.

For example, let us take bivariate regression as an example.

::: callout-tip
## Definition: Bivariate Regression as a Method of Moments Estimator

Consider the bivariate regression model:

$$
y = \beta_0 + \beta_1x + u
$$

The OLS estimator can be derived as a method of moments estimator, with 2 moments, one for each parameter ($\beta_0, \beta_1$):

$$
\begin{split}
& E(y-\beta_0 -\beta_1x) = 0 \\
& E(x(y - \beta_0 - \beta_1 x)) = 0
\end{split}
$$

Since we know $u = y - \beta_0 - \beta_1 x$, we can rewrite the two moments as:

$$
\begin{split}
& E(u) = 0 \\
& E(xu) = 0
\end{split}
$$

The estimates of these moments would use the sample equivalents: $\hat\beta_0$ and $\hat\beta_1$.

$$
\begin{split}
& E(y-\hat\beta_0 -\hat\beta_1x) = 0 \\
& E(x(y - \hat\beta_0 - \hat\beta_1 x)) = 0
\end{split}
$$

These result in the same estimates as the OLS estimator.
:::

<br />

The moment conditions that define OLS as a method of moments are $E(u) = 0$ and $E(xu) = 0$. These two together imply that $Cov(x,u) = 0$ (a very similar proof provided in [1.5.1](https://statsnotes.github.io/theory/5.html#residuals-and-algebraic-properties-of-ols).

-   An explanatory variable $x$ is an **exogenous** regressor if $Cov(x,u) = 0$.

However, if $Cov(x,u) ≠ 0$, that means our moment conditions are invalid.

-   An explanatory variable $x$ is an **endogenous** regressor if $Cov(x,u)≠0$.

Thus, when an endogenous regressor is present, our moment conditions are invalid, and thus, our estimates are biased.

-   This is another way to show why endogeneity is an issue for regression, as we covered previously in [1.7.5](https://statsnotes.github.io/theory/7.html#endogeneity-and-violation-of-gauss-markov).

We will come back to the Method of Moments estimator in lesson 1.10, when we discuss the Instrumental Variables Estimator, and how that estimator solves the issue of the invalid moment due to endogeneity.

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)
