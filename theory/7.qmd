---
title: "Statistics for Social Scientists"
subtitle: "Lesson 1.7: Statistical Inference with the OLS Estimator"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Lesson 1.7: Statistical Inference with the OLS Estimator"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

::: callout-note
## Lesson Overview

In the last lesson, we discussed the properties of the OLS estimator. In this lesson, we use these OLS estimates to conduct statistical inference tests, and discuss some other properties of OLS from an inference perspective.

This lesson covers the following topics:

-   How standard errors are calculated under the assumption of homoscedasticity.
-   How hypothesis tests (including multiple coefficients at once) and confidence intervals are conducted.
-   How the presence of heteroscedasticity affects our standard errors, and how we can adjust for this with robust standard errors.
:::

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.1: Gauss-Markov Conditions and Homoscedasticity

We have proved that OLS is unbiased under 4 conditions in the last lesson.

However, if we recall from [1.2.8](https://statsnotes.github.io/theory/2.html#properties-of-estimators), unbiasedness is not the only thing we care about in an estimator. We also care about the estimator's variance.

For example, in the figure below, we have two unbiased estimators centred on the same real parameter value. However, the second estimator $w_2$ is far more varied than the first estimator $w_1$. That means while both are unbiased, the estimator $w_2$ often produces specific sample estimates that are farther from the true parameter value.

![](images/clipboard-4236264762.png){fig-align="center" width="80%"}

Luckily, the Gauss-Markov Theorem does not stop at unbiasedness. With an additional condition (homoscedasticity), we can determine that OLS is not only unbiased, but the linear estimator with the least variance:

::: callout-tip
## Definition: Gauss-Markov Theorem

The Gauss-Markov Theorem states that under 5 conditions, the OLS estimator is the Best Linear Unbiased Estimator (BLUE) for $\beta_1$, being the unbiased linear estimator with the least variance.

-   **SLR.1/MLR.1 (Linearity in Parameters)**: The parameters of the model are linear.
-   **SLR.2/MLR.2 (Random Sampling)**: All observations in our sample are randomly sampled from the population.
-   **SLR.3/MLR.3 (Sample Variation in** $x$**/ No Perfect Multicollinearity)**: $Var(x) ≠ 0$, and no perfect correlation between explanatory variables.
-   **SLR.4/MLR.4 (Zero Conditional Mean)**. The error term $u$ has an expectation of 0, given any value of the explanatory variables.
-   **SLR.5/MLR.5 (Homoscedasticity)**: The error term has the same variance given any value of $x$.
:::

<br />

But what does this new condition of homoscedasticity mean?

Mathematically, homoscedasticity is defined as:

$$
Var(u|x_1, \dots, x_k) = \sigma^2 \text{ for all } x
$$

-   Of course, for simple linear regression, there is only one $x$, so the condition is $Var(u|x) = \sigma^2$.

First of all, what even is the variance $Var(u)$ representing? It is the variance of the "errors" of our error term $u_i$, which is also a random variable. The figure below displays this, with the green lines representing the distribution of $u_i$.

![](images/clipboard-3513031173.png){fig-align="center" width="90%"}

In the figure above, you can see the variance (spread) of the error term $u_i$'s distribution is consistent, no matter the value of $x$. This means that homoscedasticity is met.

When $Var(u)$ is not constant (and changes with the value of $x$), we have **heteroscedasticity**.

The best way to identify if this assumption is met is to look at a plot of the residuals (errors). If the variance in the residuals is constnant, we have homoscedasticity. If not, we have heteroscedasticity.

![](images/clipboard-1713529842.png){fig-align="center" width="100%"}

In the figure above, we can see on the left chart, the variance of the residuals is clearly smaller when $x$ is lower, and the variance of the residuals is larger when $x$ is higher. That is a clear violation of the homoscedasticity assumption.

-   NOTE: heteroscedasticity (failure to meet SLR.5 homoscedasticity) does not bias OLS estimates. It only determines if OLS has the lowest variance of unbiased linear estimators.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.2: Variance of OLS Estimates in Simple Linear Regression

Assuming Homoscedasticity is met, we know $Var(u|x) = \sigma^2$.

-   When homoscedasticity is not met, nothing in this section applies. See [1.7.7](https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-bivariate-regression) for more information on how to do estimates when this is violated.

What is variance? From [1.1.5](https://statsnotes.github.io/theory/1.html#variance-and-standard-deviation), we know that the formula for variance is:

$$
E(x - \mu)^2
$$

We know from SLR.4 Zero-Conditional Mean assumption that $E(u|x) = 0$. Thus, we can use that to calculate the variance $Var(u|x)$ using the variance formula:

$$
\begin{split}
Var(u|x) = \sigma^2 & = E[ \ ((u|x) - E(u|x))^2 \ ] \\
& = E((u|x) - 0)^2 \\
& = E[(u|x)^2] \\
& = E(u^2 |x)
\end{split}
$$

And since by homoscedasticity, we know variance does not depend on $x$, so $Var(u|x) = Var(u)$. Thus, we also know that:

$$
E(u^2|x) = E(u^2) = \sigma^2
$$

<br />

Remember when we were proving unbiasedness of OLS in [1.6.3](https://statsnotes.github.io/theory/6.html#proof-of-ols-unbiasedness-in-simple-linear-regression), we got to this stage:

$$
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^n w_i u_i
$$

-   Where $w_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}$, which is a function of random variable $x$.
-   We could also write $w_i$ as $\frac{x_i - \bar{x}}{SST_x}$ (where $SST_x$ is total sum of squares for $x$).

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, $\sum w_i u_i$ is the variance in $\hat\beta_1$.

$$
\begin{split}
Var(\hat\beta_1|x) & = Var\left( \sum\limits_{i=1}^n w_i u_i \bigg| x\right) \\
& = \sum\limits_{i=1}^n Var(w_i u_i | x) \\
& = \sum\limits_{i=1}^n w_i^2 Var(u_i | x)
\end{split}
$$

And given SLR.2 Random Sampling (see [1.6.2](https://statsnotes.github.io/theory/6.html#unbiasedness-of-ols-under-the-gauss-markov-theorem)), we know $Var(u_i | x)$ is also equal to $Var(u_i|x_i)$. Thus:

$$
Var(\hat\beta_1|x) = \sum\limits_{i=1}^n w_i^2 Var(u_i | x_i)
$$

And using SLR.5 homoscedasticity, we know $Var(u|x) = \sigma^2$ and is constant, thus:

$$
\begin{split}
Var(\hat\beta_1 | x) & = \sum\limits_{i=1}^n w_i^2 Var(u_i | x_i) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma^2 \\
& = \sigma^2 \sum\limits_{i=1}^n w_i^2
\end{split}
$$

Remember, $w_i$ is its own function of $x$, where $w_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}$, or can be written as $\frac{x_i - \bar{x}}{SST_x}$ (see [1.6.3](https://statsnotes.github.io/theory/6.html#proof-of-ols-unbiasedness-in-simple-linear-regression) for derivation).

We have $\sum w_i^2$ in our final equation, and we can do some quick algebra to rearrange it (note, if you are not familiar with SST, see [1.4.6](https://statsnotes.github.io/theory/4.html#r-squared-and-goodness-of-fit)):

$$
\begin{split}
\sum\limits_{i=1}^n w_i^2 & = \sum\limits_{i=1}^n \frac{(x_i - \bar{x})^2}{(SST_x)^2} \\
& = \frac{\sum_{i=1}^n(x_i - \bar x)^2}{(SST_x)^2} \\
& = \frac{SST_x}{(SST_x)^2} \\
& = \frac{1}{SST_x}
\end{split}
$$Thus, we can plug that in to get our final variance of $\hat\beta_1$ formula:

$$
\begin{split}
Var(\hat\beta|x) & = \sigma^2 \sum\limits_{i=1}^n w_i^2 \\
& = \sigma^2 \frac{1}{SST_x} \\
& = \frac{\sigma^2}{SST_x}
\end{split}
$$

Thus, that is the variance of our OLS estimator $\hat\beta_1$, and also the variance of the sampling distribution of $\hat\beta_1$.

-   This is only the case if SLR.5 homoscedasticity assumption holds. It is not valid if the assumption is not met. See [1.7.7](https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-bivariate-regression) for more information on how to do estimates when this is violated.
-   One might notice we calculated $Var(\hat\beta_1|x)$, not $Var(\hat\beta_1)$. However, this does not matter, since our final formula does not depend on the value of $x$ (from homoscedasticity, we know $\sigma^2$ is independent of $x$, and also $SST_x$ is constant no matter the specific value of $x$ as it includes all values of $x$).

There is one issue: we know that $\sigma^2 = E(u^2)$ (see earlier in the section). However, we do not actually know the value of $E(u^2)$! Remember, that is the error term $u$ - while we only know the residual term $\hat u$.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.3: Standard Errors in Simple Linear Regression

We know, as shown in the last section, that the variance of the sampling distribution is:

$$
Var(\hat\beta) = \frac{\sigma^2}{SST_x}
$$

If we want the standard deviation of the sampling distribution, we simply take the square root:

$$
sd(\hat\beta_1) = \frac{\sigma}{\sqrt{SST_x}}
$$

There is one issue: we know that $\sigma^2 = E(u^2)$ (see previous section for proof). However, we do not actually know the value of $E(u^2)$! Remember, that is the error term $u$ - while we only know the residual term $\hat u$.

So, what we can do is well, simply replace $u$ with its estimate, $\hat u$.

-   Recall that $u_i = y_i - \beta_0 - \beta_1 x_i$
-   And $\hat u_i = y_i - \hat\beta_0 - \hat\beta_1 x_i$.

So naturally, instead of $\sigma^2 = E(u^2)$, we could estimate it with $E(\hat u^2)$. Mathematically:

$$
\begin{split}
\hat\sigma^2 = E(\hat u^2) & = \frac{1}{n} \sum\limits_{i=1}^n \hat u_i^2 \\
& = SSR/n
\end{split}
$$

-   Where $SSR$ is the square sum of residuals (see [1.4.6](https://statsnotes.github.io/theory/4.html#r-squared-and-goodness-of-fit) for more details).

However, there is an issue with this estimate of $\sigma^2$. It is biased - the expected value is actually slightly less than $\sigma^2$.

We will not prove this mathematically, but this bias is because OLS imposes two conditions on its estimation process (that we discussed in [1.6.1](https://statsnotes.github.io/theory/6.html#residuals-and-properties-of-ols-estimator)):

$$
\begin{split}
& \sum\limits_{i=1}^n \hat u_i = 0 \\
& \sum\limits_{i=1}^n x_i \hat u_i = 0
\end{split}
$$

-   The actual error term $u_i$ (not the OLS residuals $\hat u_i$) do not have these restrictions.

We can adjust the estimator to be more accurate by including a degrees of freedom adjustment. So, instead of $SSR/n$, we can do $SSR/(n-2)$. Thus, our estimator for $\sigma^2$ is:

$$
\hat\sigma^2 = \frac{SSR}{n-2} = \frac{\sum_{i=1}^n \hat u_i^2}{n-2}
$$

With that estimate of $\sigma^2$, we can plug it back into our formula for the standard deviation of $\hat\beta_1$.

-   We call this standard deviation the standard error (as discussed in [1.2.3](https://statsnotes.github.io/theory/2.html#uncertainty-in-estimates-and-sampling-distributions))

$$
\widehat{se}(\hat\beta_1) = \frac{\hat\sigma}{\sqrt{SST_x}}
$$

::: callout-tip
## Definition: Standard Errors for Simple Linear Regression

The standard error for the coefficient $\hat\beta_1$ from an OLS estimator in a simple linear regression is:

$$
\widehat{se}(\hat\beta_1) = \frac{\hat\sigma}{\sqrt{SST_x}}
$$

Where $\hat\sigma$ is defined as:

$$
\hat\sigma = \sqrt{\frac{SSR}{n-2}} = \sqrt{\frac{\sum_{i=1}^n \hat u_i^2}{n-2}}
$$

And where $SST_x$ is defined as:

$$
SST_x = \sum_{i=1}^n(x_i - \bar x)^2
$$
:::

When homoscedasticity is not met, this standard error is invalid. See [1.7.7](https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-bivariate-regression) for more information on how to do estimates when this is violated.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.4: Standard Errors in Multiple Linear Regression

Now, we will do the same exercise, but for multiple linear regression.

Assuming homoscedasticity is met, we know $Var(u|x_1, \dots, x_k) = \sigma^2$.

-   When homoscedasticity is not met, nothing in this section applies. See [1.7.8](https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-multiple-regression) for more information on how to do estimates when this is violated.

Let us find the variance of OLS estimates (we will use $\hat\beta_1$ for simplicity, but this applies to any other coefficient $\hat\beta_1 , \dots, \hat\beta_k$. In proving unbiasedness of OLS in [1.6.4](https://statsnotes.github.io/theory/6.html#proof-of-ols-unbiasedness-in-multiple-linear-regression-1), we got to this stage:

$$
\hat\beta_1 = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
$$

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, the second part is the variance in $\hat\beta_1$.

Let us define $w_i$ as following, as a function of $x_1, \dots, x_k$:

$$
w_i = \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
$$

This allows us to write $\hat\beta_1$ as:

$$
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^nw_i u_i
$$

<br />

Thus, we can proceed in the same way as the simple linear regression case (see [1.7.2](https://statsnotes.github.io/theory/7.html#variance-of-ols-estimates-in-simple-linear-regression) for more details):

$$
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) & = Var\left( \sum\limits_{i=1}^nw_i u_i \biggr|x_1, \dots ,x_k \right) \\ & = \sum\limits_{i=1}^n Var(w_i u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \ Var(u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma^2 \\
& = \sigma^2 \sum\limits_{i=1}^n w_i^2
\end{split}
$$

Now, plugging back in $w_i$, we get:

$$
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) & = \sigma^2 \sum\limits_{i=1}^n w_i^2 \\
& = \sigma^2 \sum_{i=1}^n \left( \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \right) \\
& = \sigma^2 \frac{\sum_{i=1}^n \widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \\
& = \frac{\sigma^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
$$

And assuming homoscedasticity (where $Var(\hat\beta_1)$ does not depend on $x_1, \dots, x_k$, we thus know:

$$
Var(\hat\beta_1) = \frac{\sigma^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
$$

-   When homoscedasticity is not met, this is not valid. See [1.7.8](https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-multiple-regression) for more information on how to do estimates when this is violated.

<br />

However, just like we mentioned in [1.7.3](https://statsnotes.github.io/theory/7.html#standard-errors-in-simple-linear-regression), the real value of $\sigma^2$ of a regression not calculable. This is an issue because our variance formula has $\sigma^2$ in the numerator.

Just like in simple linear regression (see [1.7.3](https://statsnotes.github.io/theory/7.html#standard-errors-in-simple-linear-regression)), we can estimate $\sigma^2$ by using the residual term $\hat u_i$ with a degrees of freedom adjustment.

$$
\hat\sigma^2 = \frac{SSR}{n-k-1} = \frac{\sum_{i=1}^n \hat u_i^2}{n-k-1}
$$

-   Where $n$ is the number of observations in our sample data.
-   Where $k$ is the number of explanatory variables in our model.

Thus, with this estimate, we can calculate the **standard errors** (square root of variance) of our estimates of coefficients $\hat\beta_j$.

::: callout-tip
## Definition: Standard Errors for Multiple Linear Regression

The standard error for the coefficient $\hat\beta_j$ from an OLS estimator in multiple linear regression is:

$$
\widehat{se}(\hat\beta_j) = \frac{\hat\sigma}{\sqrt{\sum_{i=1}^n \widetilde{r_{1i}}^2}}
$$

-   We will never calculate this by hand, we will use a statistical software to do this.

Where $\hat\sigma$ is defined as:

$$
\hat\sigma = \sqrt{\frac{SSR}{n-k-1}} = \sqrt{\frac{\sum_{i=1}^n \hat u_i^2}{n-k-1}}
$$
:::

When homoscedasticity is not met, this is not valid. See [1.7.8](https://statsnotes.github.io/theory/7.html#robust-standard-errors-for-multiple-regression) for more information on how to do estimates when this is violated.

<br />

Notice how the denominator contains $\sum\widetilde{r_{1i}}^2$. That means, the smaller $\sum\widetilde{r_{1i}}^2$ is, the larger the variance is.

-   $\sum\widetilde{r_{1i}}^2$ is smaller when our explanatory variable of interest $x_1$ is highly correlated with another explanatory variable, since $\widetilde{r_{1i}}^2$ represents the part of $x_1$ that is uncorrelated with other explanatory variables.
-   This means that if we have highly correlated explanatory variables, our variance will in our OLS estimates will be high.

This is something important to think about when choosing explanatory variables. We want to include all confounding variables, but highly correlated confounders will make our variance very high for our estimates.

-   One way of dealing with this is dimensional reduction techniques, which we will discuss in Part III of the guide dealing with multivariate analysis.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.5: Statistical Inference in Simple Linear Regression

In the last few sections, we calculated the standard error of OLS estimates of $\hat\beta_j$.

If we remember from [Lesson 1.2](https://statsnotes.github.io/theory/2.html), the standard error is a measurement of uncertainty, and allows us to conduct statistical inference tests, such as hypothesis testing and confidence intervals.

-   Note: the statistical tests are only really useful if we are comfortable with the assertion that OLS is unbiased - or in other words, they are only really useful if the first 4 Gauss-Markov conditions are met.

<br />

### Hypothesis Testing

Hypothesis testing follows the same procedure as outlined in [1.2.5](https://statsnotes.github.io/theory/2.html#intuition-of-hypothesis-testing) and [1.2.6](https://statsnotes.github.io/theory/2.html#implementing-a-hypothesis-test). Reread these sections to understand the intuition.

In regression, our typical null hypotheses is that there is no relationship between $x_j$ and $y$, and our alternate hypothesis is that there is a relationship between $x_j$ and $y$. Thus, our hypotheses are:

$$
\begin{split}
& H_0 : \beta_j = 0 \\
& H_1: \beta_j ≠ 0
\end{split}
$$

Now, we calculate a t-test statistic:

$$
t = \frac{\hat\beta_j - 0}{\widehat{se}(\hat\beta_j)}
$$

-   Where the 0 represents the null hypothesis value. If you have any other null hypothesis value, change the 0 to your hypothesis value.

Now, we will consult a t-distribution (not a normal distribution) to calculate the p-values.

-   We use a t-distribution, not a normal distribution, even if we have met the central limit theorem (see [1.2.4](https://statsnotes.github.io/theory/2.html#central-limit-theorem) if you do not know what the central limit theorem is).

-   The reason we do this is because our estimate of variance $\hat\sigma^2$ is not exactly the same as the true variance $\sigma^2$, and the t-distribution better accounts for this issue.

Once we have obtained our p-values from the t-distribution, we can interpret the p-values as follows:

::: callout-tip
## Interpretation of p-Values for Regression

The p-value is the probability of getting a test statistic equally or more extreme than the one we got with our sample estimate $\hat\beta_j$, given the null hypothesis is true.

-   If $p<0.05$, we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between $x_j$ and $y$), and conclude our alternate hypothesis (that there is a relationship between $x_j$ and $y$).

-   If $p>0.05$, we cannot reject the null hypothesis, and cannot reject that there is no relationship between $x_j$ and $y$.
:::

::: callout-warning
## Warning!

Note how I have been using the word **relationship**, not causal effect.

Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).
:::

<br />

### Confidence Intervals

We can also create confidence intervals of plausible true $\beta_j$ values from the population, given our estimate $\hat\beta_1$. The intuition is the same as discussed in [1.2.7](https://statsnotes.github.io/theory/2.html#confidence-intervals).

Just like previously discussed in [1.2.7](https://statsnotes.github.io/theory/2.html#confidence-intervals), the 95% confidence interval has the bounds:

$$
\hat\beta_j - 1.96 \widehat{se}(\hat\beta_1), \ \hat\beta_j + 1.96 \widehat{se}(\hat\beta_1)
$$

::: callout-tip
## Interpretation of Confidence Intervals

The confidence interval means that under repeated sampling and estimating $\hat\beta_j$, 95% of the confidence intervals we construct will include the true $\beta_j$ value in the population.
:::

::: callout-warning
## Interpretation Warning!

It is very important to note that confidence intervals do not mean a 95% probability that the true $\beta_j$ is within any specific confidence interval we calculated.

We cannot know based on one confidence interval, whether it covers or does not cover the true $\beta_j$.

The correct interpretation is that over many samples from the same population, we would expect 95% of our confidence intervals to contain the true $\beta_j$ value.
:::

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.6: Hypothesis Testing with More than One Coefficient

Sometimes, we want to test more than one coefficient at a time in a hypothesis test.

-   This will be especially obvious why after [lesson 1.8](https://statsnotes.github.io/theory/8.html).

For example, let us say we want to test the statistical significance of $\hat\beta_2$ and $\hat\beta_3$ at the same time in the following regression model:

$$
\hat y = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_3
$$

What we can do is create two models - the alternate model $M_a$, and the null model $M_0$. The alternate model $M_a$ is the model we have above, and the null modle $M_0$ is the model without the two coefficients that we want to test ($\hat\beta_2$ and $\hat\beta_3$).

$$
\begin{split}
& M_0: y = \beta_0 + \beta_1x_1 \\
& M_a: y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3x_3
\end{split}
$$

Our statistical test will be to test if the alternative model $M_a$ is significantly "better" than our null model $M_0$. If $M_a$ is indeed significantly better, than we know that the coefficients $\hat\beta_2$ and $\hat\beta_3$ together are statistically significant.

Let us generalise this framework.

::: callout-tip
## Definition: F-Test of Nested Models

The F-test of Nested Models allows us to test multiple coefficients at once. It compares two models: $M_0$ and $M_a$.

$$
\begin{split}
& M_0: y = \beta_0 + \beta_1x_1 + \dots + \beta_g x_g \\
& M_a: y = \beta_0 + \beta_1 x_1 + \dots + \beta_g x_g + \beta_{g+1} x_{g+1} + \dots + \beta_kx_k
\end{split}
$$

-   The model $M_a$ contains all of the explanatory variables, including the ones we want to test.
-   The model $M_0$ contains the other explanatory variables that are not a part of our test. Model $M_0$ must be "nested" in model $M_a$: i.e. all explanatory variables present in $M_0$ must also be in $M_a$.

The model tests if $M_a$ is significantly better than $M_0$. If this is the case, the extra coefficients in $M_a$ that we are testing are statistically significant.
:::

<br />

How do we run a F-test of nested models?

Recall the concept of $R^2$ discussed in [1.4.6](https://statsnotes.github.io/theory/4.html#r-squared-and-goodness-of-fit). $R^2$ describes how much of the variation in $y$ our explanatory variables explain.

The F-test uses the $R^2$ of the two models, and compares them.

-   If the $M_a$ model has a statistically significantly higher $R^2$ value than the $M_0$ model, then $M_a$ is considered statistically significant, and we can conclude that the additional explanatory variables in $M_a$ are statistically significant.

<br />

As we know from hypothesis testing (see [1.2.5](https://statsnotes.github.io/theory/2.html#intuition-of-hypothesis-testing) and [1.2.6](https://statsnotes.github.io/theory/2.html#implementing-a-hypothesis-test) for intuition), we need a test statistic and distribution to run a hypothesis test.

The statistic for a F-test is the F-statistic.

-   Let us define $R^2_a$ and $SSR_a$ as the $R^2$ and sum of squared residuals for model $M_a$.
-   Let us define $R^2_0$ and $SSR_0$ as the $R^2$ and sum of squared residuals for model $M_0$.
-   The total number of coefficients in model $M_a$ is $k_a$, and for model $M_0$, is $k_0$.
-   Let us define $n$ as the number of observations (should be the same for both models).

Our F-statistic is mathematically calculated as:

$$
F = \frac{(SSR_0 - SSE_a)/(k_a - k_0)}{SSR_a / (n - k_a - 1)}
$$

After calculating our F-statistic, we consult an F-distribution with $k_a - k_0$ and $n-k_a - 1$ degrees of freedom.

With this distribution, we can obtain our p-value.

::: callout-tip
## Interpretation of p-Values for F-tests

The p-value is the probability of getting a test statistic equally or more extreme than the one we got with our alternate model $M_a$, given the null hypothesis is true.

-   If $p<0.05$, we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that $M_0$ is a better model), and conclude our alternate hypothesis (that $M_a$ is a better model). This also means that the extra coefficients in $M_a$ are jointly statistically significant.

-   If $p>0.05$, we cannot reject the null hypothesis, and cannot reject that $M_0$ is the better model. Thus, the extra coefficients in $M_a$ are jointly not statistically significant.
:::

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.7: Robust Standard Errors for Bivariate Regression

MLR.5 Homoscedasticity states that $Var(u|x_1, \dots x_k) = \sigma^2$.

-   Or in other words, no matter the value of $x_1, \dots, x_k$, the variance of the error term is constant at $\sigma^2$.

When this is violated (so when the values of the explanatory variables effect the variance of the error term), we have **heteroscedasticity**.

-   We provided graphical examples of homoscedasticity in [1.7.1](https://statsnotes.github.io/theory/7.html#gauss-markov-conditions-and-homoscedasticity). This is useful to know how to identify heteroscedasticity.

When MLR.5 Homoscedasticity is violated, the OLS estimator is still unbiased, however, it is no longer the unbiased linear estimator with the least variance.

More importantly for us however, is that heteroscedasticity invalidates the variance and standard error formulas we have calculated earlier.

<br />

Let us calculate these standard errors under heteroscedasticity. For simplicity, consider the bivariate regression:

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

However, without MLR.5 Homoscedasticity, variance $Var(u|x)$ is no longer constant at $\sigma^2$. Instead, $Var(u|x)$ varies depending on the value of $x$. Let us define the variance as:

$$
Var(u_i|x_i) = \sigma_i^2
$$

-   Where $\sigma^2_1$ is the error term variance for $x_1$, and $\sigma_2^2$ is the error term variance for $x_2$, and so on $\sigma_3^2, \dots, \sigma_n^2$.

Then, recalling from the start of [1.7.2](https://statsnotes.github.io/theory/7.html#variance-of-ols-estimates-in-simple-linear-regression), we have the following formula for the simple linear regression $\hat\beta_1$ estimate:

$$
\hat\beta_1 = \beta_1 + \sum_{i=1}^nw_i u_i
$$

-   Where $w_i = \frac{x_i - \bar x}{\sum(x_i - \bar x)^2}$, which is a function of random variable $x$.
-   We could also write $w_i$ as $\frac{x_i - \bar x}{SST_x}$.

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, the second part is the variance in $\hat\beta_1$. Thus:

$$
\begin{split}
Var(\hat\beta_1|x) & = Var \left( \sum\limits_{i=1}^n w_i u_i \biggr| x \right) \\
& = \sum\limits_{i=1}^n Var(w_i u_i |x) \\
& = \sum\limits_{i=1}^n w_i^2 \ Var (u_i | x) \\
& = \sum\limits_{i=1}^nw_i^2 \ Var(u_i|x_i) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma_i^2
\end{split}
$$

Now, plugging back $w_i$ in, we get:

$$
\begin{split}
Var(\hat\beta_1|x) & = \sum\limits_{i=1}^n w_i^2 \sigma_i^2 \\
& = \sum\limits_{i=1}^n \left(\frac{x_i - \bar x}{SST_x} \right)^2 \sigma^2_i \\
& = \frac{\sum_{i=1}^n (x_i - \bar x)^2 \sigma^2_i}{SST_x^2}
\end{split}
$$

Of course, just like with homoscedasticity, we do not know $\sigma_i^2$, and have to estimate it with $\hat u$. We do not need a degrees of freedom adjustment in this case. Thus, our estimate of variance is:

$$
\widehat{Var}(\hat\beta_1|x) = \frac{\sum_{i=1}^n (x_i - \bar x)^2 \hat u^2_i}{SST_x^2}
$$

And thus, the standard error (square root of variance) of our estimate $\hat\beta_1$ under homoscedasticity is the square root of our estimate of variance.

::: callout-tip
## Definition: Robust Standard Error for Bivariate Regression

The heteroscedasticity-robust standard error for $\hat\beta_1$ in bivariate regression is:

$$
\widehat{se}(\hat\beta_1|x) = \sqrt{\frac{\sum_{i=1}^n (x_i - \bar x)^2 \hat u^2_i}{SST_x^2}}
$$

-   Where SST is the total sum of squares $SST = \sum(y_i - \bar y)^2$.
:::

We can use these robust standard errors to conduct hypothesis testing and confidence intervals as normal.

Nowadays, it is typical to use heteroscedasticity-robust standard errors as default, and only using normal standard errors if we can prove homoscedasticity is met.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.7.8: Robust Standard Errors for Multiple Regression

Let us do the same as the last section, but for multiple regression.

Without MLR.5 Homoscedasticity, variance $Var(u|x)$ is no longer constant at $\sigma^2$. Instead, $Var(u|x)$ varies depending on the value of $x$. Let us define the variance as:

$$
Var(u_i|x_{1i}, \dots, x_{ki}) = \sigma_i^2
$$

Let us find the variance of OLS estimates (we will use $\hat\beta_1$ for simplicity, but this applies to any other coefficient $\hat\beta_1 , \dots, \hat\beta_k$. In proving unbiasedness of OLS in [1.6.4](https://statsnotes.github.io/theory/6.html#proof-of-ols-unbiasedness-in-multiple-linear-regression-1), we got to this stage:

$$
\hat\beta_1 = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
$$

We know that $\beta_1$ is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in $\hat\beta_1$. Thus, the second part is the variance in $\hat\beta_1$.

Let us define $w_i$ as following, as a function of $x_1, \dots, x_k$:

$$
w_i = \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
$$

This allows us to write $\hat\beta_1$ as:

$$
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^nw_i u_i
$$

<br />

Thus, we can proceed in the same way as the simple linear regression case in the last section.

$$
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) & = Var\left( \sum\limits_{i=1}^nw_i u_i \biggr|x_1, \dots ,x_k \right) \\ & = \sum\limits_{i=1}^n Var(w_i u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \ Var(u_i | x_1, \dots, x_k) \\
& = \sum\limits_{i=1}^n w_i^2 \sigma^2_i
\end{split}
$$

Now, plugging back in $w_i$, we get:

$$
\begin{split}
Var(\hat\beta_1|x) & = \sum\limits_{i=1}^n w_i^2 \sigma_i^2 \\
& = \sum\limits_{i=1}^n \left(\frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \right)^2 \sigma^2_i \\
& = \frac{\sum_{i=1}^n \widetilde{r_{1i}}^2 \sigma^2_i}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
\end{split}
$$

Of course, just like before, we do not know $\sigma_i^2$, and have to estimate it with $\hat u$. Thus, our estimate of variance is:

$$
\widehat{Var}(\hat\beta_1) = \frac{\sum_{i=1}^n \widetilde{r_{1i}}^2 \hat u_i^2}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
$$

And thus, the standard error (square root of variance) of our estimate $\hat\beta_1$ under homoscedasticity is the square root of our estimate of variance.

-   This can be generalised to any other coefficient $\hat\beta_1, \dots, \hat\beta_k$.

::: callout-tip
## Definition: Robust Standard Error for Multiple Regression

The heteroscedasticity-robust standard error for $\hat\beta_1$ in multiple regression is:

$$
\widehat{se}(\hat\beta_j) = \sqrt{\frac{\sum_{i=1}^n \widetilde{r_{ji}}^2 \hat u^2_i}{\sum_{i=1}^n\widetilde{r_{ji}}^2}}
$$
:::

We can use these robust standard errors to conduct hypothesis testing and confidence intervals as normal.

Nowadays, it is typical to use heteroscedasticity-robust standard errors as default, and only using normal standard errors if we can prove homoscedasticity is met.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# Implementation in R

```{r, message = FALSE, echo = FALSE}
dta <- readRDS('data/data1.rds')
```

## Hypothesis Testing with Homoscedasticity

To conduct hypothesis testing in a regression, we can use the *feols()* function from the package *fixest*, or we can use the base-R function *lm()*.

-   The syntax is the same for both (at least for now).
-   The *feols()* function is often used because it can incorporate heteroscedasticity-robust standard errors (see the next section), as well as other techniques in econometrics.

For the *feols()* function, we will need the *fixest* package. Make sure to install it if you have not previously (google how to install R-packages if needed).

```{r, message = FALSE}
library(fixest)
```

<br />

**Syntax:**

For the *feols()* function, the syntax is as follows:

```{r, eval = FALSE}
model <- feols(y ~ x1 + x2 + x3, data = mydata)
summary(model)
```

-   Replace *model* with any name you want to store your regression model in.
-   Replace *y* with your outcome variable name, and *x1, x2, x3* with your explanatory variable name.
-   You can add more explanatory variables by adding + signs and *x4 + x5 ...* and so on. You can also remove explanatory variables down to only 1.
-   Replace *mydata* with the name of your dataframe.

The *lm()* function has the exact same syntax for simple linear regression, except that we replace *feols()* with *lm()*:

```{r, eval = FALSE}
model <- lm(y ~ x1 + x2 + x3, data = mydata)
summary(model)
```

<br />

**Example:**

Let us run a regression with outcome variable *immatt* (attitude towards immigrants), explanatory variables *age* and *educ* (years of education), from the dataframe called *dta*:

```{r, message = FALSE}
my_model <- feols(immatt ~ age + educ, data = dta)
summary(my_model)
```

We can see the following output:

-   In the estimate column, we get our intercept estimate $\hat\beta_0$, and our explanatory variables coefficient estimates $\hat\beta_1$ and $\hat\beta_2$.
-   In the standard error column, we get our standard error of our sample estimates.
-   In the t-value column, we get our t-values for our sample estimates.
-   In the p-value column, we get our p-values for our sample estimates. Any asterisks \* indicate statistical significance.

The result is similar with *lm()*:

```{r, message = FALSE}
my_model <- lm(immatt ~ age + educ, data = dta)
summary(my_model)
```

<br />

## Hypothesis Testing with Heteroscedasticity

To conduct hypothesis testing with robust standard errors, we must use the *feols()* function from the package *fixest* (no *lm()* function).

For the *feols()* function, we will need the *fixest* package. Make sure to install it if you have not previously (google how to install R-packages if needed).

```{r, message = FALSE}
library(fixest)
```

<br />

**Syntax:**

For the *feols()* function, the syntax is as follows:

```{r, eval = FALSE}
model <- feols(y ~ x1 + x2 + x3, data = mydata, se = "hetero")
summary(model)
```

-   Replace *model* with any name you want to store your regression model in.
-   Replace *y* with your outcome variable name, and *x1, x2, x3* with your explanatory variable name.
-   You can add more explanatory variables by adding + signs and *x4 + x5 ...* and so on. You can also remove explanatory variables down to only 1.
-   Replace *mydata* with the name of your dataframe.

Note: the extra argument *se="hetero"* tells R to use heteroscedasticity-robust standard errors.

<br />

**Example:**

Let us run a regression with outcome variable *immatt* (attitude towards immigrants), explanatory variables *age* and *educ* (years of education), from the dataframe called *dta*:

```{r, message = FALSE}
my_model <- feols(immatt ~ age + educ, data = dta, se = "hetero")
summary(my_model)
```

We can see the following output:

-   In the estimate column, we get our intercept estimate $\hat\beta_0$, and our explanatory variables coefficient estimates $\hat\beta_1$ and $\hat\beta_2$.
-   In the standard error column, we get our standard error of our sample estimates.
-   In the t-value column, we get our t-values for our sample estimates.
-   In the p-value column, we get our p-values for our sample estimates. Any asterisks \* indicate statistical significance.

<br />

## Confidence Intervals

**Syntax:**

To estimate confidence intervals for our estimates, we will first need to run a regression and hypothesis tests (like above).

Then, we can either manually calculate our confidence intervals (as the standard errors are given in the regression output), or we can use the *confint* function:

```{r, eval = FALSE}
confint(model_name)
```

-   Replace *model_name* with whatever variable name you stored your regression in.

<br />

**Example:**

Let us find the confidence intervals for our estimates in the previous example regression above:

```{r}
confint(my_model)
```

We can see that R outputs the 95% confidence interval lower and upper bounds, for all our estimates $\hat\beta_0$ and $\hat\beta_1$ and $\hat\beta_2$

<br />

## F-Tests of Nested Models

**Syntax**

For f-tests, we will need two models. Let us save those two regression models as *m0* (null) and *m1* (alternate). Then, we can use the *anova()* function.

-   Note: our models must be created with the *lm()* function, not the *feols()* function, to perform an f-test. The reason for this will be explained in the next chapter.

```{r, eval = FALSE}
anova(m1, m2)
```

<br />

**Example:**

Let us create two different models:

```{r}
m0 <- lm(immatt ~ age, data = dta)
m1 <- lm(immatt ~ age + educ + female, data = dta)
```

So essentially, this F-test will be testing the coefficients of *educ* and *female* together.

Now, let us run the F-test:

```{r}
anova(m0, m1)
```

We can see model *m1* is statistically significant. Thus, the two coefficients of *educ* and *female* together are statistically significant.

-   Once again, we will explore in lesson 1.9 why the F-test can be very useful.

<br />

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)
