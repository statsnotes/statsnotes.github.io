<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistics for Social Scientists</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="6_files/libs/clipboard/clipboard.min.js"></script>
<script src="6_files/libs/quarto-html/quarto.js"></script>
<script src="6_files/libs/quarto-html/popper.min.js"></script>
<script src="6_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="6_files/libs/quarto-html/anchor.min.js"></script>
<link href="6_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="6_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="6_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="6_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="6_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Statistics for Social Scientists</h1>
            <p class="subtitle lead">Lesson 1.6: Multiple Regression and OLS Estimation</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Lesson 1.6: Multiple Regression and OLS Estimation</h2>
   
  <ul class="collapse">
  <li><a href="#motivation-behind-multiple-linear-regression" id="toc-motivation-behind-multiple-linear-regression" class="nav-link active" data-scroll-target="#motivation-behind-multiple-linear-regression">1.6.1: Motivation Behind Multiple Linear Regression</a></li>
  <li><a href="#the-multiple-linear-regression-model" id="toc-the-multiple-linear-regression-model" class="nav-link" data-scroll-target="#the-multiple-linear-regression-model">1.6.2: The Multiple Linear Regression Model</a></li>
  <li><a href="#multiple-linear-regression-with-linear-algebra" id="toc-multiple-linear-regression-with-linear-algebra" class="nav-link" data-scroll-target="#multiple-linear-regression-with-linear-algebra">1.6.3: Multiple Linear Regression with Linear Algebra</a></li>
  <li><a href="#mathematics-of-the-ordinary-least-squares-estimator" id="toc-mathematics-of-the-ordinary-least-squares-estimator" class="nav-link" data-scroll-target="#mathematics-of-the-ordinary-least-squares-estimator">1.6.4: Mathematics of the Ordinary Least Squares Estimator</a></li>
  <li><a href="#regression-anatomy-and-controlling-for-confounders" id="toc-regression-anatomy-and-controlling-for-confounders" class="nav-link" data-scroll-target="#regression-anatomy-and-controlling-for-confounders">1.6.5: Regression Anatomy and Controlling for Confounders</a></li>
  <li><a href="#regression-anatomy-formula-for-ols-estimation" id="toc-regression-anatomy-formula-for-ols-estimation" class="nav-link" data-scroll-target="#regression-anatomy-formula-for-ols-estimation">1.6.6: Regression Anatomy Formula for OLS Estimation</a></li>
  <li><a href="#r-squared-and-goodness-of-fit" id="toc-r-squared-and-goodness-of-fit" class="nav-link" data-scroll-target="#r-squared-and-goodness-of-fit">1.6.7: R-Squared and Goodness of Fit</a></li>
  <li><a href="#implementation-in-r" id="toc-implementation-in-r" class="nav-link" data-scroll-target="#implementation-in-r">Implementation in R</a>
  <ul class="collapse">
  <li><a href="#regression-estimation" id="toc-regression-estimation" class="nav-link" data-scroll-target="#regression-estimation">Regression Estimation</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>This lesson covers the following topics:</p>
<ul>
<li>The motivation/goal of Multiple Linear Regression, and the basic layout of Multiple Linear Regression.</li>
<li>The estimation process of the OLS estimator for Multiple Linear Regression.</li>
<li>What it means to control for a variable (using the regression anatomy theory).</li>
<li>How we can use the regression anatomy theory to create an analogous estimate for OLS.</li>
</ul>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
<section id="motivation-behind-multiple-linear-regression" class="level1">
<h1>1.6.1: Motivation Behind Multiple Linear Regression</h1>
<p>In <a href="https://statsnotes.github.io/theory/5.html#unbiasedness-of-ols-under-the-gauss-markov-theorem">1.5.2</a>, we discussed how the simple linear regression with the OLS estimator is only unbiased under 4 conditions.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Gauss-Markov Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Gauss-Markov Theorem states that under 4 conditions, the OLS estimator is unbiased for <span class="math inline">\beta_1</span>.</p>
<ul>
<li><strong>SLR.1 (Linearity in Parameters)</strong>: The parameters of the model are linear.</li>
<li><strong>SLR.2 (Random Sampling)</strong>: All observations in our sample are randomly sampled from the population.</li>
<li><strong>SLR.3 (Sample Variation in</strong> <span class="math inline">x</span><strong>)</strong>: The sample values of <span class="math inline">x</span> must have a non-zero variance.</li>
<li><strong>SLR.4 (Zero Conditional Mean)</strong>. The error term <span class="math inline">u</span> has an expectation of 0, given any value of <span class="math inline">x</span>.</li>
</ul>
<p>We will explore each condition in detail.</p>
</div>
</div>
<p>The key assumption that is most often violated is the final assumption: Zero Conditional Mean. For this condition to be met, we know that <span class="math inline">E(u|x) = 0</span> for all <span class="math inline">x</span>. This also implies that:</p>
<p><span class="math display">
Cov(x, u) = 0
</span></p>
<p>Why is this condition frequently violated? Recall from <a href="https://statsnotes.github.io/theory/4.html#the-simple-linear-regression-model">1.4.1</a>, where we discussed that the error term <span class="math inline">u_i</span> represents the effect of anything else on <span class="math inline">y</span> other than our explanatory variable <span class="math inline">x</span>.</p>
<ul>
<li>For example, if <span class="math inline">x</span> is age, and <span class="math inline">y</span> is income, there are other factors that also affect income <span class="math inline">y</span> besides age <span class="math inline">x</span>. Those other factors are included in the error term <span class="math inline">u_i</span>.</li>
</ul>
<p><br></p>
<p>However, if our explanatory variable <span class="math inline">x</span> is correlated with any of these other factors in <span class="math inline">u_i</span>, that also means that <span class="math inline">Cov(x,u)≠0</span>.</p>
<ul>
<li>For example, if <span class="math inline">x</span> is age, and <span class="math inline">y</span> is income, another factor that could affect income <span class="math inline">y</span> is a third variable, years of work experience (let us label that <span class="math inline">w</span>).</li>
<li>This extra variable <span class="math inline">w</span> that affects outcome <span class="math inline">y</span> is included in the error term <span class="math inline">u</span> of our regression <span class="math inline">y = \beta_0 + \beta_1 + u</span>.</li>
<li>However, years of work experience <span class="math inline">w</span> is also correlated with age <span class="math inline">x</span>, as obviously, older people have more work experience.</li>
<li>Since <span class="math inline">x</span> is correlated with <span class="math inline">w</span>, and <span class="math inline">w</span> is in the error term <span class="math inline">u</span>, that means that <span class="math inline">x</span> is correlated with <span class="math inline">u</span>, which violates the Zero Conditional Mean assumption.</li>
</ul>
<p>These variables <span class="math inline">w</span> that are correlated with both <span class="math inline">x</span> and <span class="math inline">y</span> are called <strong>confounding variables</strong>.</p>
<ul>
<li>Note: there can be (and almost always are) more than one confounding variable.</li>
<li>Note: This concept will be further discussed in lesson 1.7 with omitted variable bias and Endogeniety.</li>
</ul>
<p><br></p>
<p>How do we address this violation of the Zero Conditional Mean resulting from the extra variable <span class="math inline">w</span>?</p>
<ul>
<li>Well, there are actually several ways to do this, which we briefly mentioned at the end of <a href="https://statsnotes.github.io/theory/5.html#proof-of-unbiasedness-of-ols-under-gauss-markov">1.5.3</a>.</li>
</ul>
<p>The simplest way to address it is to include the extra variable <span class="math inline">w</span> into our regression as another explanatory variable.</p>
<ul>
<li>This way, <span class="math inline">w</span> is another explanatory variable along with <span class="math inline">x</span>, and is no longer in the error term <span class="math inline">u</span>, thus <span class="math inline">x</span> will not be correlated with <span class="math inline">u</span> anymore (assuming there are no other confounders).</li>
</ul>
<p>Multiple Linear Regression is a way to implement this solution - it allows us to include multiple explanatory variables in our regression model.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="the-multiple-linear-regression-model" class="level1">
<h1>1.6.2: The Multiple Linear Regression Model</h1>
<p>Multiple linear regression is an extension of simple linear regression, that can help us deal with confounding variables.</p>
<ul>
<li>Multiple linear regression allows us to “control for the effect” of confounders. We will discuss this in <a href="https://statsnotes.github.io/theory/6.html#regression-anatomy-and-controlling-for-confounders">1.6.5</a>.</li>
</ul>
<p>The <strong>response variable</strong> (outcome variable) is notated <span class="math inline">y</span>, just like in single linear regression.</p>
<p>The <strong>explanatory variable</strong>s are <span class="math inline">x_1, x_2, ..., x_k</span>. We sometimes also denote all explanatory variables as the vector <span class="math inline">\overrightarrow{x}</span>.</p>
<ul>
<li><span class="math inline">k</span> represents the total number of explanatory variables.</li>
<li>Note: if you see the notation <span class="math inline">x_j</span>, that means any explanatory variable <span class="math inline">x_1, \dots , x_k</span>. The variable <span class="math inline">x_j</span> represents any individual coefficient (for generalisation purposes).</li>
</ul>
<p><br></p>
<p>The multiple linear regression takes the following form:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>Take a set of observed data with <span class="math inline">n</span> number of pairs of <span class="math inline">(\overrightarrow{x}_i, y_i)</span> observations. The linear model takes the following form:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + \dots + \beta_k x_{ki} + u_i
</span></p>
<ul>
<li>Where the coefficients (that need to be estimated) are vector<span class="math inline">\overrightarrow{\beta} = \beta_0, \beta_1, ..., \beta_k</span>. That means we have <span class="math inline">k</span> number of variables and <span class="math inline">k+1</span> number of coefficients (with the one not attached to a variable being the intercept).</li>
<li>Where <span class="math inline">u_i</span> is the error term function - that determines the error for each unit <span class="math inline">i</span>. Error <span class="math inline">u_i</span> has a variance of <span class="math inline">\sigma^2</span>, and expectation <span class="math inline">E(u_i) = 0</span>.</li>
</ul>
<p>In <a href="https://statsnotes.github.io/theory/4.html#ols-regression-as-a-conditional-expectation-function">1.4.7</a>, we also established that the linear regression model is equivalent to the conditional expectation function. This is also true with multiple linear regression. Thus, we can also write multiple linear regression as:</p>
<p><span class="math display">
E(y_i|x_i) = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki}
</span></p>
</div>
</div>
<p><br></p>
<p>Same as in Simple Linear Regression, once we have estimated <span class="math inline">\overrightarrow\beta</span>, we will have a best-fit <strong>plane</strong>, also called a fitted-values model (see <a href="https://statsnotes.github.io/theory/4.html#fitted-values-and-best-fit-lines">1.4.2</a>). The fitted values model takes the form:</p>
<p><span class="math display">
\hat y_i = \hat\beta_0 + \hat\beta_1 x_{1i} + \dots + \hat\beta_k x_{ki}
</span></p>
<ul>
<li>Where <span class="math inline">\hat y</span> are the predicted values of <span class="math inline">y</span> based on our best-fit plane.</li>
<li>Where <span class="math inline">\hat\beta_0, \dots, \hat\beta_k</span> are our estimates for coefficients <span class="math inline">\beta_0, \dots, \beta_k</span>.</li>
<li>Just like in simple linear regression, the error term <span class="math inline">u_i</span> dispersal because <span class="math inline">E(u_i) = 0</span>.</li>
</ul>
<p>Note how I have been saying best-fit <strong>plane</strong>, not best-fit line. This is because with multiple explanatory variables, we are now no longer in a 2-dimensional space, but a <span class="math inline">k</span>-dimensional space (based on the number of variables).</p>
<ul>
<li>Essentially, each variable has its own “axis”/“dimension”.</li>
</ul>
<p>Thus, our best-fit line now is a best-fit plane. For example, take this model with 2 explanatory variables <span class="math inline">x_1</span> (years of education), <span class="math inline">x_2</span> (seniority), and <span class="math inline">y</span> (income):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-365376575.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="multiple-linear-regression-with-linear-algebra" class="level1">
<h1>1.6.3: Multiple Linear Regression with Linear Algebra</h1>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section contains Linear Algebra. It is not essential that you understand this section, however, it can be helpful to develop intuition.</p>
</div>
</div>
<p>In the previous section, we introduced the multiple linear regression model as the following:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + \dots + \beta_k x_{ki} + u_i
</span></p>
<p>However, it can be very useful to also see the multiple linear regression model in terms of linear algebra (vectors and matrices).</p>
<ul>
<li>The main reason for this is because this will make the estimation process far easier.</li>
</ul>
<p><br></p>
<p>The <span class="math inline">i</span>’th observation can be re-written in vector form as following:</p>
<p><span class="math display">
y_i = x_i'\beta + u_i, \text{ where }\beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{bmatrix} \text{ and }x_i = \begin{bmatrix}1 \\x_{1i} \\\vdots \\x_{ki}\end{bmatrix}
</span></p>
<ul>
<li>The <span class="math inline">x_i'</span> in the equation is the transpose of <span class="math inline">x_i</span>, to make matrix multiplication possible.</li>
<li>The first element of the <span class="math inline">x_i</span> matrix is 1, since <span class="math inline">1 \times \beta_0</span> gives us the first parameter (intercept) in the linear model.</li>
<li>Thus, when multiplying out, we get the same equation as the original multiple linear regression.</li>
</ul>
<p><br></p>
<p>Note how we have the subscript <span class="math inline">i</span> representing each individual observation. With a vector, we can expand out these subscripts.</p>
<ul>
<li>For example, instead of <span class="math inline">y_i</span>, we could have a vector with <span class="math inline">y_1, y_2, \dots, y_n</span> (assuming we have <span class="math inline">n</span> observations).</li>
<li>Same for <span class="math inline">x'_i</span>, which can be expanded into a vector of <span class="math inline">x_1', x_2', \dots x_n'</span>, and for the error term <span class="math inline">u_i</span>, which can be expanded into a vector of <span class="math inline">u_1, u_2, \dots, u_n</span>.</li>
</ul>
<p>Using this logic, we can obtain the following, with the <span class="math inline">x_i'</span> and <span class="math inline">\beta</span> being vectors within a vector:</p>
<p><span class="math display">
\begin{split}
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} &amp; = \begin{pmatrix}x_1'\beta + u_1 \\ x_2'\beta + u_2 \\ \vdots \\ x_n'\beta + u_n\end{pmatrix} \\
&amp; \\
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix}&amp; = \begin{pmatrix}x_1'\beta \\ x_2'\beta \\ \vdots \\ x_n'\beta\end{pmatrix} + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
\end{split}
</span></p>
<p>Since <span class="math inline">\beta</span> vector appears as a common factor for all observations <span class="math inline">i=1,...,n</span>, we can factor it out and have an equation:</p>
<p><span class="math display">
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} = \begin{pmatrix}x_1' \\ x_2' \\ \vdots \\ x_n'\end{pmatrix} \space \beta + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
</span></p>
<p><br></p>
<p>We can expand the <span class="math inline">x_1',...,x_n'</span> vector into a matrix. Remember that each <span class="math inline">x_1',...,x_n'</span> is already a vector of different explanatory variables. So, we get the following result:</p>
<p><span class="math display">
y = X \beta + u, \text{ where } X = \begin{bmatrix}1 &amp; x_{21} &amp; \dots &amp; x_{k1} \\1 &amp; x_{22} &amp; \dots &amp; x_{k2} \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\1 &amp; x_{2n} &amp; \dots &amp; x_{kn}\end{bmatrix}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression with Linear Algebra
</div>
</div>
<div class="callout-body-container callout-body">
<p>The multiple linear regression can be expressed in linear algebra as:</p>
<p><span class="math display">
y = X \beta + u
</span></p>
<p>Where vector <span class="math inline">y</span> is equal to:</p>
<p><span class="math display">
y = \begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix}
</span></p>
<p>Where matrix <span class="math inline">X</span> is equal to:</p>
<p><span class="math display">
X = \begin{bmatrix}1 &amp; x_{21} &amp; \dots &amp; x_{k1} \\1 &amp; x_{22} &amp; \dots &amp; x_{k2} \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\1 &amp; x_{2n} &amp; \dots &amp; x_{kn}\end{bmatrix}
</span></p>
<ul>
<li>Where the notation for elements of <span class="math inline">X</span> is <span class="math inline">x_{ki}</span>, with <span class="math inline">i</span> being the unit of observation <span class="math inline">i = 1, \dots n</span>, and <span class="math inline">k</span> being the explanatory variables index.</li>
<li>The first column of <span class="math inline">X</span> is a vector of 1, which exists because these 1’s are multiplied with <span class="math inline">\beta_0</span> in our model (see below).</li>
</ul>
<p>Where vector <span class="math inline">\beta</span> is equal to:</p>
<p><span class="math display">
\beta = \begin{pmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{pmatrix}
</span></p>
</div>
</div>
<p>The point of expressing the model in linear algebra is that it makes the estimation process far easier, as we will see in the next section.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="mathematics-of-the-ordinary-least-squares-estimator" class="level1">
<h1>1.6.4: Mathematics of the Ordinary Least Squares Estimator</h1>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section contains Linear Algebra. It is not essential that you understand this section, however, it can be helpful to develop intuition.</p>
</div>
</div>
<p>As we remember from <a href="https://statsnotes.github.io/theory/4.html#mathematics-of-the-ordinary-least-squares-estimator">1.4.4</a>, the goal of Ordinary Least Squares Estimation is to minimise the sum of squared errors. The sum of squared errors in multiple regression is:</p>
<p><span class="math display">
\begin{split}
SSE &amp; = \sum\limits_{i=1}^n (y_i - \hat y_i)^2\\
&amp; = \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} -  ... - \hat\beta_kx_{ki})^2
\end{split}
</span></p>
<p>Similar to our simple linear regression (but with additional variables), our minimisation condition is:</p>
<p><span class="math display">
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...) &amp; = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} ...)^2 \\
&amp; = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} S(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)
\end{split}
</span></p>
<p><br></p>
<p>Taking the partial derivatives of each parameter like in simple linear regression, we get first order conditions:</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\&amp; \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
&amp; \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>This system of equations includes <span class="math inline">k+1</span> variables and <span class="math inline">k+1</span> equations, which is way too difficult to solve.</p>
<p><br></p>
<p>Instead, we can use linear algebra. Let us define our estimation vector <span class="math inline">\hat{\beta}</span> as the value of <span class="math inline">\hat\beta</span> that minimises the sum of squared errors:</p>
<p><span class="math display">
\hat{\beta} = \arg \min\limits_{b} (y - Xb)' (y - Xb) = \arg \min\limits_b S(b)
</span></p>
<p>We can expand <span class="math inline">S(b)</span> as follows:</p>
<p><span class="math display">
\begin{split}
S(b) &amp; = y'y - b'X'y - y'Xb + b'X'Xb \\
&amp; = y'y - 2b'X'y + b'X'Xb
\end{split}
</span></p>
<p>Taking the partial derivative in respect to <span class="math inline">b</span>:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = \begin{pmatrix}\frac{\partial S(b)}{\partial b_1} \\\vdots \\\frac{\partial S(b)}{\partial b_k}\end{pmatrix}
</span></p>
<p>Differentiating with the vector <span class="math inline">b</span> yields:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = -2X'y + 2X'Xb
</span></p>
<p>Evaluted at <span class="math inline">\hat{\beta}</span>, the derivatives should equal zero (since first order condition of finding minimums):</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} \biggr|_{\hat{\beta}} = -2X'y + 2X'X \hat{\beta} = 0
</span></p>
<p>When assuming <span class="math inline">X'X</span> is invertable, we can isolate <span class="math inline">\hat{\beta}</span> to find the solution to OLS:</p>
<p><span class="math display">
\begin{split}
-2X'y + 2X'X \hat{\beta} &amp; = 0 \\
2X'X\hat\beta &amp; = 2X'y \\
\hat\beta &amp; = (2X'X)^{-1} 2 X'y \\
\hat\beta &amp; = (X'X)^{-1}X'y
\end{split}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: OLS Estimate of <span class="math inline">\hat\beta</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Ordinary Least Squares Estimate of vector <span class="math inline">\hat\beta</span> for multiple linear regression is:</p>
<p><span class="math display">
\hat{\beta} = (X'X)^{-1} X'y
</span></p>
</div>
</div>
<p>Once we have estimates of <span class="math inline">\hat{\beta}</span>, we can plug them into our linear model to obtain fitted values:</p>
<p><span class="math display">
\begin{split}
\hat{y} &amp; = X\hat{\beta} \\
&amp; = X(X'X)^{-1} X'y
\end{split}
</span></p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="regression-anatomy-and-controlling-for-confounders" class="level1">
<h1>1.6.5: Regression Anatomy and Controlling for Confounders</h1>
<p>We talked about how multiple linear regression allows us to control for confounders. But what does that mean? How does it affect our interpretations of coefficients?</p>
<p>The <strong>Regression Anatomy</strong> Theory, also called the <strong>Frisch–Waugh–Lovell (FWL)</strong> theorem, illustrates this concept. Take our standard multiple linear regression:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i
</span></p>
<p><br></p>
<p>Let us say we are interested in <span class="math inline">x_1</span> (this can be generalised to any explanatory variable <span class="math inline">x_j</span>). Let us make <span class="math inline">x_1</span> the outcome variable of a regression with explanatory variables <span class="math inline">x_2, ..., x_k</span>:</p>
<p><span class="math display">
x_{1i} = \gamma_0 + \gamma_1 x_{2i} + ... + \gamma_{k-1}x_{ki} + \widetilde{r_{1i}}
</span></p>
<ul>
<li>Where <span class="math inline">\gamma_0, ..., \gamma_{k-1}</span> are coefficients.</li>
<li>Where <span class="math inline">\widetilde{r_{1i}}</span> is the error term.</li>
</ul>
<p>The error term is <span class="math inline">\widetilde{r_{1i}}</span>, which represents the part of <span class="math inline">x_{1i}</span> that are uncorrelated to <span class="math inline">x_2, ..., x_k</span>.</p>
<ul>
<li>In other words, <span class="math inline">\widetilde{r_{1i}}</span> is the part of <span class="math inline">x_1</span> that cannot be explained by any other explanatory variable <span class="math inline">x_2, ..., x_k</span>. (uncorrelated with them)</li>
</ul>
<p><br></p>
<p>Now, take the regression of with outcome variable <span class="math inline">y</span>, with all explanatory variables <u>except</u> <span class="math inline">x_1</span>:</p>
<p><span class="math display">
y_i = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i}
</span></p>
<ul>
<li>Where <span class="math inline">\delta_0, ..., \delta_{k-1}</span> are coefficients.</li>
<li>Where <span class="math inline">\widetilde {y_i}</span> is the error term.</li>
</ul>
<p>The error term is <span class="math inline">\widetilde{y_i}</span>, which is the part of <span class="math inline">y_i</span> that cannot be explained by <span class="math inline">x_2, ..., x_k</span> (uncorrelated with them).</p>
<p><br></p>
<p>Since <span class="math inline">\widetilde{y_i}</span> is not explained by <span class="math inline">x_2, ..., x_k</span>, variable <span class="math inline">x_1</span> must be the one explaining <span class="math inline">\widetilde{y_i}</span>.</p>
<ul>
<li>But, it is not the whole of <span class="math inline">x_1</span> explaining <span class="math inline">\tilde{y_i}</span>.</li>
<li>This is since <span class="math inline">x_1</span> may also correlated with <span class="math inline">x_2, ..., x_k</span>, and the correlated parts of <span class="math inline">x_1</span> with <span class="math inline">x_2, ..., x_k</span> are already picked up in the regression by the coefficients of <span class="math inline">x_2, ..., x_k</span>.</li>
</ul>
<p>Thus, <span class="math inline">\widetilde{y_i}</span> must be explained by the part of <span class="math inline">x_1</span> that is uncorrelated and not explained by <span class="math inline">x_2, ..., x_k</span>, which we derived earlier as <span class="math inline">\widetilde{r_{1i}}</span>.</p>
<p><br></p>
<p>Thus, we can create another regression with explanatory variable <span class="math inline">\widetilde{x_{1i}}</span> and outcome variable <span class="math inline">\widetilde{y_i}</span>.</p>
<p><span class="math display">
\widetilde{y_i} = \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i
</span></p>
<p>We can plug <span class="math inline">\widetilde{y_i}</span> back into our regression of <span class="math inline">y_i</span> with explanatory variables <span class="math inline">x_2 ..., x_k</span>, and re-arrange:</p>
<p><span class="math display">
\begin{split}
y_i &amp; = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i} \\
y_i &amp; = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i \\
y_i  &amp; = (\delta_0 + \alpha_0) + \alpha_1 \widetilde{r_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i
\end{split}
</span></p>
<p>As we can see, this new regression mirrors the original standard multiple linear regression:</p>
<p><span class="math display">
\begin{split}
y_i  &amp; = (\delta_0 + \alpha_0) + \alpha_1 \widetilde{r_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i \\
y_i &amp; = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + u_i
\end{split}
</span></p>
<ul>
<li>The <span class="math inline">\beta_0</span> in the original is analogous to the <span class="math inline">(\delta_0 + \alpha 0)</span>.</li>
<li>The <span class="math inline">\beta_1 x_{1i}</span> in the original is analogous to <span class="math inline">\alpha_1 \widetilde{r_{1i}}</span>.</li>
<li>The <span class="math inline">\beta_2 x_{2i} + \dots + \beta_k x_{ki}</span> is analogous to <span class="math inline">\delta_1 x_{2i} + ... + \delta_{k-1} x_{ki}</span>.</li>
<li>The <span class="math inline">u_i</span> is in both regressions.</li>
</ul>
<p><br></p>
<p>Importantly we know the <span class="math inline">\beta_1 x_{1i}</span> in the original is analogous to <span class="math inline">\alpha_1 \widetilde{r_{1i}}</span>. Thus, <u>the estimate of <span class="math inline">\alpha_1</span> will be the same as <span class="math inline">\beta_1</span> in the original regression</u>.</p>
<ul>
<li>The coefficient <span class="math inline">\alpha_1</span> (which is equal to <span class="math inline">\beta_1</span>) explains the expected change in <span class="math inline">y</span>, given an increase in the part of <span class="math inline">x_1</span> uncorrelated with <span class="math inline">x_2, ..., x_k</span>.</li>
<li>So essentially, <u>we have <strong>partialed out</strong> the effect of the other explanatory variables, and only focus on the effect on <span class="math inline">y</span> of the uncorrelated part of <span class="math inline">x_1</span> (which is <span class="math inline">\widetilde{r_{1i}}</span>)</u></li>
</ul>
<p>Or in other words, OLS in multiple linear regression estimates the effect of <span class="math inline">\widetilde{r_{1i}}</span> on <span class="math inline">y</span>.</p>
<ul>
<li>We can apply this to any explanatory variable <span class="math inline">x_1, \dots, x_k</span>. The uncorrelated parts of any explanatory variable <span class="math inline">x_j</span> are labelled <span class="math inline">\widetilde{r_{ji}}</span>.</li>
</ul>
<p><br></p>
<p>Using all this info, we can interpret the meaning of any coefficient <span class="math inline">\hat\beta_j</span> on the relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Coefficients in OLS
</div>
</div>
<div class="callout-body-container callout-body">
<p>The interpretation of <span class="math inline">\hat\beta_j</span> coefficient, multiplied to variable <span class="math inline">x_j</span> is:</p>
<ul>
<li>When <span class="math inline">x_j</span> increases by one unit, there is an expected <span class="math inline">\hat\beta_j</span> unit change in <span class="math inline">y</span>, holding all other explanatory variables constant.</li>
</ul>
<p>The interpretation of intercept <span class="math inline">\hat\beta_0</span> is still the same: it is the expected value of <span class="math inline">y</span> when all explanatory variables equal 0.</p>
</div>
</div>
<p>We can also standardise our interpretations as shown in <a href="https://statsnotes.github.io/theory/4.html#interpretation-and-standardisation">1.4.5</a>.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="regression-anatomy-formula-for-ols-estimation" class="level1">
<h1>1.6.6: Regression Anatomy Formula for OLS Estimation</h1>
<p>In the last section, we discussed the idea of regression anatomy, and how including confounding variables changes the estimates.</p>
<ul>
<li>That instead of finding the effect of <span class="math inline">x_j</span> entirely on <span class="math inline">y</span>, we partial out the effect of other explanatory variables, and only find the effect of the uncorrelated part of <span class="math inline">x_j</span> (labelled <span class="math inline">\widetilde {r_{ji}}</span>) on <span class="math inline">y</span>.</li>
</ul>
<p>We have already discussed the mathematical solution of OLS in relation to linear algebra (see <a href="https://statsnotes.github.io/theory/6.html#mathematics-of-the-ordinary-least-squares-estimator">1.6.4</a>). However, we can also express the estimation solution of <span class="math inline">\hat\beta_j</span> in relation to the regression anatomy formula.</p>
<p><br></p>
<p>Let us start off with the OLS estimator for simple linear regression, which calculates the <span class="math inline">\hat\beta_1</span>, the relationship between <span class="math inline">x</span> and <span class="math inline">y</span> (which we derived in <a href="https://statsnotes.github.io/theory/4.html#mathematics-of-the-ordinary-least-squares-estimator">1.4.4</a>):</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (x_i - \bar{x}) = 0 \\&amp; \sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) \\&amp; \sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2\end{split}
</span></p>
</div>
</div>
<p>Let us look at the numerator. Let us expand the numerator:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
= &amp; \sum\limits_{i=1}^n [(x_i - \bar x)y_i - (x_i - \bar x) \bar y] \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \sum\limits_{i=1}^n (x_i - \bar x) \bar y \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
\end{split}
</span></p>
<p>We know that <span class="math inline">\sum (x_i - \bar x) = 0</span>. Thus, we can further simplify to:</p>
<p><span class="math display">
\begin{split}
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y(0) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i
\end{split}
</span></p>
<p>Thus, putting the numerator back in, we now we have the equation:</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})y_i}{\sum_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<p><br></p>
<p>We know that in multiple linear regression, <span class="math inline">\hat\beta_j</span> is not the full relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>. Instead, it is the relationship of the part of <span class="math inline">x_j</span> that is uncorrelated with all other explanatory variables, and <span class="math inline">y</span>.</p>
<ul>
<li>So in other words, it is the relationship of <span class="math inline">\widetilde{r_{ji}}</span> on <span class="math inline">y</span>.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reminder: Regression Anatomy
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember, in the last section, we defined <span class="math inline">\widetilde{r_{ji}}</span> as the error term in a regression of <span class="math inline">x_j</span> on all other explanatory variables:</p>
<p><span class="math display">
x_{ji} = \gamma_0 + \gamma_1 x_{1i} + ... + \gamma_{k-1}x_{ki} + \widetilde{r_{ji}}
</span></p>
<p>Thus, <span class="math inline">\widetilde{r_{ji}}</span> is the part of <span class="math inline">x_j</span> uncorrelated with any other explanatory variable.</p>
</div>
</div>
<p>So, since multiple linear regression is the relationship of <span class="math inline">\widetilde{r_{ji}}</span> on <span class="math inline">y</span>, instead of <span class="math inline">x</span> on <span class="math inline">y</span>, let us replace the <span class="math inline">x</span>’s in our formula with <span class="math inline">\widetilde{r_{ji}}</span>:</p>
<p><span class="math display">
\hat{\beta}_j = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))^2}
</span></p>
<p>We can actually simplify this more with a property of simple linear regression - remember, that the error term of a regression <span class="math inline">u</span>, should be such that <span class="math inline">E(u)=0</span>.</p>
<p>We know that <span class="math inline">\widetilde{r_{ji}}</span> is also the error term of a regression, so, <span class="math inline">E(\widetilde{r_{ji}}) = 0</span> as well. Plugging that into our equation, we can get the regression anatomy formula for OLS.</p>
<p><span class="math display">
\begin{split}
\hat{\beta}_j &amp; = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))^2} \\
\\
&amp; = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - 0)y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - 0)^2} \\
\\
&amp; = \frac{\sum_{i=1}^n \widetilde{r_{ji}} \ y_i}{\sum_{i=1}^n \widetilde {r_{ji}}^2}
\end{split}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Regression Anatomy Formula
</div>
</div>
<div class="callout-body-container callout-body">
<p>The OLS estimate of coefficient <span class="math inline">\beta_j</span> can be written in terms of regression anatomy as follows:</p>
<p><span class="math display">
\hat{\beta}_j = \frac{\sum_{i=1}^n \widetilde{r_{ji}} \ y_i}{\sum_{i=1}^n \widetilde {r_{ji}}^2}
</span></p>
</div>
</div>
<p>This formula will be very useful in analysing the properties of the OLS estimator (in lesson 7).</p>
<p><br></p>
<p>The key implication of this formula is that when we add more explanatory variables, our existing coefficient estimates might change.</p>
<ul>
<li>This is because if the new explanatory variable is at all correlated with our explanatory variable of interest, that correlation will be partialed out of our estimate.</li>
</ul>
<p>This explains intuitively why the Zero-Conditional mean assumption (discussed in <a href="https://statsnotes.github.io/theory/5.html#unbiasedness-of-ols-under-the-gauss-markov-theorem">1.5.2</a>), when violated, results in biased OLS estimates.</p>
<ul>
<li>Zero-Conditional Mean <span class="math inline">E(u|x)</span>, which also means <span class="math inline">Cov(x,u) = 0</span>, is often violated when there is some confounding variable <span class="math inline">w</span> that is correlated with both <span class="math inline">x</span> and <span class="math inline">y</span>, which causes <span class="math inline">x</span> to be correlated with <span class="math inline">u</span>.</li>
<li>When we actually include <span class="math inline">w</span> in the regression, the part of <span class="math inline">x</span> and <span class="math inline">w</span> being correlated together will be partialled out of the <span class="math inline">\hat\beta_1</span> of variable <span class="math inline">x</span>’s effect on <span class="math inline">y</span>.</li>
<li>Thus, our estimate will change when we include <span class="math inline">w</span>, getting closer to the true <span class="math inline">\beta_1</span>.</li>
<li>This explains why when Zero-Conditional Mean is violated, that means our estimates are biased.</li>
</ul>
<p>Of course, this also implies that if there are more than one confounder <span class="math inline">w</span>, to get an accurate OLS estimate, we will need to include every single possible confounder.</p>
<ul>
<li><p>We will discuss this more when we discuss the Gauss-Markov conditions of Multiple Linear Regression in Lesson 7.</p></li>
<li><p>We will also discuss in Lesson 7, how much (quantitatively) our estimates will be biased when we fail to include a confounder <span class="math inline">w</span>.</p></li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="r-squared-and-goodness-of-fit" class="level1">
<h1>1.6.7: R-Squared and Goodness of Fit</h1>
<p>In <a href="https://statsnotes.github.io/theory/4.html#r-squared-and-goodness-of-fit">1.4.6</a>, we discussed R-squared as a measure of the goodness of fit of a model.</p>
<p>In multiple linear regression, R-squared is the exact same.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: R-Squared
</div>
</div>
<div class="callout-body-container callout-body">
<p>The R-squared metric is a metric describing how good of a fit our model is.</p>
<p>R-Squared is the proportion of variation in <span class="math inline">y</span>, explained by our model with all our explanatory variables.</p>
<p>Mathematically:</p>
<p><span class="math display">
R^2 = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}
</span></p>
<p>Where:</p>
<p><span class="math display">
\begin{split}
&amp; SST = \sum\limits_{i=1}^n (y_i - \bar y)^2 \\
&amp; SSE = \sum\limits_{i=1}^n (\hat y_i - \bar y )^2 \\
&amp; SSR = \sum\limits_{i=1}^n (\hat u_i^2)
\end{split}
</span></p>
</div>
</div>
<p><br></p>
<p>There is an additional property with <span class="math inline">R^2</span> in multiple linear regression: <u><span class="math inline">R^2</span> never falls when another explanatory is added to the regression.</u></p>
<ul>
<li>So essentially, any explanatory variable added to the regression will always boost R-squared.</li>
</ul>
<p>This shows the downside of R-squared as a metric - if we just include random variables, by chance, these variables will explain the variation in <span class="math inline">y</span>.</p>
<ul>
<li>Thus, focusing on R-squared can result in models with silly or nonsensical explanatory variables.</li>
</ul>
<p>We will discuss the importance of careful model selection in Part II of the course, where we discuss causal inference.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="implementation-in-r" class="level1">
<h1>Implementation in R</h1>
<section id="regression-estimation" class="level2">
<h2 class="anchored" data-anchor-id="regression-estimation">Regression Estimation</h2>
<p>To estimate a regression, we can use the <em>feols()</em> function from the package <em>fixest</em>, or we can use the base-R function <em>lm()</em>.</p>
<ul>
<li>The syntax is the same for both (at least for now).</li>
<li>The <em>feols()</em> function does have a few advantages for techniques that will be discussed later, especially when it comes to causal inference and econometrics.</li>
</ul>
<p>For the <em>feols()</em> function, we will need the <em>fixest</em> package. Make sure to install it if you have not previously (google how to install R-packages if needed).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fixest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
<p><strong>Syntax:</strong></p>
<p>For the <em>feols()</em> function, the syntax is as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">feols</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3, <span class="at">data =</span> mydata)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Replace <em>model</em> with any name you want to store your regression model in.</li>
<li>Replace <em>y</em> with your outcome variable name, and <em>x1, x2, x3</em> with your explanatory variable name.</li>
<li>You can add more explanatory variables by adding + signs and <em>x4 + x5 …</em> and so on. You can also remove explanatory variables down to only 1.</li>
<li>Replace <em>mydata</em> with the name of your dataframe.</li>
</ul>
<p>The <em>lm()</em> function has the exact same syntax for simple linear regression, except that we replace <em>feols()</em> with <em>lm()</em>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3, <span class="at">data =</span> mydata)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
<p><strong>Example:</strong></p>
<p>Let us run a regression with outcome variable <em>immatt</em> (attitude towards immigrants), explanatory variables <em>age</em> and <em>educ</em> (years of education), from the dataframe called <em>dta</em>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>my_model <span class="ot">&lt;-</span> <span class="fu">feols</span>(immatt <span class="sc">~</span> age <span class="sc">+</span> educ, <span class="at">data =</span> dta)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(my_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OLS estimation, Dep. Var.: immatt
Observations: 33,706
Standard-errors: IID 
             Estimate Std. Error  t value  Pr(&gt;|t|)    
(Intercept) -0.529355   0.026638 -19.8720 &lt; 2.2e-16 ***
age         -0.005175   0.000291 -17.8117 &lt; 2.2e-16 ***
educ         0.063833   0.001381  46.2108 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
RMSE: 0.932834   Adj. R2: 0.08225</code></pre>
</div>
</div>
<p>We can see in the estimate column, we get our intercept estimate <span class="math inline">\hat\beta_0</span>, and our explanatory variables coefficient estimates <span class="math inline">\hat\beta_1</span> and <span class="math inline">\hat\beta_2</span>.</p>
<p>The result is similar with <em>lm()</em>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>my_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(immatt <span class="sc">~</span> age <span class="sc">+</span> educ, <span class="at">data =</span> dta)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(my_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = immatt ~ age + educ, data = dta)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.97099 -0.58912  0.06513  0.63948  2.77153 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.5293545  0.0266382  -19.87   &lt;2e-16 ***
age         -0.0051753  0.0002906  -17.81   &lt;2e-16 ***
educ         0.0638327  0.0013813   46.21   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9329 on 33703 degrees of freedom
Multiple R-squared:  0.0823,    Adjusted R-squared:  0.08225 
F-statistic:  1511 on 2 and 33703 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>