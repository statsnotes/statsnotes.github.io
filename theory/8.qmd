---
title: "Statistics for Social Scientists"
subtitle: "Lesson 1.8: Categorical Explanatory Variables"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Lesson 1.8: Categorical Variables in Regression"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

::: callout-note
## Lesson Overview

In the last few lessons, we have discussed the linear regression model, focusing on continuous $x$ variables. In this lesson, we explore how binary and categorical variables, which are very frequent in the social sciences, can be used in regression with OLS.

This lesson covers the following topics:

-   What categorical variables are (including binary and ordinal variables).
-   How binary and categorical explanatory variables work in regression.
-   Extensions of categorical variables used in regression.
:::

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.8.1: What are Categorical Variables

The three main types of variables in statistics are **categorical**, **ordinal**, and **continuous** variables. What are these different types of variables, and how do they differ?

<br />

**Categorical variables** are variables that have discrete categories, that cannot be ordered.

1.  Discrete categories means that there are different "categories" of outcomes, and you can only get values in those categories. For example, if you roll a dice, you can only get one of 6 outcomes. You cannot get outcome 3.4234783 - that is not a category.
2.  Categories cannot be ordered. For example, take the categories Microsoft, Apple, and Google. There is no "order" to these. For example, we know $5 > 1$. But we cannot say Microsoft \> apple, or Google \> apple.

Categorical variables are described by discrete random variables, as discussed in [1.1.3](https://statsnotes.github.io/theory/1.html#probability-mass-and-density-functions), and are described by probability mass functions.

A special type of categorical variable is the **binary/dummy variable**, which is a variable that has only two categories.

-   These are extremely common in social sciences. For example, variables involving treatment/control groups, true/false, did/didn't, yes/no, are all examples of binary/dummy variables.

[Categorical variables require special treatment in linear regression (and regression models in general), which we will discuss in this lesson.]{.underline}

<br />

**Ordinal variables** are variables that have discrete categories, that can be ordered.

-   Discrete categories means the same thing as above with categorical variables.

However, ordinal variables are ordered.

-   For example, pretent you fill out a survey ranking your experience with a service, that gives you the options *poor experience, neutral, good experience*. We can say that *poor experience \< neutral*, and *neutral \< good experience*. These categories can be ordered.
-   Or for example, you are to giving a star rating to a restaurant: either 1, 2, 3, 4, or 5 stars. You cannot give 4.5 stars, or 3.432748 stars, so there are discrete categories. However you can order these categories: clearly 5 stars \> 3 stars.

However, we cannot say anything about the *distance* between categories.

-   For example, we cannot say that the distance between *poor experience* and *neutral* is equal to the distance between *neutral* and *good experience*.
-   We can only conclude on the order, not the distance.

Ordinal variables can be "treated" as continuous variables in a regression setting (see below for what continuous variables are).

-   However, they can also be treated as categorical variables (if you have some reason to believe the ordering is arbitrary and not useful).

<br />

**Continuous** variables are variables that are, well, continuous. More specifically, continuous variables have 2 characteristics:

1.  They have infinite subdivisions between any two values - there is always some value in between two different values. For example, take two values: $1$ and $2$. In a continuous variable, $1.5$ is in between them. Take another example of two values: $1$ and $1.000001$. In a continuous variable, $1.00000000001$ is between those two values.
2.  The distance between values is consistent and meaningful. For example, the distance between 3 and 1 is 2, and the distance between 11 and 9 is also 2. That distance 2 is consistent and meaningful. This is in contrast to ordinal variables.

An example of a continuous variable is temperature:

-   Temperature has infinite subdivisions: We can have temperature at 20.34372 Celsius.
-   Temperature also has consistent distance between values. Going from 5 to 6 Celsius, or 7 to 8 Celsius, are all a 1 Celsius increase.

Continuous variables can be described by probability density functions as discussed in [1.1.3](https://statsnotes.github.io/theory/1.html#probability-mass-and-density-functions).

<br />

Continuous variables have been what we have discussed so far in regression.

-   All of our previous interpretation has focused on assuming both $x$ and $y$ are continuous (or ordinal if we treat the ordinal variable as continuous).

However, categorical variables have slightly different interpretations and implementations, due to their unique characteristics of discreteness and unorderedness. This chapter focuses on how we can adjust the linear regression model to fit these characteristics.

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.8.2: Binary Explanatory Variables in Regression

**Binary explanatory** variables are categorical variables with 2 categories, 0 and 1.

-   Binary explanatory variables are extremely common in the social sciences. They can include things such as yes/no questions, treatment/control, true/false questions, voted/did not vote, etc.

Implementing binary explanatory variables is very simple in regression - we just include them in our regression model as a normal $x$:

$$
y_i = \beta_0 + \beta_1x_i + u_i
$$

-   Where $x$ is a binary explanatory variable.

However, Binary explanatory variables will change the interpretations of our coefficients, and slightly change the estimation process of OLS.

<br />

How are binary explaantory variables interpreted in linear regression? We can "solve" for these interpretations. Assume $x$ has two categories $x=0$ and $x=1$:

-   I will use the conditional expectation function since it makes more sense (as you will see in a second).

$$
\begin{split}
&E(y|x=0) = \hat\beta_0 + \hat\beta_1(0) = \hat\beta_0 \\
& E(y|x=1) = \hat\beta_0 + \hat\beta_1(1) = \hat\beta_0 + \hat\beta_1 \\
& E(y|x=1) - E(y|x=0) = (\hat\beta_0 + \hat\beta_1) - \hat\beta_0 = \hat\beta_1
\end{split}
$$

Thus, through this, we can see:

-   The expected value of $y$ when $x = 0$ is $\hat\beta_0$
-   The expected value of $y$ when $x = 1$ is $\hat\beta_0 + \hat\beta_1$
-   The difference between the expected values of $y$ when $x=1$ and $x=0$ is $\hat\beta_1$

Thus, we can interpret the coefficients as following:

::: callout-tip
## Interpretation of Coefficient with a Binary Explanatory Variable

When $x$ is a binary explanatory variable:

-   $\hat\beta_0$ is the expected value of $y$ given an observation in category $x = 0$
-   $\hat\beta_0 + \hat\beta_1$ is the expected value of $y$ given an observation in category $x = 1$
-   $\hat\beta_1$ is the expected difference in $y$ between the categories $x=1$ and $x=0$.
:::

::: callout-warning
## Warning!

Note how I have been using the word **relationship**, not causal effect.

Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).
:::

<br />

We can see here that our $\hat\beta_1$ is the expected [difference-in-means]{.underline} of the two categories of $x$.

-   In fact, computer software doing OLS estimation will simply estimate the mean $y$ of each category of $x$, and find the difference.

We will prove why computer softwares can take this shortcut, and show that OLS is equal to the difference-in-means in the next section.

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.8.3: OLS as a Difference-in-means Estimator

In the last section, we established that $\hat\beta_1$ was simply a difference in sample means of $y$ in categories $x=1$ and $x=0$.

In this section, let us mathematically prove this is the case.

We will use simple linear regression for simplicity. Recall our OLS estimation solution derived in [1.4.4](https://statsnotes.github.io/theory/4.html#mathematics-of-the-ordinary-least-squares-estimator):

$$
\hat\beta_1 = \frac{\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n (x_i - \bar x)^2}
$$

Let us focus on the numerator. Let us expand it out:

$$
\begin{split}
& \sum_{i=1}^n (x_i - \bar x)(y_i - \bar y) \\
& = \sum_{i=1}^n  (x_i y_i - x_i \bar y - \bar x y_i + \bar x \bar y) \\
& = \sum_{i=1}^n x_i y_i - \sum_{i=1}^n x_i \bar y - \sum_{i=1}^n  \bar x y_i + \sum_{i=1}^n \bar x \bar y \\
& = \sum_{i=1}^n  x_i y_i - \bar y \sum_{i=1}^n x_i - \bar x \sum_{i=1}^n  y_i + n \bar x \bar y
\end{split}
$$

Now, let us consider a few properties of summation in this scenario.

::: callout-note
## Useful Properties

Remember, $x$ is a binary variable, with two values: $x=1$ and $x=0$.

We know $n$ is the number of total observations. Let us define $n_1$ as the number of observations in category $x=1$, and $n_0$ as the number of observations in category $x=0$.

With this, we have a few properties:

**Property 1:**

$$
\sum\limits_{i=1}^n x_i y_i = n_1 \bar y _1
$$

-   Why? Think about the scenarios of $x_iy_i$
-   If we have category $x_i=0$ for observation $i$, then $x_i y_i = 0$.
-   If we have category $x_i = 1$ for observation $i$, then $x_i y_i = y_i$. So, only observations in category $x=1$ get their $y$ values as non-zero in this summation.
-   That means our sum is now $\sum_{i=1}^{n_1} y_i$ (for only observations in $x=1$), and by definition of average, we know that should equal $\sum_{i=1}^{n_1} \bar y_1$.
-   Since $\bar y$ is a constant, we know the value of the summation is $n_1 \times \bar y_1$.

**Property 2:**

$$
\sum_{i=1}^{n} x_i = n_1
$$

-   Why? Well when observation $i$ is in category $x=0$, $x_i = 0$. When observation $i$ is in category $x=1$, $x_i = 1$.
-   Thus, our sum is only adding a 1 for observations in the category $x=1$.
-   Since the sum is only summing 1's, that should equal the number of observations in category $x=1$, which we defined as $n_1$.

**Property 3:**

$$
\bar x = \frac{n_1}{n}
$$

-   Why? The expected value of a binary variable is simply the proportion of observations in category $x=1$.

**Property 4:**

$$
\sum_{i=1}^n y_i = n\bar y
$$

-   Why? By definition of average, $\sum y_i = \sum \bar y$.
-   And since $\bar y$ is a constant, $\sum^n \bar y = n \times \bar y$.
:::

<br />

Now, with these properties, we can return to the numerator, and further simplify:

$$
\begin{split}
& \sum_{i=1}^n  x_i y_i - \bar y \sum_{i=1}^n x_i - \bar x \sum_{i=1}^n  y_i + n \bar x \bar y \\
& = n_1 \bar y_1 - n_1 \bar y - \frac{n_1}{n} n \bar y + n \frac{n_1}{n}  \bar y \\
& = n_1 \bar y_1 - n_1 \bar y - n_1 \bar y + n_1 \bar y \\
& = n_1 \bar y_1 - n_1 \bar y \\
& = n_1 (\bar y_1 - \bar y)
\end{split}
$$

Now, we can consult another property, derived from the definition of expecation.

::: callout-note
## Useful Property

The definition of expectation for a discrete random variable is:

$$
E(x) = \sum\limits_{i=1}^n (x_i \times p(x_i))
$$

-   Where $p(x_i)$ is the probability associated with the specific random outcome.

We know that in our example, $\bar y$ is an expectation: $E(y)$.

We also can think of variable $y$ as a discrete variable:

-   How? We only know $x$ is a discrete variable.
-   But, let us think of $y$ in terms of $x$. We have two groups of $y$ values, one in category $x=0$, and one in category $x=1$.
-   Each group of $y$ values can be thought of having their own discrete outcome: the average $y$ in each group: $\bar y_0$ and bar $y_1$.
-   The probability of each group is simply the proportion of observations in each $x$ category compared to the total number of observations.

Using this idea, we know that:

$$
\begin{split}
E(y) = \bar y & = \sum\limits_{i=1}^n (y_i \times p(y_i)) \\
& =\bar y_0\frac{n_0}{n} + \bar y_1 \frac{n_1}{n} \\
& = \frac{1}{n}n_0 \bar y_0 + \frac{1}{n}n_1 \bar y_1 \\
& = \frac{1}{n}(n_0 \bar y_0 + n_1 \bar y_1)
\end{split}
$$

This final equation, we can plug in for $\bar y$ as our property.
:::

<br />

Now with this property for $\bar y$, we can plug it into our numerator, and get:

$$
\begin{split}
& n_1 (\bar y_1 - \bar y) \\
& = n_1 \left( \bar y_1 - \frac{1}{n}(n_0 \bar y_0 + n_1 \bar y_1) \right) \\
& = n_1 \bar y_1 - \frac{n_1}{n}(n_0 \bar y_0 + n_1 \bar y_1) \\
& = n_1 \bar y_1 - \left(  \frac{n_1n_0 \bar y_0}{n} + \frac{n_1n_1 \bar y_1}{n} \right) \\
& = n_1 \bar y_1 - \frac{n_1n_0 \bar y_0}{n} - \frac{n_1^2 \bar y_1}{n} \\
& = n_1 \bar y_1 -\frac{n_1^2 \bar y_1}{n} - \frac{n_1n_0 \bar y_0}{n} \\
& = \bar y_1 \left( n_1 - \frac{n_1^2}{n} \right) - \bar y_0 \left(\frac{n_1n_0}{n}\right)
\end{split}
$$

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.9.4: Categorical Explanatory Variables in Regression

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.9.5: The Dummy Variable Trap

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.9.6: Introduction to Fixed Effects

<br />

------------------------------------------------------------------------

[Homepage](https://statsnotes.github.io)

# 1.9.7: Clustered Standard Errors
