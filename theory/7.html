<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistics for Social Scientists</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="7_files/libs/clipboard/clipboard.min.js"></script>
<script src="7_files/libs/quarto-html/quarto.js"></script>
<script src="7_files/libs/quarto-html/popper.min.js"></script>
<script src="7_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="7_files/libs/quarto-html/anchor.min.js"></script>
<link href="7_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="7_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="7_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="7_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="7_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Statistics for Social Scientists</h1>
            <p class="subtitle lead">Lesson 1.7: Multiple Regression - Properties and Inference</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Lesson 1.7: Multiple Regression - Properties and Inference</h2>
   
  <ul class="collapse">
  <li><a href="#residuals-and-algebraic-properties-of-ols" id="toc-residuals-and-algebraic-properties-of-ols" class="nav-link active" data-scroll-target="#residuals-and-algebraic-properties-of-ols">1.7.1: Residuals and Algebraic Properties of OLS</a></li>
  <li><a href="#unbiasedness-of-ols-under-the-gauss-markov-theorem" id="toc-unbiasedness-of-ols-under-the-gauss-markov-theorem" class="nav-link" data-scroll-target="#unbiasedness-of-ols-under-the-gauss-markov-theorem">1.7.2: Unbiasedness of OLS Under the Gauss-Markov Theorem</a></li>
  <li><a href="#proof-of-unbiasedness-of-ols-under-gauss-markov" id="toc-proof-of-unbiasedness-of-ols-under-gauss-markov" class="nav-link" data-scroll-target="#proof-of-unbiasedness-of-ols-under-gauss-markov">1.7.3: Proof of Unbiasedness of OLS under Gauss-Markov</a></li>
  <li><a href="#omitted-variable-bias" id="toc-omitted-variable-bias" class="nav-link" data-scroll-target="#omitted-variable-bias">1.7.4: Omitted Variable Bias</a></li>
  <li><a href="#endogeneity-and-violation-of-gauss-markov" id="toc-endogeneity-and-violation-of-gauss-markov" class="nav-link" data-scroll-target="#endogeneity-and-violation-of-gauss-markov">1.7.5: Endogeneity and Violation of Gauss-Markov</a></li>
  <li><a href="#variance-of-the-ols-estimator" id="toc-variance-of-the-ols-estimator" class="nav-link" data-scroll-target="#variance-of-the-ols-estimator">1.7.6: Variance of the OLS Estimator</a></li>
  <li><a href="#statistical-inference-in-multiple-linear-regression" id="toc-statistical-inference-in-multiple-linear-regression" class="nav-link" data-scroll-target="#statistical-inference-in-multiple-linear-regression">1.7.7: Statistical Inference in Multiple Linear Regression</a></li>
  <li><a href="#hypothesis-testing-with-more-than-one-coefficient" id="toc-hypothesis-testing-with-more-than-one-coefficient" class="nav-link" data-scroll-target="#hypothesis-testing-with-more-than-one-coefficient">1.7.8: Hypothesis Testing with More than One Coefficient</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>This lesson covers the following topics:</p>
<ul>
<li>Some simple algebraic properties of our best-fit plane through OLS estimation.</li>
<li>The conditions on which OLS estimation is an unbiased estimator for multiple regression (Gauss-Markov conditions), and how violations of these conditions will lead to biased estimates.</li>
<li>Omitted Variable Bias, and how it causes violations of the Gauss-Markov conditions.</li>
<li>How we can conduct hypothesis testing in multiple linear regression.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This entire lesson will mirror <a href="https://statsnotes.github.io/theory/5.html">lesson 1.5</a>, but will adjust the properties of bivariate regression to multiple regression. I recommend a strong understanding of those topics before starting this chapter.</p>
</div>
</div>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
<section id="residuals-and-algebraic-properties-of-ols" class="level1">
<h1>1.7.1: Residuals and Algebraic Properties of OLS</h1>
<p>Once we have our OLS estimates of parameters <span class="math inline">\hat\beta_0 , \dots \hat\beta_k</span> (estimated in <a href="https://statsnotes.github.io/theory/6.html#mathematics-of-the-ordinary-least-squares-estimator">1.6.4</a>), we can obtain the OLS residuals <span class="math inline">\hat u_i</span>:</p>
<p><span class="math display">
\begin{split}
\hat u_i &amp; = y_i - \hat y_i \\
&amp; = y_i - (\hat \beta_0 + \hat\beta_1x_{1i} + \dots + \hat\beta_k x_{ki}) \\
&amp; = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}
\end{split}
</span></p>
<ul>
<li>If you do not know what residuals <span class="math inline">\hat u_i</span> are, and why they are different from <span class="math inline">u_i</span> error term, see <a href="https://statsnotes.github.io/theory/5.html#residuals-and-algebraic-properties-of-ols">1.5.1</a>.</li>
</ul>
<p>We can also derive our residuals <span class="math inline">\hat u</span> from the linear algebra solution of OLS.</p>
<p><span class="math display">
\begin{split}
\hat u &amp; = y - \hat y \\
&amp; = y - X \hat\beta \\
&amp; = y - X(X'X)^{-1}X'y
\end{split}
</span></p>
<p><br></p>
<p>Recall that the OLS estimates of <span class="math inline">\hat\beta_0 , \dots \hat\beta_k</span> are chosen to satisfy the following first order conditions (see <a href="https://statsnotes.github.io/theory/6.html#mathematics-of-the-ordinary-least-squares-estimator">1.6.4</a>):</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\&amp; \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
&amp; \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>Using these properties, we can plug in <span class="math inline">\hat u_i = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}</span> to get the above conditions:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n \hat u_i = 0 \\
&amp; \sum\limits_{i=1}^n x_{1i} \hat u_i = 0 \\
&amp; \sum\limits_{i=1}^n x_{2i} \hat u_i = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>These conditions can be used to find a few properties of OLS in multiple linear regression.</p>
<p><br></p>
<section id="property-1-sum-of-residuals-is-0" class="level3">
<h3 class="anchored" data-anchor-id="property-1-sum-of-residuals-is-0">Property 1: Sum of Residuals is 0</h3>
<p>OLS residuals always add up to zero, since:</p>
<p><span class="math display">
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = \sum\limits_{i=1}^n \hat u_i = 0
</span></p>
<ul>
<li>This property ensures that the average value of <span class="math inline">y_i</span> in our data is the same as the average value of our predictions <span class="math inline">\hat y_i</span>.</li>
</ul>
<p><br></p>
</section>
<section id="property-2-no-covariance-between-any-x-and-residual." class="level3">
<h3 class="anchored" data-anchor-id="property-2-no-covariance-between-any-x-and-residual.">Property 2: No Covariance Between Any <span class="math inline">x</span> and Residual.</h3>
<p>Using the first order conditions of OLS, and the definition of <span class="math inline">\hat u_i</span> from above, we know the following to be true:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n x_{1i} \hat u_i = 0 \\
&amp; \sum\limits_{i=1}^n x_{2i} \hat u_i = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>Using the same proof from <a href="https://statsnotes.github.io/theory/5.html#residuals-and-algebraic-properties-of-ols">1.5.1</a>, we know the following are true:</p>
<p><span class="math display">
\begin{split}
&amp; Cov(x_1, \hat u) = 0 \\
&amp; Cov(x_2, \hat u) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>Thus, the covariances (and thus correlations) between all explanatory variables <span class="math inline">x_1, \dots, x_k</span> and <span class="math inline">\hat u</span> must be zero.</p>
<p><br></p>
</section>
<section id="property-3-regression-line-passes-through-means" class="level3">
<h3 class="anchored" data-anchor-id="property-3-regression-line-passes-through-means">Property 3: Regression Line Passes Through Means</h3>
<p>For a similar proof in <a href="https://statsnotes.github.io/theory/5.html#residuals-and-algebraic-properties-of-ols">1.5.1</a>, we know that the point <span class="math inline">(\bar y, \bar x_1, \dots, \bar x_k)</span> is on the regression plane.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="unbiasedness-of-ols-under-the-gauss-markov-theorem" class="level1">
<h1>1.7.2: Unbiasedness of OLS Under the Gauss-Markov Theorem</h1>
<p>Unbiasedness, as we have discussed in both <a href="https://statsnotes.github.io/theory/2.html#properties-of-estimators">1.2.8</a> and <a href="https://statsnotes.github.io/theory/5.html#unbiasedness-of-ols-under-the-gauss-markov-theorem">1.5.2</a>, is a desirable property of estimators.</p>
<p>Just like for simple linear regression, the Gauss-Markov Theorem proves that OLS is unbiased under 4 conditions.</p>
<ul>
<li>Notation note: <span class="math inline">\beta_j</span> refers to any one of <span class="math inline">\beta_1, \dots, \beta_k</span>. It can be generalised to any of the coefficients besides the intercept.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Gauss-Markov Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Gauss-Markov Theorem states that under 4 conditions, the OLS estimator is an unbiased estimator for <span class="math inline">\beta_j</span>.</p>
<ul>
<li><strong>MLR.1 (Linearity in Parameters)</strong>: The parameters of the model are linear.</li>
<li><strong>MLR.2 (Random Sampling)</strong>: All observations in our sample are randomly sampled from the population.</li>
<li><strong>MLR.3 (No Perfect Mutlicollinearity)</strong>: There are no exact linear relationships among variables (where correlation coefficient equals 1).</li>
<li><strong>MLR.4 (Zero Conditional Mean)</strong>. The error term <span class="math inline">u</span> has an expectation of 0, given any value of <span class="math inline">x_j</span> for any explanatory variable.</li>
</ul>
<p><u>Note specifically that MLR.3 is different than the equivalent condition for simple linear regression</u>. We will explore each condition in detail.</p>
</div>
</div>
<p><br></p>
<section id="assumption-mlr.1-linearity" class="level3">
<h3 class="anchored" data-anchor-id="assumption-mlr.1-linearity">Assumption MLR.1: Linearity</h3>
<p>Assumption MLR.1 states that a model must be <strong>linear in parameters</strong>.</p>
<p>This means that the parameters of the model <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> must not be multiplied together - they must be added together.</p>
<p>For example, the following is linear:</p>
<p><span class="math display">
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u
</span></p>
<p>While the following is not linear (since parameters <span class="math inline">\beta_1</span> and <span class="math inline">\beta_2</span> are multiplied):</p>
<p><span class="math display">
y = \beta_0 + \beta_1\beta_2x_1 + \beta_3 x_2 + u
</span></p>
<p>Note: This does not mean that the actual regression line must be linear - only the parameters/coefficients must not be multiplied, the variables can be. For example, the following model is still linear in parameters:</p>
<p><span class="math display">
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + u
</span></p>
<p><br></p>
</section>
<section id="assumption-mlr.2-random-sampling" class="level3">
<h3 class="anchored" data-anchor-id="assumption-mlr.2-random-sampling">Assumption MLR.2: Random Sampling</h3>
<p>This assumption says that all observations in our sample are randomly sampled from the same population.</p>
<p>Why is this useful? Take the standard linear model:</p>
<p><span class="math display">
y = \beta_0 + \beta_1x_1 + \dots + \beta_kx_k + u
</span></p>
<p>The error term <span class="math inline">u</span> is some random variable (with its own probability distribution), that can be defined by its expectation <span class="math inline">E(u)</span>.</p>
<ul>
<li>If we randomly select one observation <span class="math inline">i</span> from the data, each observation <span class="math inline">i</span> has an equal chance of being selected.</li>
<li>The error term for that observation, <span class="math inline">u_i</span> should also have the same expectation as the random variable <span class="math inline">u</span>, since each observation <span class="math inline">i</span> within <span class="math inline">u</span> has the same chance of being selected.</li>
</ul>
<p>Thus, random sampling allows us to say <span class="math inline">E(u) = E(u_i)</span>.</p>
<p>Why is this not the case when random sampling is not present?</p>
<ul>
<li>Imagine you are not random sampling. Perhaps, some observations <span class="math inline">i</span> with lower <span class="math inline">u_i</span> values are more likely to be sampled.</li>
<li>If there is no random sampling, then <span class="math inline">E(u_i)</span> may not equal <span class="math inline">E(u)</span>.</li>
</ul>
<p><br></p>
</section>
<section id="assumption-mlr.3-no-perfect-multicollinearity" class="level3">
<h3 class="anchored" data-anchor-id="assumption-mlr.3-no-perfect-multicollinearity">Assumption MLR.3: No Perfect Multicollinearity</h3>
<p>Recall that in <a href="https://statsnotes.github.io/theory/6.html#regression-anatomy-formula-for-ols-estimation">1.6.6</a>, we derived the OLS estimator in terms of regression anatomy as:</p>
<p><span class="math display">
\hat\beta_j = \frac{\sum_{i=1}^n \widetilde{r_{ji}} \ y_i}{\sum_{i=1}^n \widetilde{r_{ji}}^2}
</span></p>
<p>Where <span class="math inline">\widetilde{r_{ij}}</span> is the part of <span class="math inline">x_j</span> that is uncorrelated with any other explanatory variables.</p>
<p>Obviously, as we know from mathematics, the denominator cannot be zero, or else, <span class="math inline">\hat\beta_j</span> will not exists. This implies that we cannot derive <span class="math inline">\hat\beta_j</span> if <span class="math inline">\sum \widetilde{r_{ij}}^2 = 0</span>.</p>
<ul>
<li>What causes <span class="math inline">\sum\widetilde{r_{ij}}^2 = 0</span>?</li>
<li>If <span class="math inline">\sum\widetilde{r_{ij}}^2 = 0</span>, that means there is no part of <span class="math inline">x_j</span> that is completely uncorrelated with any other explanatory variable.</li>
<li>In other words, that means <span class="math inline">x_j</span> is <strong>perfectly correlated</strong> to at least one other explanatory variable.</li>
</ul>
<p>Thus, if <span class="math inline">x_j</span> is perfectly correlated with any other explanatory variable, coefficient <span class="math inline">\hat\beta_j</span> is not calculable.</p>
<p><br></p>
<p>But what does perfectly correlated mean? Mathematically, it means the correlation coefficient between <span class="math inline">x_j</span> and any other explanatory variable <span class="math inline">x_m</span> is equal to 1 or -1.</p>
<p><span class="math display">
|Corr(x_j, x_m)| = 1
</span></p>
<p>But practically, what does that mean when choosing our explanatory variables?</p>
<ul>
<li>It means that we cannot choose measurements of the same concept in different units.</li>
<li>For example, you cannot include the two variables <em>height in inches</em> and <em>height in cm</em> in the same regression. This is because both variables are perfectly correlated, just differing only by a common factor.</li>
</ul>
<p>This only applies if variables are <strong>perfectly</strong> correlated.</p>
<ul>
<li>However, even if there is no perfect correlation, but a high correlation, this can cause some issues (not related to unbiasedness). We will discuss this later in section 1.7.6.</li>
</ul>
<p><br></p>
</section>
<section id="assumption-mlr.4-zero-conditional-mean" class="level3">
<h3 class="anchored" data-anchor-id="assumption-mlr.4-zero-conditional-mean">Assumption MLR.4: Zero-Conditional Mean</h3>
<p>In the population, the error term <span class="math inline">u</span> must have an expecation of 0, given all values of <span class="math inline">x</span>. Mathematically:</p>
<p><span class="math display">
E(u|x_1, \dots x_k) = 0 \text{ for all } (x_1, \dots , x_k)
</span></p>
<p>And using condition MLR.2 Random Sampling (from above), we also know that:</p>
<p><span class="math display">
E(u|x_1, \dots ,x_k) = E(u_i|x_{1i}, \dots, x_{ki}) = 0
</span></p>
<p>This also implies that the error term <span class="math inline">u</span> and any <span class="math inline">x_1, \dots , x_k</span> are uncorrelated with each other.</p>
<ul>
<li>This is because if <span class="math inline">u</span> was correlated with <span class="math inline">x_j</span>, it would not always be equal to 0 no matter the value of <span class="math inline">x</span>.</li>
</ul>
<p>Since the correlation is 0, the covariance between <span class="math inline">u</span> and any <span class="math inline">x_1, \dots, x_k</span> must also be 0:</p>
<p><span class="math display">
\begin{split}
&amp; Cov(x_1,u) = 0 \\
&amp; Cov(x_2, u) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p><br></p>
<p>This alternative statement relating to correlation/covariance is called <strong>exogeneity</strong>.</p>
<ul>
<li>If this assumption is violated (where <span class="math inline">x_j</span> is correlated with error <span class="math inline">u</span>), then we have <strong>endogeneity</strong>, and <span class="math inline">x_j</span> is considered an <strong>endogeneous regressor</strong>.</li>
<li>We will discuss endogeneity in more detail in lesson 1.7.</li>
</ul>
<p>This is the most frequently violated of assumptions, yet arguable the most important of assumptions to ensure OLS is unbiased.</p>
<ul>
<li>We will mathematically show this in the next lesson.</li>
<li>Because it is so frequently violated, we will need to introduce further methods beyond simple linear regression for accurate estimation.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="proof-of-unbiasedness-of-ols-under-gauss-markov" class="level1">
<h1>1.7.3: Proof of Unbiasedness of OLS under Gauss-Markov</h1>
<p>In the last section, we covered the Gauss-Markov assumptions, and how when they are met, OLS is unbiased. In this section, we will use those assumptions to prove the unbiasedness of OLS.</p>
<ul>
<li>Or in other words, we want to show <span class="math inline">E(\hat\beta_j) = \beta_j</span>.</li>
</ul>
<p>For simplicity, let us focus on <span class="math inline">\hat\beta_1</span>. However, this can be generalised to any <span class="math inline">\hat\beta_2, \dots, \hat\beta_k</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summations
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><strong>Property 1:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n \widetilde{r_{1i}} = 0
</span></p>
<ul>
<li>This is because <span class="math inline">\widetilde{r_{1i}}</span> is a residual term of a OLS regression of outcome <span class="math inline">x_1</span> and explanatory variables <span class="math inline">x_2, \dots, x_k</span>, and we know OLS residuals sum to 0 (algebraic properties discussed in <a href="https://statsnotes.github.io/theory/7.html#residuals-and-algebraic-properties-of-ols">1.7.1</a>).</li>
</ul>
<p><strong>Property 2:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n \widetilde{r_{1i}} x_{ji} = 0, \text{ given } j=2, \dots, k
</span></p>
<ul>
<li>Because for OLS, <span class="math inline">\sum x_i \hat u_i = 0</span> (<a href="https://statsnotes.github.io/theory/7.html#residuals-and-algebraic-properties-of-ols">1.7.1</a>), and we know <span class="math inline">\widetilde{r_{1i}}</span> is the residual <span class="math inline">\hat u_i</span> in a regression with explanatory variables <span class="math inline">x_2, \dots, x_k</span> and outcome variable <span class="math inline">x_1</span>.</li>
</ul>
<p><strong>Property 3:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n \widetilde{r_{1i}} x_{1i} = \sum\limits_{i=1}^n \widetilde{r_{1i}}(\hat x_{1i} + \sum\limits_{i=1}^n \widetilde{r_{1i}}) = \sum\limits_{i=1}^n \widetilde{r_{1i}}^2
</span></p>
<ul>
<li>Because we have the regression fitted values <span class="math inline">\hat x_{1i} = \hat \gamma_0 + \hat\gamma_1 x_2 + \dots + \hat\gamma_{k-1} x_{k} + \widetilde{r_{1i}}</span> from <a href="https://statsnotes.github.io/theory/6.html#regression-anatomy-formula-for-ols-estimation">1.6.6</a>.</li>
<li>And we know with regression, actual values are the predicted plus residual: <span class="math inline">y_i = \hat y_i + \hat u_i</span>. Thus, <span class="math inline">x_i = \hat x_i + \widetilde{r_{1i}}</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>Recall the regression anatomy solution of OLS for <span class="math inline">\hat\beta_1</span> (derived in <a href="https://statsnotes.github.io/theory/6.html#regression-anatomy-formula-for-ols-estimation">1.6.6</a>):</p>
<p><span class="math display">
\hat\beta_1 = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ y_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
</span></p>
<ul>
<li>Where <span class="math inline">\widetilde{r_{1i}}</span> is the part of <span class="math inline">x_1</span> uncorrelated with <span class="math inline">x_2, \dots, x_k</span>.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.3 No Perfect Multicollinearity
</div>
</div>
<div class="callout-body-container callout-body">
<p>The existence of <span class="math inline">\hat\beta_1</span> is guaranteed by condition 3 from above, since the denominator <span class="math inline">\sum\widetilde{r_{1i}}^2</span> must not be 0.</p>
</div>
</div>
<p>Now, let us plug in <span class="math inline">y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i</span> into our regression anatomy formula:</p>
<p><span class="math display">
\begin{split}
\hat\beta_1 &amp; = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ y_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ (\beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i)}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
\end{split}
</span></p>
<p>Now, focusing on the numerator, and using the summation properties above, let us simplify:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n \widetilde{r_{1i}} \ (\beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i) \\
&amp; = \sum\limits_{i=1}^n (\widetilde{r_{1i}}\beta_0 + \widetilde{r_{1i}}\beta_1x_{1i} + \dots + \widetilde{r_{1i}}\beta_k x_{ki} + \widetilde{r_{1i}} u_i) \\
&amp; = \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_0 + \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_1 x_{1i} + \dots + \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_k x_{ki} + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
&amp; = \beta_0 \sum\limits_{i=1}^n \widetilde{r_{1i}} + \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}} x_{1i} + \dots + \beta_k \sum\limits_{i=1}^n \widetilde{r_{1i}} x_{ki} + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
&amp; = \beta_0 (0) + \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}}^2 + \beta_2 (0) + \dots + \beta_k (0) + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
&amp; = \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}}^2 + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i
\end{split}
</span></p>
<p>Now, putting the numerator back in, we can simplify:</p>
<p><span class="math display">
\begin{split}
\hat\beta_1 &amp; = \frac{\beta_1 \sum_{i=1}^n \widetilde{r_{1i}}^2 + \sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; \\
&amp; = \frac{\beta_1 \sum_{i=1}^n \widetilde{r_{1i}}^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2} + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; \\
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
</span></p>
<p><br></p>
<p>Now, we want to find <span class="math inline">E(\hat\beta_1)</span>. Note that the second part of the equation is a function of <span class="math inline">u_i</span>, of which itself is a function of all explanatory variables <span class="math inline">x_{1i}, \dots, x_{ki}</span>. Thus, we know:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1 | x_{1i}, \dots x_{ki}) &amp; = \beta_1 + E\left( \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \biggr|x_{1i},\dots, x_{ki} \right) \\
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ E(u_i|x_{1i}, \dots,x_{ki})}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
</span></p>
<p>But what is <span class="math inline">E(u_i|x_{1i}, \dots , x_{ki})</span>? We can use two Gauss-Markov conditions to evaluate this:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.4 Zero Conditional Mean
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Zero-Conditional Mean assumption says <span class="math inline">E(u|x_1, \dots, x_k) = 0</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.2 Random Sampling
</div>
</div>
<div class="callout-body-container callout-body">
<p>Random sampling allows us to say that <span class="math inline">E(u|x_1, \dots, x_k)</span> is the same as <span class="math inline">E(u_i|x_{1i}, \dots, x_{ki})</span>, where both equal 0.</p>
</div>
</div>
<p>Thus, with these two assumptions, we know that:</p>
<p><span class="math display">
E(u_i|x_{1i}, \dots, x_{ki}) = 0
</span></p>
<p>Which means that:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1 | x_{1i}, \dots x_{ki})
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ E(u_i|x_{1i}, \dots,x_{ki})}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ (0)}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; = \beta_1 + 0 \\
&amp; = \beta_1
\end{split}
</span></p>
<p>Now, just like in simple linear regression (see <a href="https://statsnotes.github.io/theory/5.html#proof-of-unbiasedness-of-ols-under-gauss-markov">1.5.3</a>), we use the <strong>law of iterated expectations</strong> to conclude this proof:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1) &amp; = E[E(\hat\beta_1|x_{1i}, \dots, x_{ki})] \\
&amp; = E(\beta_1) \\
&amp; = \beta_1
\end{split}
</span></p>
<p>Thus, OLS is unbiased under the Gauss-Markov conditions.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="omitted-variable-bias" class="level1">
<h1>1.7.4: Omitted Variable Bias</h1>
<p>In the previous section (and in simple linear regression section <a href="https://statsnotes.github.io/theory/5.html#unbiasedness-of-ols-under-the-gauss-markov-theorem">1.5.2</a>), we discussed how the assumption of <strong>zero-conditional mean</strong> <span class="math inline">E(u | x_1, \dots, x_k) = 0</span> is critical to the unbiasdness of OLS.</p>
<ul>
<li>This assumption is also called <strong>exogeneity</strong>.</li>
</ul>
<p>However, this assumption is frequently violated. The most common reason for this is because of <strong>omitted variable bias</strong>.</p>
<p><br></p>
<p>Consider two regressions, one with only our exlanatory variable of interest <span class="math inline">D</span>, and, and one with an extra control variable <span class="math inline">x</span> that is omitted from the first regression:</p>
<p><span class="math display">
\begin{split}y_i &amp; = \beta_0^S + \beta_1^SD_i + u_i^S \quad \text{short} \\y_i &amp; = \beta_0 + \beta_1D_i + \beta_2x_i + u_i \quad \text{long}\end{split}
</span></p>
<p>Now consider an auxiliary regression, where the omitted variable <span class="math inline">x</span> is the outcome variable, and <span class="math inline">D_i</span> is the explanatory variable:</p>
<p><span class="math display">
x_i = \delta_0 + \delta_1 D_i + v_i
</span></p>
<ul>
<li>where <span class="math inline">\delta_0, \delta_1</span> are coefficients and <span class="math inline">v_i</span> is the error term</li>
</ul>
<p><br></p>
<p>Now we have <span class="math inline">x_i</span> in terms of <span class="math inline">D_i</span>, let us plug <span class="math inline">x_i</span> into our long regression to “recreate” the short regression:</p>
<p><span class="math display">
\begin{split}y_i &amp; = \beta_0 + \beta_1D_i + \beta_2x_i + u_i \\y_i &amp; = \beta_0 + \beta_1 D_i + \beta_2(\delta_0 + \delta_1D_i + v_i) + u_i \\y_i &amp; = \beta_0 + \beta_1 D_i + \beta_2 \delta_0 + \beta_2 \delta_1 D_i + \beta_2v_i + u_i \\y_i &amp; = \beta_0 + \beta_2 \delta_0 + (\beta_1 + \beta_2 \delta_1)D_i + \beta_2v_i + u_i\end{split}
</span></p>
<p>We have “recreated” the short regression with one variable <span class="math inline">D</span>. Let us see our recreation next to the original short regression:</p>
<p><span class="math display">
\begin{split}
y_i &amp; = \beta_0^S + \beta_1^SD_i + u_i^S \\
y_i &amp; = \beta_0 + \beta_2 \delta_0 + (\beta_1 + \beta_2 \delta_1)D_i + \beta_2v_i + u_i
\end{split}
</span></p>
<ul>
<li>The short regression <span class="math inline">\beta_0^S</span> is analogous to the <span class="math inline">\beta_0 + \beta_2 \delta_0</span> in the recreation (both are the intercepts)</li>
<li>The short regression <span class="math inline">\beta_1^S D_i</span> is analogous to the <span class="math inline">(\beta_1 + \beta_2 \delta_1)D_i</span> in the recreation (both are the slope and variable of interest)</li>
<li>The short regression <span class="math inline">u_i^S</span> is analogous to the <span class="math inline">\beta_2 v_i + u_i</span> in the recreation (both are the error terms).</li>
</ul>
<p>Since the short regression <span class="math inline">\beta_1^S D_i</span> is analogous to the <span class="math inline">(\beta_1 + \beta_2 \delta_1)D_i</span> in the recreation, that means coefficient <span class="math inline">\beta_1^S = \beta_1 + \beta_2 \delta_1</span>.</p>
<p><br></p>
<p>The difference between the short regression coefficient <span class="math inline">\beta_1^S</span>, and the original long regression coefficient <span class="math inline">\beta_1</span>, is <span class="math inline">\beta_2 \delta_1</span>.</p>
<ul>
<li>Or in other words, <span class="math inline">\beta_2 \delta_1</span>, which is actually some of the effect of omitted <span class="math inline">x</span> on <span class="math inline">y</span>, is being “tangled up” into the coefficient of the short regression <span class="math inline">\beta_1^S</span> (which is only supposed to describe the relationship between <span class="math inline">D</span> and <span class="math inline">y</span>).</li>
</ul>
<p>If <span class="math inline">\beta_2 = 0</span> (meaning no relationship between omitted <span class="math inline">x_i</span> and <span class="math inline">y</span>), or <span class="math inline">\delta_1 = 0</span> (meaning no relationship between omitted <span class="math inline">x_i</span> and <span class="math inline">D_i</span>), then difference <span class="math inline">\beta_2 \delta_1 = 0</span>, thus there is no issue.</p>
<ul>
<li>But if either of those facts are not true, then <span class="math inline">\beta_2 \delta_1 ≠ 0</span>, and omitted variable bias is non-zero.</li>
</ul>
<p>In the next section, we will talk about the implications of this.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="endogeneity-and-violation-of-gauss-markov" class="level1">
<h1>1.7.5: Endogeneity and Violation of Gauss-Markov</h1>
<p>In the last section, we discussed omitted variable bias. But why is omitted variable bias (if non-zero) an issue?</p>
<ul>
<li>The omitted variable <span class="math inline">x</span>’s effect is mostly subsumed into the error term <span class="math inline">u_i^S</span> of the short regression.</li>
<li>But some bit of <span class="math inline">x</span> (that is correlated with <span class="math inline">D</span>) is included in our coefficient (specifically, <span class="math inline">\beta_2 \delta_1</span>).</li>
<li>That means our treatment variable <span class="math inline">D</span> will be correlated with the error term, <u>violating the Gauss-Markov assumption of zero-conditional mean, and producing biased OLS results</u>.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Endogeneity
</div>
</div>
<div class="callout-body-container callout-body">
<p>Endogeneity is when a regressor/explanatory variable <span class="math inline">x_j</span> is correlated with the error term <span class="math inline">u_i</span>.</p>
<ul>
<li>This frequently occurs due to <strong>omitted variable bias</strong>.</li>
<li>The explanatory variable that is correlated with the error term is called an <strong>endogenous variable</strong>.</li>
</ul>
<p>Mathematically, endogeneity is defined as when:</p>
<p><span class="math display">
Cov(x_j, u) ≠ 0
</span></p>
<p>When endogeneity exists, the assumption of zero-conditional mean/exogeneity <span class="math inline">E(u|x_1, \dots, x_k) = 0</span> is violated (this assumption was introduced in <a href="https://statsnotes.github.io/theory/7.html#unbiasedness-of-ols-under-the-gauss-markov-theorem">1.7.2</a>).</p>
<ul>
<li>This means that OLS estimates are no longer unbiased if endogeneity/an endogenous variable are present.</li>
</ul>
</div>
</div>
<p><br></p>
<p><u>Thus, the violation of the MLR.4 Zero-Conditional Mean Asusmption frequently results from omitting confounding variables that are correlated with both our explanatory variable of interest and the outcome variable.</u></p>
<ul>
<li>Endogeneity can also be caused by other factors, such as measurement error, or simultaneity (when both the explanatory variable and outcome variable explain each other). However, the primary concern that we worry about in econometrics is omitted variable bias.</li>
</ul>
<p>How can we solve this problem?</p>
<ul>
<li>The easiest way is to add more control variables.</li>
<li>If we include every single possible confounder that is correlated both with our explanatory variable of interest, and the outcome variable, then, there will be no more omitted variable bias, and no more endogeneity.</li>
</ul>
<p>However, as we will discuss a lot more in Part II on causal inference, including all confounding variables is often impossible, as many social science situations will have thousands of them, many of them <strong>unobservable</strong> or impossible to measure.</p>
<ul>
<li>Thus, there are also other techniques to “unbias” OLS without including all possible confounders. We will discuss an example of this in lesson 1.11 on instrumental variables.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="variance-of-the-ols-estimator" class="level1">
<h1>1.7.6: Variance of the OLS Estimator</h1>
<p>We have proved that OLS is unbiased under 4 conditions.</p>
<p>However, if we recall from <a href="https://statsnotes.github.io/theory/2.html#properties-of-estimators">1.2.8</a>, unbiasedness is not the only thing we care about in an estimator. We also care about the estimators variance.</p>
<ul>
<li>This was further discussed in <a href="https://statsnotes.github.io/theory/5.html#variance-of-the-ols-estimator-and-homoscedasticity">1.5.4</a> about the simple linear regression model.</li>
</ul>
<p>Just like in simple linear regression, an additional condition, homoscedasticity, can be added to the 4 existing conditions to determine that OLS is the linear estimator with the least variance (see <a href="https://statsnotes.github.io/theory/5.html#variance-of-the-ols-estimator-and-homoscedasticity">1.5.4</a> for more details on homoscedasticity):</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Gauss-Markov Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Gauss-Markov Theorem states that under 5 conditions, the OLS estimator is the Best Linear Unbiased Estimator (BLUE) for <span class="math inline">\beta_j</span>, being the unbiased linear estimator with the least variance.</p>
<ul>
<li><strong>MLR.1 (Linearity in Parameters)</strong>: The parameters of the model are linear.</li>
<li><strong>MLR.2 (Random Sampling)</strong>: All observations in our sample are randomly sampled from the population.</li>
<li><strong>MLR.3 (No Perfect Mutlicollinearity)</strong>: There are no exact linear relationships among variables (where correlation coefficient equals 1).</li>
<li><strong>MLR.4 (Zero Conditional Mean)</strong>. The error term <span class="math inline">u</span> has an expectation of 0, given any value of <span class="math inline">x_j</span> for any explanatory variable.</li>
<li><strong>MLR.5 (Homoscedasticity)</strong>: The error term has the same variance given any value of <span class="math inline">x</span>: <span class="math inline">Var(u|x_1, \dots, x_k) = \sigma^2</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>Assuming homoscedasticity is met, we know <span class="math inline">Var(u|x_1, \dots, x_k) = \sigma^2</span>.</p>
<ul>
<li>When homoscedasticity is not met, nothing in this section applies. See lesson 1.8 for more information on how to do estimates when this is violated.</li>
</ul>
<p>Let us find the variance of OLS estimates (we will use <span class="math inline">\hat\beta_1</span> for simplicity, but this applies to any other coefficient <span class="math inline">\hat\beta_1 , \dots, \hat\beta_k</span>. In proving unbiasedness of OLS in <a href="https://statsnotes.github.io/theory/7.html#proof-of-unbiasedness-of-ols-under-gauss-markov">1.7.3</a>, we got to this stage:</p>
<p><span class="math display">
\hat\beta_1 = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
</span></p>
<p>We know that <span class="math inline">\beta_1</span> is a constant (the true value in the population), so that never changes. Thus, it cannot be the variance in <span class="math inline">\hat\beta_1</span>. Thus, the second part is the variance in <span class="math inline">\hat\beta_1</span>.</p>
<p>Let us define <span class="math inline">w_i</span> as following, as a function of <span class="math inline">x_1, \dots, x_k</span>:</p>
<p><span class="math display">
w_i = \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2}
</span></p>
<p>This allows us to write <span class="math inline">\hat\beta_1</span> as:</p>
<p><span class="math display">
\hat\beta_1 = \beta_1 + \sum\limits_{i=1}^nw_i u_i
</span></p>
<p><br></p>
<p>Thus, we can proceed in the same way as the simple linear regression case (see <a href="https://statsnotes.github.io/theory/5.html#variance-of-the-ols-estimator-and-homoscedasticity">1.5.4</a> for more details):</p>
<p><span class="math display">
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) &amp; = Var\left( \sum\limits_{i=1}^nw_i u_i \biggr|x_1, \dots ,x_k \right) \\ &amp; = \sum\limits_{i=1}^n Var(w_i u_i | x_1, \dots, x_k) \\
&amp; = \sum\limits_{i=1}^n w_i^2 \ Var(u_i | x_1, \dots, x_k) \\
&amp; = \sum\limits_{i=1}^n w_i^2 \sigma^2 \\
&amp; = \sigma^2 \sum\limits_{i=1}^n w_i^2
\end{split}
</span></p>
<p>Now, plugging back in <span class="math inline">w_i</span>, we get:</p>
<p><span class="math display">
\begin{split}
Var(\hat\beta_1|x_1, \dots, x_k) &amp; = \sigma^2 \sum\limits_{i=1}^n w_i^2 \\
&amp; = \sigma^2 \sum_{i=1}^n \left( \frac{\widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \right) \\
&amp; = \sigma^2 \frac{\sum_{i=1}^n \widetilde{r_{1i}}}{\sum_{i=1}^n\widetilde{r_{1i}}^2} \\
&amp; = \frac{\sigma^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
</span></p>
<p>And assuming homoscedasticity (where <span class="math inline">Var(\hat\beta_1)</span> does not depend on <span class="math inline">x_1, \dots, x_k</span>, we thus know:</p>
<p><span class="math display">
Var(\hat\beta_1) = \frac{\sigma^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
</span></p>
<p><br></p>
<p>Notice how the denominator is <span class="math inline">\sum\widetilde{r_{1i}}^2</span>. That means, the smaller <span class="math inline">\sum\widetilde{r_{1i}}^2</span> is, the larger the variance is.</p>
<ul>
<li><span class="math inline">\sum\widetilde{r_{1i}}^2</span> is smaller when our explanatory variable of interest <span class="math inline">x_1</span> is highly correlated with another explanatory variable, since <span class="math inline">\widetilde{r_{1i}}^2</span> represents the part of <span class="math inline">x_1</span> that is uncorrelated with other explanatory variables.</li>
<li>This means that if we have highly correlated explanatory variables, our variance will in our OLS estimates will be high.</li>
</ul>
<p>This is something important to think about when choosing explanatory variables. We want to include all confounding variables, but highly correlated confounders will make our variance very high for our estimates.</p>
<ul>
<li>One way of dealing with this is dimensional reduction techniques, which we will discuss in Part III of the guide dealing with multivariate anlaysis.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
<section id="statistical-inference-in-multiple-linear-regression" class="level1">
<h1>1.7.7: Statistical Inference in Multiple Linear Regression</h1>
<section id="standard-error-of-ols-estimates" class="level3">
<h3 class="anchored" data-anchor-id="standard-error-of-ols-estimates">Standard Error of OLS Estimates</h3>
<p>In the last section, we calculated the variance of the OLS estimates of <span class="math inline">\hat\beta_1</span>.</p>
<p>However, just like we mentioned in <a href="https://statsnotes.github.io/theory/5.html#standard-errors-of-the-ols-estimator">1.5.6</a>, the real value of <span class="math inline">\sigma^2</span> of a regression not calculable. This is an issue because our variance formula has <span class="math inline">\sigma^2</span> in the numerator.</p>
<p>Just like in simple linear regression (see <a href="https://statsnotes.github.io/theory/5.html#standard-errors-of-the-ols-estimator">1.5.6</a>), we can estimate <span class="math inline">\sigma^2</span> by using the residual term <span class="math inline">\hat u_i</span> with a degrees of freedom adjustment.</p>
<p><span class="math display">
\hat\sigma^2 = \frac{SSR}{n-k-1} = \frac{\sum_{i=1}^n \hat u_i^2}{n-k-1}
</span></p>
<ul>
<li>Where <span class="math inline">n</span> is the number of observations in our sample data.</li>
<li>Where <span class="math inline">k</span> is the number of explanatory variables in our model.</li>
</ul>
<p>Thus, with this estimate, we can calculate the <strong>standard errors</strong> (square root of variance) of our estimates of coefficients <span class="math inline">\hat\beta_j</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Standard Errors for Multiple Linear Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The standard error for the coefficient <span class="math inline">\hat\beta_j</span> from an OLS estimator in multiple linear regression is:</p>
<p><span class="math display">
\widehat{se}(\hat\beta_j) = \frac{\hat\sigma}{\sqrt{\sum_{i=1}^n \widetilde{r_{1i}}^2}}
</span></p>
<ul>
<li>We will never calculate this by hand, we will use a statistical software to do this.</li>
</ul>
<p>Where <span class="math inline">\hat\sigma</span> is defined as:</p>
<p><span class="math display">
\hat\sigma = \sqrt{\frac{SSR}{n-k-1}} = \sqrt{\frac{\sum_{i=1}^n \hat u_i^2}{n-k-1}}
</span></p>
</div>
</div>
<p><br></p>
</section>
<section id="hypothesis-tests" class="level3">
<h3 class="anchored" data-anchor-id="hypothesis-tests">Hypothesis Tests</h3>
<p>With the standard error, we can conduct hypothesis tests.</p>
<ul>
<li>The procedure is identical to simple linear regression, see <a href="https://statsnotes.github.io/theory/5.html#statistical-inference-in-simple-linear-regression">1.5.7</a>.</li>
<li>The intuition of hypothesis testing was outlined in <a href="https://statsnotes.github.io/theory/2.html#intuition-of-hypothesis-testing">1.2.5</a> and <a href="https://statsnotes.github.io/theory/2.html#implementing-a-hypothesis-test">1.2.6</a>.</li>
</ul>
<p>Our hypotheses will typically be:</p>
<p><span class="math display">
\begin{split}
&amp; H_0 : \beta_j = 0 \\
&amp; H_1: \beta_j ≠ 0
\end{split}
</span></p>
<p>Our t-test statistic will be:</p>
<p><span class="math display">
t = \frac{\hat\beta_j - 0}{\widehat{se}(\hat\beta_j)}
</span></p>
<p>And through a t-distribution, we will calculate the p-values.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of p-Values for Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The p-value is the probability of getting a test statistic equally or more extreme than the one we got with our sample estimate <span class="math inline">\hat\beta_j</span>, given the null hypothesis is true.</p>
<ul>
<li><p>If <span class="math inline">p&lt;0.05</span>, we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>), and conclude our alternate hypothesis (that there is a relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>).</p></li>
<li><p>If <span class="math inline">p&gt;0.05</span>, we cannot reject the null hypothesis, and cannot reject that there is no relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how I have been using the word <strong>relationship</strong>, not causal effect.</p>
<p>Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).</p>
</div>
</div>
<p><br></p>
</section>
<section id="confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h3>
<p>We can also create confidence intervals of plausible true <span class="math inline">\beta_j</span> values from the population, given our estimate <span class="math inline">\hat\beta_j</span>. The intuition is the same as discussed in <a href="https://statsnotes.github.io/theory/2.html#confidence-intervals">1.2.7</a>.</p>
<p>Just like previously discussed in <a href="https://statsnotes.github.io/theory/2.html#confidence-intervals">1.2.7</a>, the 95% confidence interval has the bounds:</p>
<p><span class="math display">
\hat\beta_j - 1.96 \widehat{se}(\hat\beta_j), \ \hat\beta_j + 1.96 \widehat{se}(\hat\beta_j)
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Confidence Intervals
</div>
</div>
<div class="callout-body-container callout-body">
<p>The confidence interval means that under repeated sampling and estimating <span class="math inline">\hat\beta_j</span>, 95% of the confidence intervals we construct will include the true <span class="math inline">\beta_j</span> value in the population.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is very important to note that confidence intervals do not mean a 95% probability that the true <span class="math inline">\beta_1</span> is within any specific confidence interval we calculated.</p>
<p>We cannot know based on one confidence interval, whether it covers or does not cover the true <span class="math inline">\beta_j</span>.</p>
<p>The correct interpretation is that over many samples from the same population, we would expect 95% of our confidence intervals to contain the true <span class="math inline">\beta_j</span> value.</p>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>
</section>
<section id="hypothesis-testing-with-more-than-one-coefficient" class="level1">
<h1>1.7.8: Hypothesis Testing with More than One Coefficient</h1>
<p>Sometimes, we want to test more than one coefficient at a time in a hypothesis test.</p>
<ul>
<li>This will be especially obvious why after lesson 1.9.</li>
</ul>
<p>For example, let us say we want to test the statistical significance of <span class="math inline">\hat\beta_2</span> and <span class="math inline">\hat\beta_3</span> at the same time in the following regression model:</p>
<p><span class="math display">
\hat y = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + \hat\beta_3x_3
</span></p>
<p>What we can do is create two models - the alternate model <span class="math inline">M_a</span>, and the null model <span class="math inline">M_0</span>. The alternate model <span class="math inline">M_a</span> is the model we have above, and the null modle <span class="math inline">M_0</span> is the model without the two coefficients that we want to test (<span class="math inline">\hat\beta_2</span> and <span class="math inline">\hat\beta_3</span>).</p>
<p><span class="math display">
\begin{split}
&amp; M_0: y = \beta_0 + \beta_1x_1 \\
&amp; M_a: y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3x_3
\end{split}
</span></p>
<p>Our statistical test will be to test if the alternative model <span class="math inline">M_a</span> is significantly “better” than our null model <span class="math inline">M_0</span>. If <span class="math inline">M_a</span> is indeed significantly better, than we know that the coefficients <span class="math inline">\hat\beta_2</span> and <span class="math inline">\hat\beta_3</span> together are statistically significant.</p>
<p>Let us generalise this framework.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: F-Test of Nested Models
</div>
</div>
<div class="callout-body-container callout-body">
<p>The F-test of Nested Models allows us to test multiple coefficients at once. It compares two models: <span class="math inline">M_0</span> and <span class="math inline">M_a</span>.</p>
<p><span class="math display">
\begin{split}
&amp; M_0: y = \beta_0 + \beta_1x_1 + \dots + \beta_g x_g \\
&amp; M_a: y = \beta_0 + \beta_1 x_1 + \dots + \beta_g x_g + \beta_{g+1} x_{g+1} + \dots + \beta_kx_k
\end{split}
</span></p>
<ul>
<li>The model <span class="math inline">M_a</span> contains all of the explanatory variables, including the ones we want to test.</li>
<li>The model <span class="math inline">M_0</span> contains the other explanatory variables that are not a part of our test. Model <span class="math inline">M_0</span> must be “nested” in model <span class="math inline">M_a</span>: i.e.&nbsp;all explanatory variables present in <span class="math inline">M_0</span> must also be in <span class="math inline">M_a</span>.</li>
</ul>
<p>The model tests if <span class="math inline">M_a</span> is significantly better than <span class="math inline">M_0</span>. If this is the case, the extra coefficients in <span class="math inline">M_a</span> that we are testing are statistically significant.</p>
</div>
</div>
<p><br></p>
<p>How do we run a F-test of nested models?</p>
<p>Recall the concept of <span class="math inline">R^2</span> discussed in <a href="https://statsnotes.github.io/theory/6.html#r-squared-and-goodness-of-fit">1.6.7</a>. <span class="math inline">R^2</span> describes how much of the variation in <span class="math inline">y</span> our explanatory variables explain.</p>
<p>The F-test uses the <span class="math inline">R^2</span> of the two models, and compares them.</p>
<ul>
<li>If the <span class="math inline">M_a</span> model has a statistically significantly higher <span class="math inline">R^2</span> value than the <span class="math inline">M_0</span> model, then <span class="math inline">M_a</span> is considered statistically significant, and we can conclude that the additional explanatory variables in <span class="math inline">M_a</span> are statistically significant.</li>
</ul>
<p><br></p>
<p>As we know from hypothesis testing (see <a href="https://statsnotes.github.io/theory/2.html#intuition-of-hypothesis-testing">1.2.5</a> and <a href="https://statsnotes.github.io/theory/2.html#implementing-a-hypothesis-test">1.2.6</a> for intuition), we need a test statistic and distribution to run a hypothesis test.</p>
<p>The statistic for a F-test is the F-statistic.</p>
<ul>
<li>Let us define <span class="math inline">R^2_a</span> and <span class="math inline">SSR_a</span> as the <span class="math inline">R^2</span> and sum of squared residuals for model <span class="math inline">M_a</span>.</li>
<li>Let us define <span class="math inline">R^2_0</span> and <span class="math inline">SSR_0</span> as the <span class="math inline">R^2</span> and sum of squared residuals for model <span class="math inline">M_0</span>.</li>
<li>The total number of coefficients in model <span class="math inline">M_a</span> is <span class="math inline">k_a</span>, and for model <span class="math inline">M_0</span>, is <span class="math inline">k_0</span>.</li>
<li>Let us define <span class="math inline">n</span> as the number of observations (should be the same for both models).</li>
</ul>
<p>Our F-statistic is mathematically calculated as:</p>
<p><span class="math display">
F = \frac{(SSR_0 - SSE_a)/(k_a - k_0)}{SSR_a / (n - k_a - 1)}
</span></p>
<p>After calculating our F-statistic, we consult an F-distribution with <span class="math inline">k_a - k_0</span> and <span class="math inline">n-k_a - 1</span> degrees of freedom.</p>
<p>With this distribution, we can obtain our p-value.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of p-Values for F-tests
</div>
</div>
<div class="callout-body-container callout-body">
<p>The p-value is the probability of getting a test statistic equally or more extreme than the one we got with our alternate model <span class="math inline">M_a</span>, given the null hypothesis is true.</p>
<ul>
<li><p>If <span class="math inline">p&lt;0.05</span>, we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that <span class="math inline">M_0</span> is a better model), and conclude our alternate hypothesis (that <span class="math inline">M_a</span> is a better model). This also means that the extra coefficients in <span class="math inline">M_a</span> are jointly statistically significant.</p></li>
<li><p>If <span class="math inline">p&gt;0.05</span>, we cannot reject the null hypothesis, and cannot reject that <span class="math inline">M_0</span> is the better model. Thus, the extra coefficients in <span class="math inline">M_a</span> are jointly not statistically significant.</p></li>
</ul>
</div>
</div>
<p><br></p>
<hr>
<p><a href="https://statsnotes.github.io">Homepage</a></p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>