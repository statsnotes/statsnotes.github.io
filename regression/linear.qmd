---
title: "Multiple Linear Regression"
subtitle: "Kevin's Statistical Toolkit"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 3
        toc-location: left
        toc-expand: true
        toc-title: "Table of Contents"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

Use the sidebar to navigate to relevant sections. Additional notes and proofs are provided in the blue collapsible information sections - simply click them to open and close.

<br />

------------------------------------------------------------------------

# When to Use Multiple Linear Regression

Regressions are used to model the relationship between explanatory variables and a outcome variable.

The Multiple Linear Regression Model can be used with any type of explanatory variables. However, it is limited by the possible outcome variables.

The Multiple Linear Regression Model **can** be used when:

::: {.callout-tip collapse="true" icon="false"}
## The Outcome Variable is Continuous

Multiple Linear Regression can always be used when our outcome variable $y$ is continuous, i.e., $y \in (-∞, ∞)$.

There are no other models for such outcome variables.
:::

::: {.callout-tip collapse="true" icon="false"}
## The Outcome Variable is Binary

Multiple Linear Regression can be used when our outcome variable $y$ is binary, i.e., $y \in \{0, 1\}$.

However, Linear Regression can have drawbacks when it is used with binary outcome variables.

-   The main drawback is that linear regression can produce fitted values outside of the boundaries of $[0, 1]$, which makes no sense in terms of interpretation.
-   Linear regression can also have issues with statistical tests (due to the violation of homoscedasticity).

An alternative model to use in the (binary) Logistic Regression Model, which solves these weaknesses of linear regression with binary outcome variables.

-   However, if your only objective is causal inference, predictions outside of $[0,1]$ do not really matter, since you are only concerned with the coefficients, not fitted values. In this case, linear regression is perfectly fine to use for binary outcome variables.
:::

::: {.callout-tip collapse="true" icon="false"}
## The Outcome Variable is Ordinal

Multiple Linear Regression can be used when our outcome variable $y$ is ordinal, i.e., $y \in \{0, 1, \dots, n \}$.

However, Linear Regression can have drawbacks when it is used with ordinal outcome variables.

-   Linear Regression essentially treats ordinal variables as continuous variables. This includes the assumption that the distance between different categories is consistent. This assumption is not always realistic.

An alternative model to use is the Ordinal Logistic Regression Model, which fixes these weaknesses.
:::

::: {.callout-tip collapse="true" icon="false"}
## The Outcome Variable is a Count/Rate

Multiple Linear Regression can be used when our outcome variable $y$ is a count variable, i.e. $y \in \{0, 1, 2, \dots, ∞ \}$. It can also be used for a rate variable (which is a count divided by a factor).

However, Linear Regression can have drawbacks when it is used with count/rate variables.

-   Linear Regression will not produce probabilities for being at each count value, only the expected count value.
-   Linear Regression will also often produce negative outcomes, which make little sense in terms of count/rate outcomes.

An alternative model to use is the Negative Binomial (or Poisson) Regression.
:::

<br />

The Multiple Linear Regression Model should **not** be used when:

::: {.callout-important collapse="true" icon="false"}
## The Outcome Variable is Categorical

Multiple Linear Regression cannot be used when our outcome variable $y$ is categorical, i.e. $y \in \{A, B, \dots \}$ where $A, B, \dots$ have no natural order.

An alternative model to use is the Multinomial Logistic Regression Model.
:::

::: {.callout-important collapse="true" icon="false"}
## The Goal is Causal Interpretation

Regression can be a tool in causal inference, however, we can never claim a causal relationship with a regression alone - we must pair regression with some form of experimental design (see the causal inference toolkit).

Thus, avoid drawing causal conclusions directly from regression.
:::

<br />

<br />

------------------------------------------------------------------------

# Model Specification

Take a set of observed data, with $i=1,\dots,n$ number of observations, each with values $(x_{1i}, \dots, x_{ki}, y_i)$.

The multiple linear regression model takes the following form:

$$
E(y_i|\overrightarrow x_i) = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki}
$$

-   $y$ is the outcome variable, and $x_1, \dots, x_k$ are the explanatory variables.
-   $x_{1i}, \dots, x_{ki}$ are the observed values of the explanatory variables for observation $i$.
-   $E(y_i|\overrightarrow x_i)$ is the expectation of the conditional distribution of $y$ given $\overrightarrow x = (x_1, \dots x_k)$. Or in other words, it is the expected value of $y$ given some values of $x_1, \dots, x_k$.
-   $\beta_0, \dots , \beta_k$ are coefficients that need to be estimated (based on our observed data) in order to produce the best-fit model.

<br />

The above form specifies the model in relation to $E(y_i|\overrightarrow x_i)$, the conditional expectation. However, we can also specify a model in relation to $y_i$, the actual $y$ value for each observation $i$ in our observed data:

$$
y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i
$$

-   $y_i$ is the observed value of the outcome variable $y$ for observation $i$.
-   $x_{1i}, \dots, x_{ki}$ are the observed values of the explanatory variables for observation $i$.
-   $\beta_0, \dots , \beta_k$ are coefficients that need to be estimated (based on our observed data) in order to produce the best-fit model (same as above).
-   $u_i$ is the **error term**. This represents the idea that not all observed values of $y_i$ will be exactly on the expected value of the conditional distribution of $y|x$ - the actual value of $y_i$ will not always be the expected value of $E(y_i)$. The expected value of the error term $u$ should be $E(u) = 0$.

<br />

We can also specify this second model in terms of **linear algebra** as:

$$
y = X \beta + u
$$

-   where vector $y$ is equal to all the values of $y$ for each observation $i = 1, \dots, n$: $y = \begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix}$
-   where matrix $X$ is equal to all the values of each $x_1, \dots, x_k$ for each observation $i = 1, \dots, n$ : $X = \begin{bmatrix}1 & x_{21} & \dots & x_{k1} \\1 & x_{22} & \dots & x_{k2} \\\vdots & \vdots & \vdots & \vdots \\1 & x_{2n} & \dots & x_{kn}\end{bmatrix}$
    -   Where the notation for elements of $X$ is $x_{ki}$, with $i$ being the unit of observation $i = 1, \dots n$, and $k$ being the explanatory variables index.
    -   The first column of $X$ is a vector of 1, which exists because these 1's are multiplied with $\beta_0$ in our model (see below).
-   Where vector $\beta$ is a vector of all coefficients in the model: $\beta = \begin{pmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{pmatrix}$

::: {.callout-note collapse="true"}
## Proof of Representation in Linear Algebra

Start with the linear model written in terms of $y_i$:

$$
y_i = \beta_0 + \beta_1x_{1i} + \dots + \beta_k x_{ki} + u_i
$$

We can rewrite $y_i$ to be equal to:

$$
y_i = x_i'\beta + u_i, \text{ where }\beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{bmatrix} \text{ and }x_i = \begin{bmatrix}1 \\x_{1i} \\\vdots \\x_{ki}\end{bmatrix}
$$

-   The $x_i'$ in the equation is the transpose of $x_i$, to make matrix multiplication possible.
-   The first element of the $x_i$ matrix is 1, since $1 \times \beta_0$ gives us the first parameter (intercept) in the linear model.
-   Thus, when multiplying out, we get the same equation as the original multiple linear regression.

<br />

Note how we have the subscript $i$ representing each individual observation. With a vector, we can expand out these subscripts.

-   For example, instead of $y_i$, we could have a vector with $y_1, y_2, \dots, y_n$ (assuming we have $n$ observations).
-   Same for $x'_i$, which can be expanded into a vector of $x_1', x_2', \dots x_n'$, and for the error term $u_i$, which can be expanded into a vector of $u_1, u_2, \dots, u_n$.

Using this logic, we can obtain the following, with the $x_i'$ and $\beta$ being vectors within a vector:

$$
\begin{split}
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} & = \begin{pmatrix}x_1'\beta + u_1 \\ x_2'\beta + u_2 \\ \vdots \\ x_n'\beta + u_n\end{pmatrix} \\
& \\
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix}& = \begin{pmatrix}x_1'\beta \\ x_2'\beta \\ \vdots \\ x_n'\beta\end{pmatrix} + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
\end{split}
$$

Since $\beta$ vector appears as a common factor for all observations $i=1,...,n$, we can factor it out and have an equation:

$$
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} = \begin{pmatrix}x_1' \\ x_2' \\ \vdots \\ x_n'\end{pmatrix} \space \beta + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
$$

<br />

We can expand the $x_1',...,x_n'$ vector into a matrix. Remember that each $x_1',...,x_n'$ is already a vector of different explanatory variables. So, we get the following result:

$$
y = X \beta + u, \text{ where } X = \begin{bmatrix}1 & x_{21} & \dots & x_{k1} \\1 & x_{22} & \dots & x_{k2} \\\vdots & \vdots & \vdots & \vdots \\1 & x_{2n} & \dots & x_{kn}\end{bmatrix}
$$
:::

<br />

<br />

------------------------------------------------------------------------

# Model Estimation

## Ordinary Least Squares Estimator

We need to estimate coefficients $\beta_0, \dots, \beta_k$ to create a best-fit line (also called **fitted values**) in order to create a model for our observed data:

$$
\widehat{y_i} = \widehat{\beta_0} + \widehat{\beta_1}x_{1i} + \dots + \widehat{\beta_k} x_{ki}
$$

<br />

The most common estimation is to use the **Ordinary Least Squares Estimator**. This estimator estimates $\beta_0, \dots, \beta_k$ by finding the values of $\widehat{\beta_0}. \dots, \widehat{\beta_k}$ that minimise the sum of squared residuals (SSR):

$$
\begin{split}
SSR & = \sum\limits_{i=1}^n (y_i - \widehat{y_i})^2 \\
& = \sum\limits_{i=1}^n (y_i - (\widehat{\beta_0} + \widehat{\beta_1}x_{1i} + \dots + \widehat{\beta_k}x_{ki})) \\
& = \sum\limits_{i=1}^n (y_i - \widehat{\beta_0} - \widehat{\beta_1}x_{1i} - \dots - \widehat{\beta_k} x_{ki})
\end{split}
$$

::: {.callout-note collapse="true"}
## Intuitive Visualisation of SSR

The residuals are the difference from our predicted best-fit line result $\widehat{y_i}$, and the actual value of $y_i$ in the data. Below highlighted in red are the residuals.

![](images/clipboard-846785636.png){fig-align="center" width="70%"}

After we have the residual values, we simply square each of them, then sum all of them together. That is the sum of squared residuals.
:::

::: {.callout-note collapse="true"}
## Why Sum of Squared Residuals

The residuals are squared because we care about the magnitude of errors, not the direction of error. For example, look at this figure:

![](images/clipboard-846785636.png){fig-align="center" width="70%"}

Here, the residuals d1, d3, and dn are positive, but the residuals of d2 and d4 are negative. If we just add them together, the negative and positive residuals would cancel out. But by squaring them, we are measuring the magnitude, not the direction of error.

Then you might ask, why not absolute value them all?

-   First of all, the absolute value function is not differentiable at its vertex, which makes finding a mathematical closed-form solution difficult.
-   As we will also see in the interpretation section, the SSR minimisation condition also has several nice properties that make it better than alternatives.
:::

<br />

This minimisation problem can be solved mathematically. Derivations are provided below:

::: {.callout-note collapse="true"}
## Derivation of OLS for Simple Linear Regression

Let us define the sum of squared residuals as function $S$. We want to minimise $S$, so we have to find the first order conditions and set them equal to 0.

#### First Order Conditions

First, let us find the partial derivative of $S$ in respect to $\hat\beta_0$:

$$
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} = \frac{\partial }{\partial \hat{\beta}_0} \left[ \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \right]
$$

First, ignore the summation. The partial derivative of the internal section, using chain rule, is the following:

$$
\frac{\partial}{\partial \hat{\beta}_0} \left[ (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \right] = -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
$$

We know that there is the sum rule of derivatives $[f(x) + g(x)]' = f'(x) + g'(x)$. Thus, we know we just sum up the individual derivatives to get the derivative of the sum:

$$
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} & = \sum\limits_{-i=1}^n \left[ -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
& = -2 \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
$$

To find the value of $\hat\beta_0$ that minimises $S$, we set the derivative equal to 0. We can ignore the -2, since if the summation is equal to 0, the whole derivative will equal 0. Thus, the first order condition is:

$$
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
$$

<br />

Now, let us do the same for $\hat\beta_1$. Using the same steps as before

$$
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_1} & = \sum\limits_{i=1}^n \left[ -2x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
& = -2 \sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
$$

The first order condition for $\hat\beta_1$ will be (again, ignoring the -2 for the same reason as before):

$$
\sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
$$

Thus, the **first order conditions** of OLS are:

$$
\begin{split}
& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
& \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
$$

<br />

#### Solving the System of Equations

We now have our two first-order conditions. Now, we have a 2-equation system of equations, with 2 variables.

First, let us solve the first equation for $\hat\beta_0$ in terms of $\hat\beta_1$:

$$
\begin{split}
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) & =  0 \\
\sum\limits_{i=1}^n y_i - n \hat{\beta}_0 - \hat{\beta}_1 \sum\limits_{i=1}^n x_i & = 0 \\
-n\hat{\beta}_0 &= -\sum\limits_{i=1}^n y_i + \hat{\beta}_1\sum\limits_{i=1}^nx_i \\
\hat\beta_0 & = \frac{1}{-n} \left( -\sum\limits_{i=1}^n y_i + \hat{\beta}_1\sum\limits_{i=1}^n x_i \right) \\
\hat{\beta}_0 & = \frac{1}{n} \sum\limits_{i=1}^n y_i - \frac{1}{n}\hat{\beta}_1 \sum\limits_{i=1}^n x_i \\
\hat\beta_0& = \bar{y} - \hat{\beta}_1 \bar{x}
\end{split}
$$

Now, let us substitute our calculated $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$ into the $\hat{\beta}_1$ condition and solve for $\hat{\beta}_1$:

$$
\begin{split}
0 & =\sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)  \\
& = \sum\limits_{i=1}^n \left[ x_i(y_i - [\bar{y} - \hat{\beta}_1\bar{x}] - \hat{\beta}_1x_i) \right] \\
& = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y} - \hat{\beta}_1 (x_i - \bar{x})) \right] \\
& = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y}) - x_i \hat{\beta}_1(x_i - \bar{x}) \right] \\
& = \sum\limits_{i=1}^n x_i (y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^nx_i (x_i - \bar{x})
\end{split}
$$

::: {.callout-warning collapse="true"}
## Useful Properties of Summation

Before we finish, here are a few key properties of summation

**Property 1:**

$$
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
$$

-   This is because we can expand the left to $\sum x_i - \sum \bar x$.
-   Then, we know $\sum x_i = \sum \bar x$ (by the formula for mean), so $\sum x_i - \sum \bar x = 0$.

**Property 2:**

$$
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) 
$$

-   This is because on the right side can expand to $\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]$.
-   Then, split into $\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)$.
-   We know that by property 1 (which applies to any variable), $\sum (y_i - \bar y) = 0$. Thus, the right side disappears, and we are left with $\sum x_i (y_i - \bar y)$.

**Property 3:**

$$
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
$$

-   Start by expanding right side to $\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]$
-   Which splits into $\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)$
-   By the first property, we know $\sum x_i - \bar x = 0$, so we are only left with $\sum x_i (x_i - \bar x)$
:::

Knowing these properties of summation, we can transform what we had before:

$$
\begin{split}
0 & = \sum\limits_{i=1}^n x_i (y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^nx_i (x_i - \bar{x}) \\
0 & = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 \\
\hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 & = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) \\
\hat{\beta}_1 & = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
\end{split}
$$

Note that the numerator is equivalent to the formula of covariance $Cov(x,y)$, and the denominator is equal to the variance $Var(x)$.

We found that $\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}$ earlier, so we just plug our solution of $\hat\beta_1$ in to get $\hat\beta_0$.
:::

::: {.callout-note collapse="true"}
## Derivation of OLS for Multiple Linear Regression

Similar to our simple linear regression (but with additional variables), our minimisation condition is:

$$
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...) & = \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} ...)^2 \\
& = \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} S(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)
\end{split}
$$

Taking the partial derivatives of each parameter like in simple linear regression, we get first order conditions:

$$
\begin{split}& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\& \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
& \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
$$

-   and so on for $x_{3i}, ..., x_{ki}$.

This system of equations includes $k+1$ variables and $k+1$ equations, which is way too difficult to solve.

<br />

Instead, we can use linear algebra. Let us define our estimation vector $\hat{\beta}$ as the value of $\hat\beta$ that minimises the sum of squared errors:

$$
\hat{\beta} = \min\limits_{b} (y - Xb)' (y - Xb) = \min\limits_b S(b)
$$

-   $(y - Xb)$ is our error, since $\hat y = Xb$,

We can expand $S(b)$ as follows:

$$
\begin{split}
S(b) & = y'y - b'X'y - y'Xb + b'X'Xb \\ 
& = y'y - 2b'X'y + b'X'Xb
\end{split}
$$

Taking the partial derivative in respect to $b$:

$$
\frac{\partial S(b)}{\partial b} = \begin{pmatrix}\frac{\partial S(b)}{\partial b_1} \\\vdots \\\frac{\partial S(b)}{\partial b_k}\end{pmatrix}
$$

Differentiating with the vector $b$ yields:

$$
\frac{\partial S(b)}{\partial b} = -2X'y + 2X'Xb
$$

Evaluated at $\hat{\beta}$, the derivatives should equal zero (since first order condition of finding minimums):

$$
\frac{\partial S(b)}{\partial b} \biggr|_{\hat{\beta}} = -2X'y + 2X'X \hat{\beta} = 0
$$

When assuming $X'X$ is invertable, we can isolate $\hat{\beta}$ to find the solution to OLS:

$$
\begin{split}
-2X'y + 2X'X \hat{\beta} & = 0 \\
2X'X\hat\beta & = 2X'y \\
\hat\beta & = (2X'X)^{-1} 2 X'y \\
\hat\beta & = (X'X)^{-1}X'y
\end{split}
$$
:::

<br />

## Properties of the OLS Estimator

The OLS Regression has a few useful properties that are outlined below:

::: {.callout-tip collapse="true" icon="false"}
## OLS Estimates for Explanatory Variable Partial Out the Effect of Controls (Regression Anatomy)

The **Regression Anatomy Theorem**, also called the **Frisch-Waugh-Lovell** theorem, shows how multiple linear regression and OLS can be used to "control" for confounding variables.

Essentially, the theorem states that given explanatory variables $x_j= x_1, \dots, x_k$ and an outcome variable $y$, the coefficient $\beta_j$ (of any explanatory variable $x_j$) is the effect of the uncorrelated part of $x_j$ with all other explanatory variables, $\widetilde{r_j}$, on $y$. This means by including control variables in our regression, our $\beta_j$ coefficient will partial out the effect of the controls, and only find the independent effect of $x_j$ on $y$.

::: {.callout-note collapse="true"}
## Proof of Regression Anatomy Theorem

Take our standard multiple linear regression:

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i
$$

Let us say we are interested in $x_1$ (this can be generalised to any explanatory variable $x_j$). Let us make $x_1$ the outcome variable of a regression with explanatory variables $x_2, ..., x_k$:

$$
x_{1i} = \gamma_0 + \gamma_1 x_{2i} + ... + \gamma_{k-1}x_{ki} + \widetilde{r_{1i}}
$$

-   Where $\gamma_0, ..., \gamma_{k-1}$ are coefficients.
-   Where $\widetilde{r_{1i}}$ is the error term.

The error term is $\widetilde{r_{1i}}$, which represents the part of $x_{1i}$ that are uncorrelated to $x_2, ..., x_k$.

-   In other words, $\widetilde{r_{1i}}$ is the part of $x_1$ that cannot be explained by any other explanatory variable $x_2, ..., x_k$. (uncorrelated with them)

<br />

Now, take the regression of with outcome variable $y$, with all explanatory variables [except]{.underline} $x_1$:

$$
y_i = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i}
$$

-   Where $\delta_0, ..., \delta_{k-1}$ are coefficients.
-   Where $\widetilde {y_i}$ is the error term.

The error term is $\widetilde{y_i}$, which is the part of $y_i$ that cannot be explained by $x_2, ..., x_k$ (uncorrelated with them).

<br />

Since $\widetilde{y_i}$ is not explained by $x_2, ..., x_k$, variable $x_1$ must be the one explaining $\widetilde{y_i}$.

-   But, it is not the whole of $x_1$ explaining $\tilde{y_i}$.
-   This is since $x_1$ may also correlated with $x_2, ..., x_k$, and the correlated parts of $x_1$ with $x_2, ..., x_k$ are already picked up in the regression by the coefficients of $x_2, ..., x_k$.

Thus, $\widetilde{y_i}$ must be explained by the part of $x_1$ that is uncorrelated and not explained by $x_2, ..., x_k$, which we derived earlier as $\widetilde{r_{1i}}$.

<br />

Thus, we can create another regression with explanatory variable $\widetilde{x_{1i}}$ and outcome variable $\widetilde{y_i}$.

$$
\widetilde{y_i} = \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i
$$

We can plug $\widetilde{y_i}$ back into our regression of $y_i$ with explanatory variables $x_2 ..., x_k$, and re-arrange:

$$
\begin{split}
y_i & = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i} \\
y_i & = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i \\
y_i  & = (\delta_0 + \alpha_0) + \alpha_1 \widetilde{r_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i 
\end{split}
$$

As we can see, this new regression mirrors the original standard multiple linear regression:

$$
\begin{split}
y_i  & = (\delta_0 + \alpha_0) + \alpha_1 \widetilde{r_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i \\
y_i & = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + u_i
\end{split}
$$

-   The $\beta_0$ in the original is analogous to the $(\delta_0 + \alpha 0)$.
-   The $\beta_1 x_{1i}$ in the original is analogous to $\alpha_1 \widetilde{r_{1i}}$.
-   The $\beta_2 x_{2i} + \dots + \beta_k x_{ki}$ is analogous to $\delta_1 x_{2i} + ... + \delta_{k-1} x_{ki}$.
-   The $u_i$ is in both regressions.

<br />

Importantly we know the $\beta_1 x_{1i}$ in the original is analogous to $\alpha_1 \widetilde{r_{1i}}$. Thus, [the estimate of $\alpha_1$ will be the same as $\beta_1$ in the original regression]{.underline}.

-   The coefficient $\alpha_1$ (which is equal to $\beta_1$) explains the expected change in $y$, given an increase in the part of $x_1$ uncorrelated with $x_2, ..., x_k$.
-   So essentially, [we have **partialed out** the effect of the other explanatory variables, and only focus on the effect on $y$ of the uncorrelated part of $x_1$ (which is $\widetilde{r_{1i}}$)]{.underline}

Or in other words, OLS in multiple linear regression estimates the effect of $\widetilde{r_{1i}}$ on $y$.

-   We can apply this to any explanatory variable $x_1, \dots, x_k$. The uncorrelated parts of any explanatory variable $x_j$ are labelled $\widetilde{r_{ji}}$.
:::

This theorem allows us to essentially "rewrite" the solution of $\widehat{\beta_j}$ in multiple regression as a simple linear regression between $\widetilde{r_j}$ and $y$:

$$
\widehat{\beta_j} = \frac{\sum_{i=1}^n \widetilde{r_{ji}} y_i}{\sum_{i=1}^n(\widetilde{r_{ji}})^2}
$$

This formula is called the Regression Anatomy Formula.

::: {.callout-note collapse="true"}
## Proof of Regression Anatomy Formula

We can express the estimation solution of $\hat\beta_j$ in relation to the regression anatomy formula.

::: {.callout-warning collapse="true"}
## Useful Properties of Summation

Before we start, here are a few key properties of summation

**Property 1:**

$$
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
$$

-   This is because we can expand the left to $\sum x_i - \sum \bar x$.
-   Then, we know $\sum x_i = \sum \bar x$ (by the formula for mean), so $\sum x_i - \sum \bar x = 0$.

**Property 2:**

$$
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) 
$$

-   This is because on the right side can expand to $\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]$.
-   Then, split into $\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)$.
-   We know that by property 1 (which applies to any variable), $\sum (y_i - \bar y) = 0$. Thus, the right side disappears, and we are left with $\sum x_i (y_i - \bar y)$.

**Property 3:**

$$
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
$$

-   Start by expanding right side to $\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]$
-   Which splits into $\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)$
-   By the first property, we know $\sum x_i - \bar x = 0$, so we are only left with $\sum x_i (x_i - \bar x)$
:::

Let us start off with the OLS estimator for simple linear regression, which calculates the $\hat\beta_1$, the relationship between $x$ and $y$ (which we derived previously)

$$
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
$$

Let us look at the numerator. Let us expand the numerator:

$$
\begin{split}
& \sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
= & \sum\limits_{i=1}^n [(x_i - \bar x)y_i - (x_i - \bar x) \bar y] \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \sum\limits_{i=1}^n (x_i - \bar x) \bar y \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
\end{split}
$$

We know that $\sum (x_i - \bar x) = 0$ from the above properties. Thus, we can further simplify to:

$$
\begin{split}
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y(0) \\
= & \sum\limits_{i=1}^n(x_i - \bar x)y_i
\end{split}
$$

Thus, putting the numerator back in, we now we have the equation:

$$
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})y_i}{\sum_{i=1}^n(x_i - \bar{x})^2}
$$

<br />

We know from the Regression Anatomy Theorem, that in multiple linear regression, $\hat\beta_j$ is not the full relationship between $x_j$ and $y$. Instead, it is the relationship of the part of $x_j$ that is uncorrelated with all other explanatory variables, and $y$.

-   So in other words, it is the relationship of $\widetilde{r_{ji}}$ on $y$.

So, since multiple linear regression is the relationship of $\widetilde{r_{ji}}$ on $y$, instead of $x$ on $y$, let us replace the $x$'s in our formula with $\widetilde{r_{ji}}$:

$$
\hat{\beta}_j = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))^2}
$$

We can actually simplify this more with a property of regression - remember, that the error term of a regression $u$, should be such that $E(u)=0$.

We know that $\widetilde{r_{ji}}$ is also the error term of a regression, so, $E(\widetilde{r_{ji}}) = 0$ as well. Plugging that into our equation, we can get the regression anatomy formula for OLS.

$$
\begin{split}
\hat{\beta}_j & = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))^2} \\
\\
& = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - 0)y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - 0)^2} \\
\\
& = \frac{\sum_{i=1}^n \widetilde{r_{ji}} \ y_i}{\sum_{i=1}^n \widetilde {r_{ji}}^2}
\end{split}
$$
:::
:::

::: {.callout-tip collapse="true" icon="false"}
## OLS Best Approximates the Conditional Expectation Function

A **conditional expectation function** says that the value of $E(y)$ depends on the value of $x$. We notate a conditional expectation function as $E(y|x)$. As we noted earlier, the linear regression model can be a conditional expectation function of $E(y|x)$.

A **best linear approximation** of a conditional expectation function, can take the following form:

$$
E(y_i|x_i) = b_0 + b_1x_i
$$

-   Where $b_0$ and $b_1$ are the parameters/coefficients of the model.
-   Where $E(y_i|x_i)$ is the expectation of the conditional distribution $y|x$.
-   Where the conditional distribution $y|x$ has variance $\sigma^2$.

The best linear approximation of the conditional expectation function is defined as the parameters that minimise the mean squared errors (MSE).

$$
\begin{split}
MSE & = E(y_i - E(y_i|x_i))^2 \\
& = \frac{1}{n}\sum\limits_{i=1}^n( y_i - E(y_i|x_i))^2
\end{split}
$$

<br />

Previously, we expressed the linear regression model in both terms of $y_i$, and the conditional expectation $E(y_i|x_i)$. However, when conducting OLS estimation, we have only looked at the $y_i$ form.

The Ordinary Least Squares Regression line also is the best linear approximation of the conditional expectation function $E(y|x)$. That means, we can view a regression as also a best-estimate of a conditional expectation function.

::: {.callout-note collapse="true"}
## Proof that OLS is the Best Approximation of the CEF

What we want to prove is that the OLS estimates $\hat\beta_0$ and $\hat\beta_1$ best estimate the parameters $b_0$ and $b_1$ of the Conditional Expectation Function, which means that if true, OLS is the best linear approximation of the conditional expectation function.

Suppose we have the conditional expectation function:

$$
E(y_i|x_i) = b_0 + b_1x_i
$$

We also know that our typical regression equation is:

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

We know that $E(u_i|x_i) = 0$. Let us define $u_i$ as the following:

$$
u_i = y_i - E(y_i|x_i)
$$

If the above defined $u_i$ is true, $E(u_i|x_i)$ should also be equal to 0. So, let us plug in the above $u_i$ into $E(u_i | x_i)$.

$$
\begin{split}
E(u_i|x_i) & = E(y_i - E(y_i|x_i) \ | \ x_i) \\
& = E(y_i|x_i) - E(y_i|x_i) \\
& = 0
\end{split}
$$

Thus, we know $u_i = y_i - E(y_i|x_i)$ to be true. Thus, rearranging, we know:

$$
y_i = E(y_i|x_i) + u_i
$$

We also know that $y_i = \beta_0 + \beta_1 x_i + u_i$. Thus, the following is true:

$$
\begin{split}
E(y_i|x_i) + u_i & = \beta_0 + \beta_1 + u_i \\
b_0 + b_1 + u_i & = \beta_0 + \beta_1 + u_i \\
b_0 + b_1 & = \beta_0 + \beta_1
\end{split}
$$

Well, you might point out, it is still possible that $b_1 ≠ \beta_1$ in this scenario. We can go further. We know that the conditional expectation function minimises the mean of squared errors.

$$
\begin{split}
MSE & = \min\limits_{b_0, b_1} E(y_i - E(y_i|x_i))^2 \\
& = \min\limits_{b_0, b_1} E(y_i - \beta_0 - \beta_1x_i)^2
\end{split}
$$

The first order conditions are (using chain rule and partial derivatives, just like in the OLS derivation):

$$
\begin{split}
& E(y_i - b_0 - b_1x_i) = 0 \\
& E(x_i(y_i - b_0 - b_1x_i) = 0
\end{split}
$$

<br />

Now, recall our OLS minimisation conditions (from [2.2.3](https://statsnotes.github.io/metrics/2.html#estimation-for-simple-linear-regression))

$$
\begin{split}
& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
& \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
$$

Since by definition, average/expectation is $E(x) = \frac{1}{n} \sum x_i$, we can rewrite the OLS minimisation conditions as:

$$
\begin{split}
& n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
& n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
$$

And since anything multiplied to a zero turns into zero, we can ignore the $n$ in the first order condition, and only focus on the expected value part. Thus, our conditions are:

$$
\begin{split}
& E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
& E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
$$

Which as we can see, are the exact same minimisation conditions as the conditional expectation function. Thus, OLS is the best approximation of the conditional expectation function.
:::
:::

::: {.callout-tip collapse="true"}
## OLS is a Special Case of the Method of Moments Estimator

The Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population **moments** of interest - which are the population parameters written in terms of expected value functions set equal to 0.

Then, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter.

-   For example, to estimate the population mean, the Method of Moments uses the sample mean.

::: {.callout-note collapse="true"}
## Details of the Method of Moments Estimator

In order to define a method of moments for a set of parameters $\theta_1, \dots, \theta_k$, we need to specify at least one population moment per parameter. Or in other words, we must have more than $k$ population moments.

Our population moments can be defined as the expected value of some function $m(\theta; y)$ that consists of both the variable $y$ and our unknown parameter $\theta$. The expectation of the function $m(\theta; y)$ should equal 0.

$$
E(m(\theta; y)) = 0
$$

Our sample moments will be the sample analogues of $\theta$ and $y$, which are $\hat\theta$ and $y_i$:

$$
\frac{1}{n}\sum\limits_{i=1}^n m(\hat\theta; y_i) = 0
$$

-   The $\frac{1}{n} \sum$ is there because the definition of expectation/mean is that.
:::

::: {.callout-note collapse="true" icon="false"}
## Example of a Method of Moments Estimator

Let us say that we have some random variable $y$, with a true population mean $\mu$. We want to estimate $\mu$, but we only have a sample of the population.

How can we define our true population parameter $\mu$ in an expectation equation of the form: $E(m(\mu, y)) = 0$?

-   Well, what is $\mu$, the mean, intuitively speaking? It is the expectation of $y$, so $\mu = E(y)$.

Now that we know that $\mu = E(y)$, since they are equal, $\mu - E(y) = 0$. Thus, we can define the mean as a moment of the following condition:

$$
E(y - \mu) = 0
$$

The method of moments says we should use the sample equivalent of the population parameter. The sample equivalent of $\mu$ (the true mean of the population), is of course, the sample mean $\bar y$.

Thus, our sample estimate of the moment would be:

$$
E(y_i - \hat\mu) = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) = 0
$$

With this equation, we can then solve for $\hat\mu$:

$$
\begin{split}
0 & = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) \\
0 & = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n}\sum\limits_{i=1}^n \hat\mu \\
0 & = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n} n \hat\mu\\
0 & = \bar y - \hat \mu \\
\hat\mu & = \bar y
\end{split}
$$

So, we see the method of moments estimates our true population mean $\mu$, with the sample mean $\bar y$.
:::

OLS is a special case of the Method of Moments Estimator. This is a useful property as we move towards causal inference, as it shows why Endogeneity is an important condition of causal inference (and forms the bases of the Instrumental Variables Estimator).

::: {.callout-note collapse="true" icon="false"}
## Proof that OLS is a Method of Moments Estimator

Consider the bivariate regression model:

$$
y = \beta_0 + \beta_1x + u
$$

The OLS estimator can be derived as a method of moments estimator, with 2 moments (expectation functions set equal to 0), one for each parameter ($\beta_0, \beta_1$):

$$
\begin{split}
& E(y-\beta_0 -\beta_1x) = 0 \\
& E(x(y - \beta_0 - \beta_1 x)) = 0
\end{split}
$$

Since we know $u = y - \beta_0 - \beta_1 x$, we can rewrite the two moments as:

$$
\begin{split}
& E(u) = 0 \\
& E(xu) = 0
\end{split}
$$

The estimates of these moments would use the sample equivalents: $\hat\beta_0$ and $\hat\beta_1$.

$$
\begin{split}
& E(y-\hat\beta_0 -\hat\beta_1x) = 0 \\
& E(x(y - \hat\beta_0 - \hat\beta_1 x)) = 0
\end{split}
$$

<br />

Let us prove that OLS is a special case of the method of moments estimator. Remember our OLS minimisation conditions:

$$
\begin{split}
& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
& \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
$$

Since by definition, average/expectation is $E(x) = \frac{1}{n} \sum x_i$, we can rewrite the OLS minimisation conditions as:

$$
\begin{split}
& n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
& n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
$$

And since anything multiplied to a zero turns into zero, we can ignore the $n$ in the first order condition, and only focus on the expected value part. Thus, our conditions are:

$$
\begin{split}
& E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
& E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
$$

Which as we can see, are the exact same minimisation conditions as the method of moments estimator. Thus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients.
:::
:::

::: {.callout-tip collapse="true" icon="false"}
## OLS Residuals Sum to 0

The residuals of OLS are the errors (but this time, not squared):

$$
\begin{split}\hat u_i & = y_i - \hat y_i \\& = y_i - (\hat \beta_0 + \hat\beta_1x_{1i} + \dots + \hat\beta_k x_{ki}) \\& = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}\end{split}
$$

OLS Residuals sum to 0 - i.e. the average true value of $y$ and the average predicted value of $\hat y$ are equal.

::: {.callout-note collapse="true"}
## Proof OLS Residuals Sum to 0

Recall our OLS first order conditions:

$$
\begin{split}& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\& \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
& \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
$$

-   and so on for $x_{3i}, ..., x_{ki}$.

We know residuals $\hat u_i = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}$.

If we plug that definition of $\hat u_i$ into the first minimisation condition, we get:

$$
\sum\limits_{i=1}^n \hat u_i = 0
$$
:::
:::

::: {.callout-tip collapse="true" icon="false"}
## No Covariance Between Any Explanatory Variable and OLS Residuals

In OLS estimation, any explanatory variable $x_1, \dots, x_k$ is uncorrelated with the OLS residuals $\hat u_i = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}$.

::: {.callout-note collapse="true"}
## Proof of No Covariance

Recall our OLS first order conditions:

$$
\begin{split}& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\& \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
& \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
$$

-   and so on for $x_{3i}, ..., x_{ki}$.

We know residuals $\hat u_i = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}$.

-   Thus, plugging into the first condition, $\sum\limits_{i=1}^n \hat u_i = 0$.
-   Thus, plugging into the other conditions, $\sum\limits_{i=1}^n x_j \hat u_i = 0$.

For simplicity, we will use simple linear regression, but the same applies to multiple linear regression.

Now, recall the formula for covariance discussed in [1.3.4](https://statsnotes.github.io/theory/3.html#quantifying-relationships-with-covariance):

$$
Cov(x,y) = \frac{1}{n}\sum\limits_{i=1}^n [(x_i - \bar x)(y_i - \bar y)] \\
$$

Thus, the covariance between $x_j$ (for notation simplicity, just $x$) and $\hat u$ is:

$$
\begin{split}
Cov(x, \hat u) & = \frac{1}{n}\sum\limits_{i=1}^n [(x_i - \bar x)(\hat u_i - \bar{\hat u})] \\
& = \frac{1}{n}\sum\limits_{i=1}^n(x_i \hat u_i - x_i \bar{\hat u} - \bar x \hat u_i + \bar x \bar {\hat u}) \\
& = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - \sum\limits_{i=1}^n x_i\bar{\hat u} - \sum\limits_{i=1}^n \bar x \hat u_i + \sum\limits_{i=1}^n\bar x \bar{\hat u} \right) \\
& = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - \bar{\hat u}\sum\limits_{i=1}^n x_i - \bar x\sum\limits_{i=1}^n \hat u_i + \bar{\hat u}\sum\limits_{i=1}^n\bar x  \right) \\
& = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - 0\sum\limits_{i=1}^n x_i - \bar x\sum\limits_{i=1}^n \hat u_i + 0\sum\limits_{i=1}^n\bar x  \right) \\
& = \frac{1}{n}(0 -0-\bar x(0) + 0) \\
& = 0
\end{split}
$$

Thus, the covariance (and thus correlation) between $x_j$ and $\hat u$ must be zero.
:::
:::

::: {.callout-tip collapse="true" icon="false"}
## OLS Best-Fit Line Passes Through Means

The OLS estimated coefficients produces a best-fit line, that always passes through the point $(\bar x_1, \dots, \bar x_k, \bar y)$, which is the point of the means of all the variables.

For simplicity, take simple linear regression. Remember our solution for $\hat\beta_0$ in OLS for simple linear regression was $\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}$. Rearranging this equation, we get:

$$
\begin{split}
\hat\beta_0  & = \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat\beta_0 + \hat\beta_1 \bar x & = \bar y \\
\bar y & = \hat\beta_0 + \hat\beta_1 \bar x
\end{split}
$$

Thus, the OLS estimated best-fit line always passes though point $(\bar x, \bar y)$ (the means of our data). The same property applies to multiple linear regression, but for point $(\bar x_1, \dots, \bar x_k, \bar y)$.
:::

<br />

<br />

------------------------------------------------------------------------

# Interpretation

## Interpretation of Coefficients

Once we have estimated our coefficients model, we will have the following fitted-values model.

$$
\widehat{y_i} = E(y_i|x_i)= \widehat{\beta_0} + \widehat{\beta_1}x_{1i} + \dots + \widehat{\beta_k} x_{ki}
$$

The coefficient $\widehat{\beta_0}$, also called the intercept, is the expected value of $y$ given all explanatory variables $x_1, \dots, x_k = 0$.

<br />

The coefficient $\widehat{\beta_j} = \widehat{\beta_1}, \dots, \widehat{\beta_k}$ has an interpretation that depends on the type of variables $y$ and $x_j. =x_1, \dots, x_k$ are:

::: {.callout-tip collapse="true" icon="false"}
## $\widehat{\beta_j}$ for Continuous $y$, Continuous $x_j$

For every one unit increase in $x_j$, there is an expected $\widehat{\beta_j}$ unit change in $y$.

We can also standardise our interpretations in respect to standard deviations: For a one standard deviation increase in $x_j$, there is an expected $\widehat{\beta_1}\sigma_x/\sigma_y$ standard deviation change in $y$.
:::

::: {.callout-tip collapse="true" icon="false"}
## $\widehat{\beta_j}$ for Continuous $y$, Binary $x_j$

There is an expected $\widehat{\beta_j}$ unit differnce in $y$ between categories $x_j = 1$ and $x_j = 0$.

We can also calculate the expected $y$ for each category of $x_j$:

-   The expected $y$ of category $x_j = 0$ is $\widehat{\beta_0}$.
-   The expected $y$ of category $x_j = 1$ is $\widehat{\beta_0} + \widehat{\beta_1}$.
:::

::: {.callout-tip collapse="true" icon="false"}
## $\widehat{\beta_j}$ for Continuous $y$, Continuous $x_j$

For every one unit increase in $x_j$, there is an expected $\widehat{\beta_j} \times 100$ percentage point change in the chance of a unit being in category $y=1$.
:::

::: {.callout-tip collapse="true" icon="false"}
## $\widehat{\beta_j}$ for Continuous $y$, Continuous $x_j$

There is an expected $\widehat{\beta_j}$ percentage point difference in the chance of being in category $y=1$ between category $x_j = 1$ and category $x_j = 0$.

We can also calculate the expected percentage of units in category $y=1$ for each category of $x_j$:

-   The percentage of units in category $y=1$ for category $x_j = 0$ is $\widehat{\beta_0}$.
-   The percentage of units in category $y=1$ for category $x_j = 1$ is $\widehat{\beta_0} + \widehat{\beta_1}$.
:::

<br />

## Model Summary Statistics

The **Residual Standard Deviation** $\hat\sigma$ is the standard deviation of the residuals (also, the standard deviation of the conditional expectation distribution).

The formula for Residual Variance is given below (square root for standard deviation):

$$
\hat\sigma^2 = \frac{\sum_{i=1}^n (y_i -\hat y_i)^2}{n-k-1}
$$

-   Larger values mean the observed values of $y$ are more widely scattered around $E(y|x)$.
-   Smaller values mean the observed values of $y$ are more tightly concentrated around $E(y|x)$.

![](images/clipboard-2778593669.png){fig-align="center" width="90%"}

<br />

The **R-squared** Statistic, $R^2$, is a measure of the fit of our best-fit line.

-   Interpreted as: The proportion of the total variance in $y_i$ that is accounted for (or explained) by the explanatory variables in the model.
-   $R^2$ is always between 0 and 1 (or 0% and 100%).

The formula for $R^2$ is:

$$
R^2 = \frac{\sum_{i=1}^n(\widehat{y_i} - \bar y)^2}{\sum_{i=1}^n (y_i - \bar y)^2}
$$

-   The numerator is also called the explained sum of squares (SSE), the variation in $y$ that is explained by our model.
-   The denominator is called the total sum of squares (SST), the total variation in $y$.

::: {.callout-note collapse="true"}
## Derivation of R-Squared

For each observation, we know that the actual $y_i$ value is the predicted $\hat y_i$ plus the residual term $\hat u_i$. Thus:

$$
y_i = \hat y_i + \hat u_i
$$

Now, let us define these three concepts: the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR):

$$
\begin{split}
& SST = \sum\limits_{i=1}^n (y_i - \bar y)^2 \\
& SSE = \sum\limits_{i=1}^n (\hat y_i - \bar y)^2 \\
& SSR = \sum\limits_{i=1}^n (\hat u_i)^2
\end{split}
$$

-   The SST explains the total amount of variation in $y$
-   The SSE is the amount of variation in $y$ explained by our model
-   The SSR is the amount of variation in $y$ not explained by our model

Let us look at the total sum of squares (SST). We can manipulate it as follows:

$$
\begin{split}
SST & = \sum\limits_{i=1}^n (y_i - \bar y)^2 \\
& = \sum\limits_{i=1}^n(y_i - \hat y_i+ \hat y_i - \bar y)^2 \\
& = \sum\limits_{i=1}^n((y_i - \hat y_i)+ \hat y_i - \bar y)^2 \\
& = \sum\limits_{i=1}^n[\hat u_i + \hat y_i - \bar y]^2 \\
& = \sum\limits_{i=1}^n[\hat u_i^2 + \hat u_i \hat y_i - \hat u_i \bar y + \hat y_i \hat u_i + \hat y_i^2 - \hat y_i \bar y-\bar y \hat u_i -\bar y \hat  y_i+\hat y^2_i] \\
& = \sum\limits_{i=1}^n[ \hat u_i^2 + 2 \hat u_i \hat y_i+ \hat y_i^2 - 2 \hat u_i \bar y - 2 \hat y_i \bar y + \bar y ^2]
\end{split}
$$

By a property of linear regression, $\sum \hat y_i \hat u_i = 0$. Knowing this, we can further simplify to:

$$
\begin{split}
SST & = \sum\limits_{i=1}^n[ \hat u_i^2 + \hat y_i^2 - 2 \hat u_i \bar y - 2 \hat y_i \bar y + \bar y ^2] \\
& = \sum\limits_{i=1}^n[\hat u_i^2 + (\hat y_i - \bar y)^2]\\
& = \sum\limits_{i=1}^n \hat u_i^2 + \sum\limits_{i=1}^n(\hat y_i - \bar y)^2 \\
& = SSE + SSR
\end{split}
$$

This makes sense: After all, SSE is the squared errors explained by the model, and SSR is the residual (non-explained) parts of the model, so together, they should be equal to the total sum of squares.

Thus, SSE/SST should be a percentage between 0 and 100, and 1 - SSR/SST should also be equivalent to that.
:::

<br />

<br />

------------------------------------------------------------------------

# Assumptions

## Assumptions for Unbiasedness

Unbiasedness means the expected value of our estimate from our population is equal to the true population value. In other words, $E(\hat\theta) = \theta$.

The OLS estimate of $\widehat{\beta_j}$ (the relationship between $x_j$ and $y$) is only unbiased if these 4 conditions are met:

::: {.callout-tip collapse="true" icon="false"}
## MLR.1 Linearity in Parameters

A model must be linear in parameters for unbiasedness. This means that the parameters of the model $\beta_0, \dots, \beta_k$ must not be multiplied/divided together.

Note: this does not mean the actual regression line must be linear - only the parameters/coefficients must not be multiplied.

For example, the following model is still linear in parameters:

$$
y = \beta_0 + \beta_1x^2
$$
:::

::: {.callout-tip collapse="true" icon="false"}
## MLR.2 Random Sampling

This assumption says that all observations in our sample are randomly sampled from the same population.

The error term $u$ in our regression model is some random variable (with its own probability distribution), that can be defined by its expectation $E(u)$.

-   If we randomly select one observation $i$ from the data, each observation $i$ has an equal chance of being selected.
-   The error term for that observation, $u_i$, should also have the same expectation as the random variable $u$, since each observation $i$ within $u$ has the same chance of being selected.

Thus, random sampling allows us to say $E(u) = E(u_i)$.
:::

::: {.callout-tip collapse="true" icon="false"}
## MLR.3 No Perfect Multicollinearity/Variance in $x$

No perfect multicollinearity (MLR.3) means that no two explanatory variables are perfectly correlated together. This also means that in the regression anatomy formula, $\sum \widetilde{r_{ji}}^2 ≠ 0$.

If we have a simple linear regression, we instead must make sure that there is indeed variance in the explanatory variable $x$ (SLR.3). Or in other words: $Var(x) ≠ 0$.

**These two assumptions are necessary to even calculate** $\widehat{\beta_0}$, since the formulas require dividing by these values, so if they equal 0, we cannot calculate OLS.
:::

::: {.callout-tip collapse="true" icon="false"}
## MLR.4 Zero-Conditional Mean

In the population, the error term $u$ must have an expectation of 0, given all values of $\overrightarrow x$.

Mathematically:

$$
E(u|x_1, \dots, x_k) = 0, \ \forall (x_1, \dots, x_k)
$$

When combined with MLR.2 Random Sampling from above, we can say that:

$$
E(u|x_1, \dots, x_k) = E(u_i|x_{1i}, \dots, x_{ki}) = 0
$$

This is the [key assumption]{.underline} of OLS that is most frequently violated.
:::

If all 4 conditions are met, The OLS estimate of $\widehat{\beta_j}$ (the relationship between $x_j$ and $y$) is unbiased.

::: {.callout-note collapse="true"}
## Proof of Unbiasedness for Simple Linear Regression
:::

::: {.callout-note collapse="true"}
## Proof of Unbiasedness for Multiple Linear Regression
:::

<br />

## Assumptions for Consistency

<br />

<br />

------------------------------------------------------------------------

# Statistical Inference

<br />

<br />

------------------------------------------------------------------------

# Extension: Explanatory Variables

## Categorical Explanatory Variables

<br />

## Fixed Effects

<br />

## Polynomial Transformations

<br />

## Logarithmic Transformations

<br />

## Interaction Effects

<br />

<br />

------------------------------------------------------------------------

# **Implementation in R**

<br />
