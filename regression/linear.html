<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Multiple Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="linear_files/libs/clipboard/clipboard.min.js"></script>
<script src="linear_files/libs/quarto-html/quarto.js"></script>
<script src="linear_files/libs/quarto-html/popper.min.js"></script>
<script src="linear_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="linear_files/libs/quarto-html/anchor.min.js"></script>
<link href="linear_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="linear_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="linear_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="linear_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="linear_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Multiple Linear Regression</h1>
            <p class="subtitle lead">Kevin’s Statistical Toolkit</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#when-to-use-multiple-linear-regression" id="toc-when-to-use-multiple-linear-regression" class="nav-link active" data-scroll-target="#when-to-use-multiple-linear-regression">When to Use Multiple Linear Regression</a></li>
  <li><a href="#model-specification" id="toc-model-specification" class="nav-link" data-scroll-target="#model-specification">Model Specification</a></li>
  <li><a href="#model-estimation" id="toc-model-estimation" class="nav-link" data-scroll-target="#model-estimation">Model Estimation</a>
  <ul class="collapse">
  <li><a href="#ordinary-least-squares-estimator" id="toc-ordinary-least-squares-estimator" class="nav-link" data-scroll-target="#ordinary-least-squares-estimator">Ordinary Least Squares Estimator</a></li>
  <li><a href="#properties-of-the-ols-estimator" id="toc-properties-of-the-ols-estimator" class="nav-link" data-scroll-target="#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
  </ul></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation">Interpretation</a>
  <ul class="collapse">
  <li><a href="#interpretation-of-coefficients" id="toc-interpretation-of-coefficients" class="nav-link" data-scroll-target="#interpretation-of-coefficients">Interpretation of Coefficients</a></li>
  <li><a href="#model-summary-statistics" id="toc-model-summary-statistics" class="nav-link" data-scroll-target="#model-summary-statistics">Model Summary Statistics</a></li>
  </ul></li>
  <li><a href="#assumptions" id="toc-assumptions" class="nav-link" data-scroll-target="#assumptions">Assumptions</a>
  <ul class="collapse">
  <li><a href="#assumptions-for-unbiasedness" id="toc-assumptions-for-unbiasedness" class="nav-link" data-scroll-target="#assumptions-for-unbiasedness">Assumptions for Unbiasedness</a></li>
  <li><a href="#assumptions-for-consistency" id="toc-assumptions-for-consistency" class="nav-link" data-scroll-target="#assumptions-for-consistency">Assumptions for Consistency</a></li>
  </ul></li>
  <li><a href="#statistical-inference" id="toc-statistical-inference" class="nav-link" data-scroll-target="#statistical-inference">Statistical Inference</a></li>
  <li><a href="#extension-explanatory-variables" id="toc-extension-explanatory-variables" class="nav-link" data-scroll-target="#extension-explanatory-variables">Extension: Explanatory Variables</a>
  <ul class="collapse">
  <li><a href="#categorical-explanatory-variables" id="toc-categorical-explanatory-variables" class="nav-link" data-scroll-target="#categorical-explanatory-variables">Categorical Explanatory Variables</a></li>
  <li><a href="#fixed-effects" id="toc-fixed-effects" class="nav-link" data-scroll-target="#fixed-effects">Fixed Effects</a></li>
  <li><a href="#polynomial-transformations" id="toc-polynomial-transformations" class="nav-link" data-scroll-target="#polynomial-transformations">Polynomial Transformations</a></li>
  <li><a href="#logarithmic-transformations" id="toc-logarithmic-transformations" class="nav-link" data-scroll-target="#logarithmic-transformations">Logarithmic Transformations</a></li>
  <li><a href="#interaction-effects" id="toc-interaction-effects" class="nav-link" data-scroll-target="#interaction-effects">Interaction Effects</a></li>
  </ul></li>
  <li><a href="#implementation-in-r" id="toc-implementation-in-r" class="nav-link" data-scroll-target="#implementation-in-r"><strong>Implementation in R</strong></a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Use the sidebar to navigate to relevant sections. Additional notes and proofs are provided in the blue collapsible information sections - simply click them to open and close.</p>
<p><br></p>
<hr>
<section id="when-to-use-multiple-linear-regression" class="level1">
<h1>When to Use Multiple Linear Regression</h1>
<p>Regressions are used to model the relationship between explanatory variables and a outcome variable.</p>
<p>The Multiple Linear Regression Model can be used with any type of explanatory variables. However, it is limited by the possible outcome variables.</p>
<p>The Multiple Linear Regression Model <strong>can</strong> be used when:</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Outcome Variable is Continuous
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Multiple Linear Regression can always be used when our outcome variable <span class="math inline">y</span> is continuous, i.e., <span class="math inline">y \in (-∞, ∞)</span>.</p>
<p>There are no other models for such outcome variables.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Outcome Variable is Binary
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Multiple Linear Regression can be used when our outcome variable <span class="math inline">y</span> is binary, i.e., <span class="math inline">y \in \{0, 1\}</span>.</p>
<p>However, Linear Regression can have drawbacks when it is used with binary outcome variables.</p>
<ul>
<li>The main drawback is that linear regression can produce fitted values outside of the boundaries of <span class="math inline">[0, 1]</span>, which makes no sense in terms of interpretation.</li>
<li>Linear regression can also have issues with statistical tests (due to the violation of homoscedasticity).</li>
</ul>
<p>An alternative model to use in the (binary) Logistic Regression Model, which solves these weaknesses of linear regression with binary outcome variables.</p>
<ul>
<li>However, if your only objective is causal inference, predictions outside of <span class="math inline">[0,1]</span> do not really matter, since you are only concerned with the coefficients, not fitted values. In this case, linear regression is perfectly fine to use for binary outcome variables.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Outcome Variable is Ordinal
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Multiple Linear Regression can be used when our outcome variable <span class="math inline">y</span> is ordinal, i.e., <span class="math inline">y \in \{0, 1, \dots, n \}</span>.</p>
<p>However, Linear Regression can have drawbacks when it is used with ordinal outcome variables.</p>
<ul>
<li>Linear Regression essentially treats ordinal variables as continuous variables. This includes the assumption that the distance between different categories is consistent. This assumption is not always realistic.</li>
</ul>
<p>An alternative model to use is the Ordinal Logistic Regression Model, which fixes these weaknesses.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Outcome Variable is a Count/Rate
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Multiple Linear Regression can be used when our outcome variable <span class="math inline">y</span> is a count variable, i.e.&nbsp;<span class="math inline">y \in \{0, 1, 2, \dots, ∞ \}</span>. It can also be used for a rate variable (which is a count divided by a factor).</p>
<p>However, Linear Regression can have drawbacks when it is used with count/rate variables.</p>
<ul>
<li>Linear Regression will not produce probabilities for being at each count value, only the expected count value.</li>
<li>Linear Regression will also often produce negative outcomes, which make little sense in terms of count/rate outcomes.</li>
</ul>
<p>An alternative model to use is the Negative Binomial (or Poisson) Regression.</p>
</div>
</div>
</div>
<p><br></p>
<p>The Multiple Linear Regression Model should <strong>not</strong> be used when:</p>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Outcome Variable is Categorical
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Multiple Linear Regression cannot be used when our outcome variable <span class="math inline">y</span> is categorical, i.e.&nbsp;<span class="math inline">y \in \{A, B, \dots \}</span> where <span class="math inline">A, B, \dots</span> have no natural order.</p>
<p>An alternative model to use is the Multinomial Logistic Regression Model.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Goal is Causal Interpretation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Regression can be a tool in causal inference, however, we can never claim a causal relationship with a regression alone - we must pair regression with some form of experimental design (see the causal inference toolkit).</p>
<p>Thus, avoid drawing causal conclusions directly from regression.</p>
</div>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
</section>
<section id="model-specification" class="level1">
<h1>Model Specification</h1>
<p>Take a set of observed data, with <span class="math inline">i=1,\dots,n</span> number of observations, each with values <span class="math inline">(x_{1i}, \dots, x_{ki}, y_i)</span>.</p>
<p>The multiple linear regression model takes the following form:</p>
<p><span class="math display">
E(y_i|\overrightarrow x_i) = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki}
</span></p>
<ul>
<li><span class="math inline">y</span> is the outcome variable, and <span class="math inline">x_1, \dots, x_k</span> are the explanatory variables.</li>
<li><span class="math inline">x_{1i}, \dots, x_{ki}</span> are the observed values of the explanatory variables for observation <span class="math inline">i</span>.</li>
<li><span class="math inline">E(y_i|\overrightarrow x_i)</span> is the expectation of the conditional distribution of <span class="math inline">y</span> given <span class="math inline">\overrightarrow x = (x_1, \dots x_k)</span>. Or in other words, it is the expected value of <span class="math inline">y</span> given some values of <span class="math inline">x_1, \dots, x_k</span>.</li>
<li><span class="math inline">\beta_0, \dots , \beta_k</span> are coefficients that need to be estimated (based on our observed data) in order to produce the best-fit model.</li>
</ul>
<p><br></p>
<p>The above form specifies the model in relation to <span class="math inline">E(y_i|\overrightarrow x_i)</span>, the conditional expectation. However, we can also specify a model in relation to <span class="math inline">y_i</span>, the actual <span class="math inline">y</span> value for each observation <span class="math inline">i</span> in our observed data:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i
</span></p>
<ul>
<li><span class="math inline">y_i</span> is the observed value of the outcome variable <span class="math inline">y</span> for observation <span class="math inline">i</span>.</li>
<li><span class="math inline">x_{1i}, \dots, x_{ki}</span> are the observed values of the explanatory variables for observation <span class="math inline">i</span>.</li>
<li><span class="math inline">\beta_0, \dots , \beta_k</span> are coefficients that need to be estimated (based on our observed data) in order to produce the best-fit model (same as above).</li>
<li><span class="math inline">u_i</span> is the <strong>error term</strong>. This represents the idea that not all observed values of <span class="math inline">y_i</span> will be exactly on the expected value of the conditional distribution of <span class="math inline">y|x</span> - the actual value of <span class="math inline">y_i</span> will not always be the expected value of <span class="math inline">E(y_i)</span>. The expected value of the error term <span class="math inline">u</span> should be <span class="math inline">E(u) = 0</span>.</li>
</ul>
<p><br></p>
<p>We can also specify this second model in terms of <strong>linear algebra</strong> as:</p>
<p><span class="math display">
y = X \beta + u
</span></p>
<ul>
<li>where vector <span class="math inline">y</span> is equal to all the values of <span class="math inline">y</span> for each observation <span class="math inline">i = 1, \dots, n</span>: <span class="math inline">y = \begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix}</span></li>
<li>where matrix <span class="math inline">X</span> is equal to all the values of each <span class="math inline">x_1, \dots, x_k</span> for each observation <span class="math inline">i = 1, \dots, n</span> : <span class="math inline">X = \begin{bmatrix}1 &amp; x_{21} &amp; \dots &amp; x_{k1} \\1 &amp; x_{22} &amp; \dots &amp; x_{k2} \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\1 &amp; x_{2n} &amp; \dots &amp; x_{kn}\end{bmatrix}</span>
<ul>
<li>Where the notation for elements of <span class="math inline">X</span> is <span class="math inline">x_{ki}</span>, with <span class="math inline">i</span> being the unit of observation <span class="math inline">i = 1, \dots n</span>, and <span class="math inline">k</span> being the explanatory variables index.</li>
<li>The first column of <span class="math inline">X</span> is a vector of 1, which exists because these 1’s are multiplied with <span class="math inline">\beta_0</span> in our model (see below).</li>
</ul></li>
<li>Where vector <span class="math inline">\beta</span> is a vector of all coefficients in the model: <span class="math inline">\beta = \begin{pmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{pmatrix}</span></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of Representation in Linear Algebra
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Start with the linear model written in terms of <span class="math inline">y_i</span>:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + \dots + \beta_k x_{ki} + u_i
</span></p>
<p>We can rewrite <span class="math inline">y_i</span> to be equal to:</p>
<p><span class="math display">
y_i = x_i'\beta + u_i, \text{ where }\beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{bmatrix} \text{ and }x_i = \begin{bmatrix}1 \\x_{1i} \\\vdots \\x_{ki}\end{bmatrix}
</span></p>
<ul>
<li>The <span class="math inline">x_i'</span> in the equation is the transpose of <span class="math inline">x_i</span>, to make matrix multiplication possible.</li>
<li>The first element of the <span class="math inline">x_i</span> matrix is 1, since <span class="math inline">1 \times \beta_0</span> gives us the first parameter (intercept) in the linear model.</li>
<li>Thus, when multiplying out, we get the same equation as the original multiple linear regression.</li>
</ul>
<p><br></p>
<p>Note how we have the subscript <span class="math inline">i</span> representing each individual observation. With a vector, we can expand out these subscripts.</p>
<ul>
<li>For example, instead of <span class="math inline">y_i</span>, we could have a vector with <span class="math inline">y_1, y_2, \dots, y_n</span> (assuming we have <span class="math inline">n</span> observations).</li>
<li>Same for <span class="math inline">x'_i</span>, which can be expanded into a vector of <span class="math inline">x_1', x_2', \dots x_n'</span>, and for the error term <span class="math inline">u_i</span>, which can be expanded into a vector of <span class="math inline">u_1, u_2, \dots, u_n</span>.</li>
</ul>
<p>Using this logic, we can obtain the following, with the <span class="math inline">x_i'</span> and <span class="math inline">\beta</span> being vectors within a vector:</p>
<p><span class="math display">
\begin{split}
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} &amp; = \begin{pmatrix}x_1'\beta + u_1 \\ x_2'\beta + u_2 \\ \vdots \\ x_n'\beta + u_n\end{pmatrix} \\
&amp; \\
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix}&amp; = \begin{pmatrix}x_1'\beta \\ x_2'\beta \\ \vdots \\ x_n'\beta\end{pmatrix} + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
\end{split}
</span></p>
<p>Since <span class="math inline">\beta</span> vector appears as a common factor for all observations <span class="math inline">i=1,...,n</span>, we can factor it out and have an equation:</p>
<p><span class="math display">
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} = \begin{pmatrix}x_1' \\ x_2' \\ \vdots \\ x_n'\end{pmatrix} \space \beta + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
</span></p>
<p><br></p>
<p>We can expand the <span class="math inline">x_1',...,x_n'</span> vector into a matrix. Remember that each <span class="math inline">x_1',...,x_n'</span> is already a vector of different explanatory variables. So, we get the following result:</p>
<p><span class="math display">
y = X \beta + u, \text{ where } X = \begin{bmatrix}1 &amp; x_{21} &amp; \dots &amp; x_{k1} \\1 &amp; x_{22} &amp; \dots &amp; x_{k2} \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\1 &amp; x_{2n} &amp; \dots &amp; x_{kn}\end{bmatrix}
</span></p>
</div>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
</section>
<section id="model-estimation" class="level1">
<h1>Model Estimation</h1>
<section id="ordinary-least-squares-estimator" class="level2">
<h2 class="anchored" data-anchor-id="ordinary-least-squares-estimator">Ordinary Least Squares Estimator</h2>
<p>We need to estimate coefficients <span class="math inline">\beta_0, \dots, \beta_k</span> to create a best-fit line (also called <strong>fitted values</strong>) in order to create a model for our observed data:</p>
<p><span class="math display">
\widehat{y_i} = \widehat{\beta_0} + \widehat{\beta_1}x_{1i} + \dots + \widehat{\beta_k} x_{ki}
</span></p>
<p><br></p>
<p>The most common estimation is to use the <strong>Ordinary Least Squares Estimator</strong>. This estimator estimates <span class="math inline">\beta_0, \dots, \beta_k</span> by finding the values of <span class="math inline">\widehat{\beta_0}. \dots, \widehat{\beta_k}</span> that minimise the sum of squared residuals (SSR):</p>
<p><span class="math display">
\begin{split}
SSR &amp; = \sum\limits_{i=1}^n (y_i - \widehat{y_i})^2 \\
&amp; = \sum\limits_{i=1}^n (y_i - (\widehat{\beta_0} + \widehat{\beta_1}x_{1i} + \dots + \widehat{\beta_k}x_{ki})) \\
&amp; = \sum\limits_{i=1}^n (y_i - \widehat{\beta_0} - \widehat{\beta_1}x_{1i} - \dots - \widehat{\beta_k} x_{ki})
\end{split}
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuitive Visualisation of SSR
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The residuals are the difference from our predicted best-fit line result <span class="math inline">\widehat{y_i}</span>, and the actual value of <span class="math inline">y_i</span> in the data. Below highlighted in red are the residuals.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-846785636.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>After we have the residual values, we simply square each of them, then sum all of them together. That is the sum of squared residuals.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why Sum of Squared Residuals
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The residuals are squared because we care about the magnitude of errors, not the direction of error. For example, look at this figure:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-846785636.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>Here, the residuals d1, d3, and dn are positive, but the residuals of d2 and d4 are negative. If we just add them together, the negative and positive residuals would cancel out. But by squaring them, we are measuring the magnitude, not the direction of error.</p>
<p>Then you might ask, why not absolute value them all?</p>
<ul>
<li>First of all, the absolute value function is not differentiable at its vertex, which makes finding a mathematical closed-form solution difficult.</li>
<li>As we will also see in the interpretation section, the SSR minimisation condition also has several nice properties that make it better than alternatives.</li>
</ul>
</div>
</div>
</div>
<p><br></p>
<p>This minimisation problem can be solved mathematically. Derivations are provided below:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Derivation of OLS for Simple Linear Regression
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let us define the sum of squared residuals as function <span class="math inline">S</span>. We want to minimise <span class="math inline">S</span>, so we have to find the first order conditions and set them equal to 0.</p>
<section id="first-order-conditions" class="level4">
<h4 class="anchored" data-anchor-id="first-order-conditions">First Order Conditions</h4>
<p>First, let us find the partial derivative of <span class="math inline">S</span> in respect to <span class="math inline">\hat\beta_0</span>:</p>
<p><span class="math display">
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} = \frac{\partial }{\partial \hat{\beta}_0} \left[ \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \right]
</span></p>
<p>First, ignore the summation. The partial derivative of the internal section, using chain rule, is the following:</p>
<p><span class="math display">
\frac{\partial}{\partial \hat{\beta}_0} \left[ (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \right] = -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
</span></p>
<p>We know that there is the sum rule of derivatives <span class="math inline">[f(x) + g(x)]' = f'(x) + g'(x)</span>. Thus, we know we just sum up the individual derivatives to get the derivative of the sum:</p>
<p><span class="math display">
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} &amp; = \sum\limits_{-i=1}^n \left[ -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
&amp; = -2 \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
</span></p>
<p>To find the value of <span class="math inline">\hat\beta_0</span> that minimises <span class="math inline">S</span>, we set the derivative equal to 0. We can ignore the -2, since if the summation is equal to 0, the whole derivative will equal 0. Thus, the first order condition is:</p>
<p><span class="math display">
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
</span></p>
<p><br></p>
<p>Now, let us do the same for <span class="math inline">\hat\beta_1</span>. Using the same steps as before</p>
<p><span class="math display">
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_1} &amp; = \sum\limits_{i=1}^n \left[ -2x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
&amp; = -2 \sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
</span></p>
<p>The first order condition for <span class="math inline">\hat\beta_1</span> will be (again, ignoring the -2 for the same reason as before):</p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
</span></p>
<p>Thus, the <strong>first order conditions</strong> of OLS are:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
</span></p>
<p><br></p>
</section>
<section id="solving-the-system-of-equations" class="level4">
<h4 class="anchored" data-anchor-id="solving-the-system-of-equations">Solving the System of Equations</h4>
<p>We now have our two first-order conditions. Now, we have a 2-equation system of equations, with 2 variables.</p>
<p>First, let us solve the first equation for <span class="math inline">\hat\beta_0</span> in terms of <span class="math inline">\hat\beta_1</span>:</p>
<p><span class="math display">
\begin{split}
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) &amp; =  0 \\
\sum\limits_{i=1}^n y_i - n \hat{\beta}_0 - \hat{\beta}_1 \sum\limits_{i=1}^n x_i &amp; = 0 \\
-n\hat{\beta}_0 &amp;= -\sum\limits_{i=1}^n y_i + \hat{\beta}_1\sum\limits_{i=1}^nx_i \\
\hat\beta_0 &amp; = \frac{1}{-n} \left( -\sum\limits_{i=1}^n y_i + \hat{\beta}_1\sum\limits_{i=1}^n x_i \right) \\
\hat{\beta}_0 &amp; = \frac{1}{n} \sum\limits_{i=1}^n y_i - \frac{1}{n}\hat{\beta}_1 \sum\limits_{i=1}^n x_i \\
\hat\beta_0&amp; = \bar{y} - \hat{\beta}_1 \bar{x}
\end{split}
</span></p>
<p>Now, let us substitute our calculated <span class="math inline">\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}</span> into the <span class="math inline">\hat{\beta}_1</span> condition and solve for <span class="math inline">\hat{\beta}_1</span>:</p>
<p><span class="math display">
\begin{split}
0 &amp; =\sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)  \\
&amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - [\bar{y} - \hat{\beta}_1\bar{x}] - \hat{\beta}_1x_i) \right] \\
&amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y} - \hat{\beta}_1 (x_i - \bar{x})) \right] \\
&amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y}) - x_i \hat{\beta}_1(x_i - \bar{x}) \right] \\
&amp; = \sum\limits_{i=1}^n x_i (y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^nx_i (x_i - \bar{x})
\end{split}
</span></p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Before we finish, here are a few key properties of summation</p>
<p><strong>Property 1:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
</span></p>
<ul>
<li>This is because we can expand the left to <span class="math inline">\sum x_i - \sum \bar x</span>.</li>
<li>Then, we know <span class="math inline">\sum x_i = \sum \bar x</span> (by the formula for mean), so <span class="math inline">\sum x_i - \sum \bar x = 0</span>.</li>
</ul>
<p><strong>Property 2:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y})
</span></p>
<ul>
<li>This is because on the right side can expand to <span class="math inline">\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]</span>.</li>
<li>Then, split into <span class="math inline">\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)</span>.</li>
<li>We know that by property 1 (which applies to any variable), <span class="math inline">\sum (y_i - \bar y) = 0</span>. Thus, the right side disappears, and we are left with <span class="math inline">\sum x_i (y_i - \bar y)</span>.</li>
</ul>
<p><strong>Property 3:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
</span></p>
<ul>
<li>Start by expanding right side to <span class="math inline">\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]</span></li>
<li>Which splits into <span class="math inline">\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)</span></li>
<li>By the first property, we know <span class="math inline">\sum x_i - \bar x = 0</span>, so we are only left with <span class="math inline">\sum x_i (x_i - \bar x)</span></li>
</ul>
</div>
</div>
</div>
<p>Knowing these properties of summation, we can transform what we had before:</p>
<p><span class="math display">
\begin{split}
0 &amp; = \sum\limits_{i=1}^n x_i (y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^nx_i (x_i - \bar{x}) \\
0 &amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 \\
\hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 &amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) \\
\hat{\beta}_1 &amp; = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
\end{split}
</span></p>
<p>Note that the numerator is equivalent to the formula of covariance <span class="math inline">Cov(x,y)</span>, and the denominator is equal to the variance <span class="math inline">Var(x)</span>.</p>
<p>We found that <span class="math inline">\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}</span> earlier, so we just plug our solution of <span class="math inline">\hat\beta_1</span> in to get <span class="math inline">\hat\beta_0</span>.</p>
</section>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Derivation of OLS for Multiple Linear Regression
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Similar to our simple linear regression (but with additional variables), our minimisation condition is:</p>
<p><span class="math display">
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...) &amp; = \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} ...)^2 \\
&amp; = \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} S(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)
\end{split}
</span></p>
<p>Taking the partial derivatives of each parameter like in simple linear regression, we get first order conditions:</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\&amp; \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
&amp; \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>This system of equations includes <span class="math inline">k+1</span> variables and <span class="math inline">k+1</span> equations, which is way too difficult to solve.</p>
<p><br></p>
<p>Instead, we can use linear algebra. Let us define our estimation vector <span class="math inline">\hat{\beta}</span> as the value of <span class="math inline">\hat\beta</span> that minimises the sum of squared errors:</p>
<p><span class="math display">
\hat{\beta} = \min\limits_{b} (y - Xb)' (y - Xb) = \min\limits_b S(b)
</span></p>
<ul>
<li><span class="math inline">(y - Xb)</span> is our error, since <span class="math inline">\hat y = Xb</span>,</li>
</ul>
<p>We can expand <span class="math inline">S(b)</span> as follows:</p>
<p><span class="math display">
\begin{split}
S(b) &amp; = y'y - b'X'y - y'Xb + b'X'Xb \\
&amp; = y'y - 2b'X'y + b'X'Xb
\end{split}
</span></p>
<p>Taking the partial derivative in respect to <span class="math inline">b</span>:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = \begin{pmatrix}\frac{\partial S(b)}{\partial b_1} \\\vdots \\\frac{\partial S(b)}{\partial b_k}\end{pmatrix}
</span></p>
<p>Differentiating with the vector <span class="math inline">b</span> yields:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = -2X'y + 2X'Xb
</span></p>
<p>Evaluated at <span class="math inline">\hat{\beta}</span>, the derivatives should equal zero (since first order condition of finding minimums):</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} \biggr|_{\hat{\beta}} = -2X'y + 2X'X \hat{\beta} = 0
</span></p>
<p>When assuming <span class="math inline">X'X</span> is invertable, we can isolate <span class="math inline">\hat{\beta}</span> to find the solution to OLS:</p>
<p><span class="math display">
\begin{split}
-2X'y + 2X'X \hat{\beta} &amp; = 0 \\
2X'X\hat\beta &amp; = 2X'y \\
\hat\beta &amp; = (2X'X)^{-1} 2 X'y \\
\hat\beta &amp; = (X'X)^{-1}X'y
\end{split}
</span></p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="properties-of-the-ols-estimator" class="level2">
<h2 class="anchored" data-anchor-id="properties-of-the-ols-estimator">Properties of the OLS Estimator</h2>
<p>The OLS Regression has a few useful properties that are outlined below:</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
OLS Estimates for Explanatory Variable Partial Out the Effect of Controls (Regression Anatomy)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The <strong>Regression Anatomy Theorem</strong>, also called the <strong>Frisch-Waugh-Lovell</strong> theorem, shows how multiple linear regression and OLS can be used to “control” for confounding variables.</p>
<p>Essentially, the theorem states that given explanatory variables <span class="math inline">x_j= x_1, \dots, x_k</span> and an outcome variable <span class="math inline">y</span>, the coefficient <span class="math inline">\beta_j</span> (of any explanatory variable <span class="math inline">x_j</span>) is the effect of the uncorrelated part of <span class="math inline">x_j</span> with all other explanatory variables, <span class="math inline">\widetilde{r_j}</span>, on <span class="math inline">y</span>. This means by including control variables in our regression, our <span class="math inline">\beta_j</span> coefficient will partial out the effect of the controls, and only find the independent effect of <span class="math inline">x_j</span> on <span class="math inline">y</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of Regression Anatomy Theorem
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Take our standard multiple linear regression:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i
</span></p>
<p>Let us say we are interested in <span class="math inline">x_1</span> (this can be generalised to any explanatory variable <span class="math inline">x_j</span>). Let us make <span class="math inline">x_1</span> the outcome variable of a regression with explanatory variables <span class="math inline">x_2, ..., x_k</span>:</p>
<p><span class="math display">
x_{1i} = \gamma_0 + \gamma_1 x_{2i} + ... + \gamma_{k-1}x_{ki} + \widetilde{r_{1i}}
</span></p>
<ul>
<li>Where <span class="math inline">\gamma_0, ..., \gamma_{k-1}</span> are coefficients.</li>
<li>Where <span class="math inline">\widetilde{r_{1i}}</span> is the error term.</li>
</ul>
<p>The error term is <span class="math inline">\widetilde{r_{1i}}</span>, which represents the part of <span class="math inline">x_{1i}</span> that are uncorrelated to <span class="math inline">x_2, ..., x_k</span>.</p>
<ul>
<li>In other words, <span class="math inline">\widetilde{r_{1i}}</span> is the part of <span class="math inline">x_1</span> that cannot be explained by any other explanatory variable <span class="math inline">x_2, ..., x_k</span>. (uncorrelated with them)</li>
</ul>
<p><br></p>
<p>Now, take the regression of with outcome variable <span class="math inline">y</span>, with all explanatory variables <u>except</u> <span class="math inline">x_1</span>:</p>
<p><span class="math display">
y_i = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i}
</span></p>
<ul>
<li>Where <span class="math inline">\delta_0, ..., \delta_{k-1}</span> are coefficients.</li>
<li>Where <span class="math inline">\widetilde {y_i}</span> is the error term.</li>
</ul>
<p>The error term is <span class="math inline">\widetilde{y_i}</span>, which is the part of <span class="math inline">y_i</span> that cannot be explained by <span class="math inline">x_2, ..., x_k</span> (uncorrelated with them).</p>
<p><br></p>
<p>Since <span class="math inline">\widetilde{y_i}</span> is not explained by <span class="math inline">x_2, ..., x_k</span>, variable <span class="math inline">x_1</span> must be the one explaining <span class="math inline">\widetilde{y_i}</span>.</p>
<ul>
<li>But, it is not the whole of <span class="math inline">x_1</span> explaining <span class="math inline">\tilde{y_i}</span>.</li>
<li>This is since <span class="math inline">x_1</span> may also correlated with <span class="math inline">x_2, ..., x_k</span>, and the correlated parts of <span class="math inline">x_1</span> with <span class="math inline">x_2, ..., x_k</span> are already picked up in the regression by the coefficients of <span class="math inline">x_2, ..., x_k</span>.</li>
</ul>
<p>Thus, <span class="math inline">\widetilde{y_i}</span> must be explained by the part of <span class="math inline">x_1</span> that is uncorrelated and not explained by <span class="math inline">x_2, ..., x_k</span>, which we derived earlier as <span class="math inline">\widetilde{r_{1i}}</span>.</p>
<p><br></p>
<p>Thus, we can create another regression with explanatory variable <span class="math inline">\widetilde{x_{1i}}</span> and outcome variable <span class="math inline">\widetilde{y_i}</span>.</p>
<p><span class="math display">
\widetilde{y_i} = \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i
</span></p>
<p>We can plug <span class="math inline">\widetilde{y_i}</span> back into our regression of <span class="math inline">y_i</span> with explanatory variables <span class="math inline">x_2 ..., x_k</span>, and re-arrange:</p>
<p><span class="math display">
\begin{split}
y_i &amp; = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i} \\
y_i &amp; = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i \\
y_i  &amp; = (\delta_0 + \alpha_0) + \alpha_1 \widetilde{r_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i
\end{split}
</span></p>
<p>As we can see, this new regression mirrors the original standard multiple linear regression:</p>
<p><span class="math display">
\begin{split}
y_i  &amp; = (\delta_0 + \alpha_0) + \alpha_1 \widetilde{r_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i \\
y_i &amp; = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + u_i
\end{split}
</span></p>
<ul>
<li>The <span class="math inline">\beta_0</span> in the original is analogous to the <span class="math inline">(\delta_0 + \alpha 0)</span>.</li>
<li>The <span class="math inline">\beta_1 x_{1i}</span> in the original is analogous to <span class="math inline">\alpha_1 \widetilde{r_{1i}}</span>.</li>
<li>The <span class="math inline">\beta_2 x_{2i} + \dots + \beta_k x_{ki}</span> is analogous to <span class="math inline">\delta_1 x_{2i} + ... + \delta_{k-1} x_{ki}</span>.</li>
<li>The <span class="math inline">u_i</span> is in both regressions.</li>
</ul>
<p><br></p>
<p>Importantly we know the <span class="math inline">\beta_1 x_{1i}</span> in the original is analogous to <span class="math inline">\alpha_1 \widetilde{r_{1i}}</span>. Thus, <u>the estimate of <span class="math inline">\alpha_1</span> will be the same as <span class="math inline">\beta_1</span> in the original regression</u>.</p>
<ul>
<li>The coefficient <span class="math inline">\alpha_1</span> (which is equal to <span class="math inline">\beta_1</span>) explains the expected change in <span class="math inline">y</span>, given an increase in the part of <span class="math inline">x_1</span> uncorrelated with <span class="math inline">x_2, ..., x_k</span>.</li>
<li>So essentially, <u>we have <strong>partialed out</strong> the effect of the other explanatory variables, and only focus on the effect on <span class="math inline">y</span> of the uncorrelated part of <span class="math inline">x_1</span> (which is <span class="math inline">\widetilde{r_{1i}}</span>)</u></li>
</ul>
<p>Or in other words, OLS in multiple linear regression estimates the effect of <span class="math inline">\widetilde{r_{1i}}</span> on <span class="math inline">y</span>.</p>
<ul>
<li>We can apply this to any explanatory variable <span class="math inline">x_1, \dots, x_k</span>. The uncorrelated parts of any explanatory variable <span class="math inline">x_j</span> are labelled <span class="math inline">\widetilde{r_{ji}}</span>.</li>
</ul>
</div>
</div>
</div>
<p>This theorem allows us to essentially “rewrite” the solution of <span class="math inline">\widehat{\beta_j}</span> in multiple regression as a simple linear regression between <span class="math inline">\widetilde{r_j}</span> and <span class="math inline">y</span>:</p>
<p><span class="math display">
\widehat{\beta_j} = \frac{\sum_{i=1}^n \widetilde{r_{ji}} y_i}{\sum_{i=1}^n(\widetilde{r_{ji}})^2}
</span></p>
<p>This formula is called the Regression Anatomy Formula.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of Regression Anatomy Formula
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We can express the estimation solution of <span class="math inline">\hat\beta_j</span> in relation to the regression anatomy formula.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><strong>Property 1:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
</span></p>
<ul>
<li>This is because we can expand the left to <span class="math inline">\sum x_i - \sum \bar x</span>.</li>
<li>Then, we know <span class="math inline">\sum x_i = \sum \bar x</span> (by the formula for mean), so <span class="math inline">\sum x_i - \sum \bar x = 0</span>.</li>
</ul>
<p><strong>Property 2:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y})
</span></p>
<ul>
<li>This is because on the right side can expand to <span class="math inline">\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]</span>.</li>
<li>Then, split into <span class="math inline">\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)</span>.</li>
<li>We know that by property 1 (which applies to any variable), <span class="math inline">\sum (y_i - \bar y) = 0</span>. Thus, the right side disappears, and we are left with <span class="math inline">\sum x_i (y_i - \bar y)</span>.</li>
</ul>
<p><strong>Property 3:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
</span></p>
<ul>
<li>Start by expanding right side to <span class="math inline">\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]</span></li>
<li>Which splits into <span class="math inline">\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)</span></li>
<li>By the first property, we know <span class="math inline">\sum x_i - \bar x = 0</span>, so we are only left with <span class="math inline">\sum x_i (x_i - \bar x)</span></li>
</ul>
</div>
</div>
</div>
<p>Let us start off with the OLS estimator for simple linear regression, which calculates the <span class="math inline">\hat\beta_1</span>, the relationship between <span class="math inline">x</span> and <span class="math inline">y</span> (which we derived previously)</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<p>Let us look at the numerator. Let us expand the numerator:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
= &amp; \sum\limits_{i=1}^n [(x_i - \bar x)y_i - (x_i - \bar x) \bar y] \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \sum\limits_{i=1}^n (x_i - \bar x) \bar y \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
\end{split}
</span></p>
<p>We know that <span class="math inline">\sum (x_i - \bar x) = 0</span> from the above properties. Thus, we can further simplify to:</p>
<p><span class="math display">
\begin{split}
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y(0) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i
\end{split}
</span></p>
<p>Thus, putting the numerator back in, we now we have the equation:</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})y_i}{\sum_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<p><br></p>
<p>We know from the Regression Anatomy Theorem, that in multiple linear regression, <span class="math inline">\hat\beta_j</span> is not the full relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>. Instead, it is the relationship of the part of <span class="math inline">x_j</span> that is uncorrelated with all other explanatory variables, and <span class="math inline">y</span>.</p>
<ul>
<li>So in other words, it is the relationship of <span class="math inline">\widetilde{r_{ji}}</span> on <span class="math inline">y</span>.</li>
</ul>
<p>So, since multiple linear regression is the relationship of <span class="math inline">\widetilde{r_{ji}}</span> on <span class="math inline">y</span>, instead of <span class="math inline">x</span> on <span class="math inline">y</span>, let us replace the <span class="math inline">x</span>’s in our formula with <span class="math inline">\widetilde{r_{ji}}</span>:</p>
<p><span class="math display">
\hat{\beta}_j = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))^2}
</span></p>
<p>We can actually simplify this more with a property of regression - remember, that the error term of a regression <span class="math inline">u</span>, should be such that <span class="math inline">E(u)=0</span>.</p>
<p>We know that <span class="math inline">\widetilde{r_{ji}}</span> is also the error term of a regression, so, <span class="math inline">E(\widetilde{r_{ji}}) = 0</span> as well. Plugging that into our equation, we can get the regression anatomy formula for OLS.</p>
<p><span class="math display">
\begin{split}
\hat{\beta}_j &amp; = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - E(\widetilde{r_{ji}}))^2} \\
\\
&amp; = \frac{\sum_{i=1}^n (\widetilde{r_{ji}} - 0)y_i}{\sum_{i=1}^n(\widetilde{r_{ji}} - 0)^2} \\
\\
&amp; = \frac{\sum_{i=1}^n \widetilde{r_{ji}} \ y_i}{\sum_{i=1}^n \widetilde {r_{ji}}^2}
\end{split}
</span></p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
OLS Best Approximates the Conditional Expectation Function
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A <strong>conditional expectation function</strong> says that the value of <span class="math inline">E(y)</span> depends on the value of <span class="math inline">x</span>. We notate a conditional expectation function as <span class="math inline">E(y|x)</span>. As we noted earlier, the linear regression model can be a conditional expectation function of <span class="math inline">E(y|x)</span>.</p>
<p>A <strong>best linear approximation</strong> of a conditional expectation function, can take the following form:</p>
<p><span class="math display">
E(y_i|x_i) = b_0 + b_1x_i
</span></p>
<ul>
<li>Where <span class="math inline">b_0</span> and <span class="math inline">b_1</span> are the parameters/coefficients of the model.</li>
<li>Where <span class="math inline">E(y_i|x_i)</span> is the expectation of the conditional distribution <span class="math inline">y|x</span>.</li>
<li>Where the conditional distribution <span class="math inline">y|x</span> has variance <span class="math inline">\sigma^2</span>.</li>
</ul>
<p>The best linear approximation of the conditional expectation function is defined as the parameters that minimise the mean squared errors (MSE).</p>
<p><span class="math display">
\begin{split}
MSE &amp; = E(y_i - E(y_i|x_i))^2 \\
&amp; = \frac{1}{n}\sum\limits_{i=1}^n( y_i - E(y_i|x_i))^2
\end{split}
</span></p>
<p><br></p>
<p>Previously, we expressed the linear regression model in both terms of <span class="math inline">y_i</span>, and the conditional expectation <span class="math inline">E(y_i|x_i)</span>. However, when conducting OLS estimation, we have only looked at the <span class="math inline">y_i</span> form.</p>
<p>The Ordinary Least Squares Regression line also is the best linear approximation of the conditional expectation function <span class="math inline">E(y|x)</span>. That means, we can view a regression as also a best-estimate of a conditional expectation function.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof that OLS is the Best Approximation of the CEF
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>What we want to prove is that the OLS estimates <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span> best estimate the parameters <span class="math inline">b_0</span> and <span class="math inline">b_1</span> of the Conditional Expectation Function, which means that if true, OLS is the best linear approximation of the conditional expectation function.</p>
<p>Suppose we have the conditional expectation function:</p>
<p><span class="math display">
E(y_i|x_i) = b_0 + b_1x_i
</span></p>
<p>We also know that our typical regression equation is:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_i + u_i
</span></p>
<p>We know that <span class="math inline">E(u_i|x_i) = 0</span>. Let us define <span class="math inline">u_i</span> as the following:</p>
<p><span class="math display">
u_i = y_i - E(y_i|x_i)
</span></p>
<p>If the above defined <span class="math inline">u_i</span> is true, <span class="math inline">E(u_i|x_i)</span> should also be equal to 0. So, let us plug in the above <span class="math inline">u_i</span> into <span class="math inline">E(u_i | x_i)</span>.</p>
<p><span class="math display">
\begin{split}
E(u_i|x_i) &amp; = E(y_i - E(y_i|x_i) \ | \ x_i) \\
&amp; = E(y_i|x_i) - E(y_i|x_i) \\
&amp; = 0
\end{split}
</span></p>
<p>Thus, we know <span class="math inline">u_i = y_i - E(y_i|x_i)</span> to be true. Thus, rearranging, we know:</p>
<p><span class="math display">
y_i = E(y_i|x_i) + u_i
</span></p>
<p>We also know that <span class="math inline">y_i = \beta_0 + \beta_1 x_i + u_i</span>. Thus, the following is true:</p>
<p><span class="math display">
\begin{split}
E(y_i|x_i) + u_i &amp; = \beta_0 + \beta_1 + u_i \\
b_0 + b_1 + u_i &amp; = \beta_0 + \beta_1 + u_i \\
b_0 + b_1 &amp; = \beta_0 + \beta_1
\end{split}
</span></p>
<p>Well, you might point out, it is still possible that <span class="math inline">b_1 ≠ \beta_1</span> in this scenario. We can go further. We know that the conditional expectation function minimises the mean of squared errors.</p>
<p><span class="math display">
\begin{split}
MSE &amp; = \min\limits_{b_0, b_1} E(y_i - E(y_i|x_i))^2 \\
&amp; = \min\limits_{b_0, b_1} E(y_i - \beta_0 - \beta_1x_i)^2
\end{split}
</span></p>
<p>The first order conditions are (using chain rule and partial derivatives, just like in the OLS derivation):</p>
<p><span class="math display">
\begin{split}
&amp; E(y_i - b_0 - b_1x_i) = 0 \\
&amp; E(x_i(y_i - b_0 - b_1x_i) = 0
\end{split}
</span></p>
<p><br></p>
<p>Now, recall our OLS minimisation conditions (from <a href="https://statsnotes.github.io/metrics/2.html#estimation-for-simple-linear-regression">2.2.3</a>)</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
</span></p>
<p>Since by definition, average/expectation is <span class="math inline">E(x) = \frac{1}{n} \sum x_i</span>, we can rewrite the OLS minimisation conditions as:</p>
<p><span class="math display">
\begin{split}
&amp; n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
</span></p>
<p>And since anything multiplied to a zero turns into zero, we can ignore the <span class="math inline">n</span> in the first order condition, and only focus on the expected value part. Thus, our conditions are:</p>
<p><span class="math display">
\begin{split}
&amp; E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
</span></p>
<p>Which as we can see, are the exact same minimisation conditions as the conditional expectation function. Thus, OLS is the best approximation of the conditional expectation function.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
OLS is a Special Case of the Method of Moments Estimator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population <strong>moments</strong> of interest - which are the population parameters written in terms of expected value functions set equal to 0.</p>
<p>Then, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter.</p>
<ul>
<li>For example, to estimate the population mean, the Method of Moments uses the sample mean.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Details of the Method of Moments Estimator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In order to define a method of moments for a set of parameters <span class="math inline">\theta_1, \dots, \theta_k</span>, we need to specify at least one population moment per parameter. Or in other words, we must have more than <span class="math inline">k</span> population moments.</p>
<p>Our population moments can be defined as the expected value of some function <span class="math inline">m(\theta; y)</span> that consists of both the variable <span class="math inline">y</span> and our unknown parameter <span class="math inline">\theta</span>. The expectation of the function <span class="math inline">m(\theta; y)</span> should equal 0.</p>
<p><span class="math display">
E(m(\theta; y)) = 0
</span></p>
<p>Our sample moments will be the sample analogues of <span class="math inline">\theta</span> and <span class="math inline">y</span>, which are <span class="math inline">\hat\theta</span> and <span class="math inline">y_i</span>:</p>
<p><span class="math display">
\frac{1}{n}\sum\limits_{i=1}^n m(\hat\theta; y_i) = 0
</span></p>
<ul>
<li>The <span class="math inline">\frac{1}{n} \sum</span> is there because the definition of expectation/mean is that.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example of a Method of Moments Estimator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let us say that we have some random variable <span class="math inline">y</span>, with a true population mean <span class="math inline">\mu</span>. We want to estimate <span class="math inline">\mu</span>, but we only have a sample of the population.</p>
<p>How can we define our true population parameter <span class="math inline">\mu</span> in an expectation equation of the form: <span class="math inline">E(m(\mu, y)) = 0</span>?</p>
<ul>
<li>Well, what is <span class="math inline">\mu</span>, the mean, intuitively speaking? It is the expectation of <span class="math inline">y</span>, so <span class="math inline">\mu = E(y)</span>.</li>
</ul>
<p>Now that we know that <span class="math inline">\mu = E(y)</span>, since they are equal, <span class="math inline">\mu - E(y) = 0</span>. Thus, we can define the mean as a moment of the following condition:</p>
<p><span class="math display">
E(y - \mu) = 0
</span></p>
<p>The method of moments says we should use the sample equivalent of the population parameter. The sample equivalent of <span class="math inline">\mu</span> (the true mean of the population), is of course, the sample mean <span class="math inline">\bar y</span>.</p>
<p>Thus, our sample estimate of the moment would be:</p>
<p><span class="math display">
E(y_i - \hat\mu) = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) = 0
</span></p>
<p>With this equation, we can then solve for <span class="math inline">\hat\mu</span>:</p>
<p><span class="math display">
\begin{split}
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) \\
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n}\sum\limits_{i=1}^n \hat\mu \\
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n} n \hat\mu\\
0 &amp; = \bar y - \hat \mu \\
\hat\mu &amp; = \bar y
\end{split}
</span></p>
<p>So, we see the method of moments estimates our true population mean <span class="math inline">\mu</span>, with the sample mean <span class="math inline">\bar y</span>.</p>
</div>
</div>
</div>
<p>OLS is a special case of the Method of Moments Estimator. This is a useful property as we move towards causal inference, as it shows why Endogeneity is an important condition of causal inference (and forms the bases of the Instrumental Variables Estimator).</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof that OLS is a Method of Moments Estimator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consider the bivariate regression model:</p>
<p><span class="math display">
y = \beta_0 + \beta_1x + u
</span></p>
<p>The OLS estimator can be derived as a method of moments estimator, with 2 moments (expectation functions set equal to 0), one for each parameter (<span class="math inline">\beta_0, \beta_1</span>):</p>
<p><span class="math display">
\begin{split}
&amp; E(y-\beta_0 -\beta_1x) = 0 \\
&amp; E(x(y - \beta_0 - \beta_1 x)) = 0
\end{split}
</span></p>
<p>Since we know <span class="math inline">u = y - \beta_0 - \beta_1 x</span>, we can rewrite the two moments as:</p>
<p><span class="math display">
\begin{split}
&amp; E(u) = 0 \\
&amp; E(xu) = 0
\end{split}
</span></p>
<p>The estimates of these moments would use the sample equivalents: <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>.</p>
<p><span class="math display">
\begin{split}
&amp; E(y-\hat\beta_0 -\hat\beta_1x) = 0 \\
&amp; E(x(y - \hat\beta_0 - \hat\beta_1 x)) = 0
\end{split}
</span></p>
<p><br></p>
<p>Let us prove that OLS is a special case of the method of moments estimator. Remember our OLS minimisation conditions:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
</span></p>
<p>Since by definition, average/expectation is <span class="math inline">E(x) = \frac{1}{n} \sum x_i</span>, we can rewrite the OLS minimisation conditions as:</p>
<p><span class="math display">
\begin{split}
&amp; n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
</span></p>
<p>And since anything multiplied to a zero turns into zero, we can ignore the <span class="math inline">n</span> in the first order condition, and only focus on the expected value part. Thus, our conditions are:</p>
<p><span class="math display">
\begin{split}
&amp; E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
</span></p>
<p>Which as we can see, are the exact same minimisation conditions as the method of moments estimator. Thus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
OLS Residuals Sum to 0
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The residuals of OLS are the errors (but this time, not squared):</p>
<p><span class="math display">
\begin{split}\hat u_i &amp; = y_i - \hat y_i \\&amp; = y_i - (\hat \beta_0 + \hat\beta_1x_{1i} + \dots + \hat\beta_k x_{ki}) \\&amp; = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}\end{split}
</span></p>
<p>OLS Residuals sum to 0 - i.e.&nbsp;the average true value of <span class="math inline">y</span> and the average predicted value of <span class="math inline">\hat y</span> are equal.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof OLS Residuals Sum to 0
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Recall our OLS first order conditions:</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\&amp; \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
&amp; \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>We know residuals <span class="math inline">\hat u_i = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}</span>.</p>
<p>If we plug that definition of <span class="math inline">\hat u_i</span> into the first minimisation condition, we get:</p>
<p><span class="math display">
\sum\limits_{i=1}^n \hat u_i = 0
</span></p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-26-contents" aria-controls="callout-26" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
No Covariance Between Any Explanatory Variable and OLS Residuals
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-26" class="callout-26-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In OLS estimation, any explanatory variable <span class="math inline">x_1, \dots, x_k</span> is uncorrelated with the OLS residuals <span class="math inline">\hat u_i = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of No Covariance
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Recall our OLS first order conditions:</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\&amp; \sum\limits_{i=1}^n x_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
&amp; \sum\limits_{i=1}^n x_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>We know residuals <span class="math inline">\hat u_i = y_i - \hat \beta_0 - \hat\beta_1x_{1i} - \dots - \hat\beta_k x_{ki}</span>.</p>
<ul>
<li>Thus, plugging into the first condition, <span class="math inline">\sum\limits_{i=1}^n \hat u_i = 0</span>.</li>
<li>Thus, plugging into the other conditions, <span class="math inline">\sum\limits_{i=1}^n x_j \hat u_i = 0</span>.</li>
</ul>
<p>For simplicity, we will use simple linear regression, but the same applies to multiple linear regression.</p>
<p>Now, recall the formula for covariance discussed in <a href="https://statsnotes.github.io/theory/3.html#quantifying-relationships-with-covariance">1.3.4</a>:</p>
<p><span class="math display">
Cov(x,y) = \frac{1}{n}\sum\limits_{i=1}^n [(x_i - \bar x)(y_i - \bar y)] \\
</span></p>
<p>Thus, the covariance between <span class="math inline">x_j</span> (for notation simplicity, just <span class="math inline">x</span>) and <span class="math inline">\hat u</span> is:</p>
<p><span class="math display">
\begin{split}
Cov(x, \hat u) &amp; = \frac{1}{n}\sum\limits_{i=1}^n [(x_i - \bar x)(\hat u_i - \bar{\hat u})] \\
&amp; = \frac{1}{n}\sum\limits_{i=1}^n(x_i \hat u_i - x_i \bar{\hat u} - \bar x \hat u_i + \bar x \bar {\hat u}) \\
&amp; = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - \sum\limits_{i=1}^n x_i\bar{\hat u} - \sum\limits_{i=1}^n \bar x \hat u_i + \sum\limits_{i=1}^n\bar x \bar{\hat u} \right) \\
&amp; = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - \bar{\hat u}\sum\limits_{i=1}^n x_i - \bar x\sum\limits_{i=1}^n \hat u_i + \bar{\hat u}\sum\limits_{i=1}^n\bar x  \right) \\
&amp; = \frac{1}{n} \left( \sum\limits_{i=1}^nx_i \hat u_i - 0\sum\limits_{i=1}^n x_i - \bar x\sum\limits_{i=1}^n \hat u_i + 0\sum\limits_{i=1}^n\bar x  \right) \\
&amp; = \frac{1}{n}(0 -0-\bar x(0) + 0) \\
&amp; = 0
\end{split}
</span></p>
<p>Thus, the covariance (and thus correlation) between <span class="math inline">x_j</span> and <span class="math inline">\hat u</span> must be zero.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-27-contents" aria-controls="callout-27" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
OLS Best-Fit Line Passes Through Means
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-27" class="callout-27-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The OLS estimated coefficients produces a best-fit line, that always passes through the point <span class="math inline">(\bar x_1, \dots, \bar x_k, \bar y)</span>, which is the point of the means of all the variables.</p>
<p>For simplicity, take simple linear regression. Remember our solution for <span class="math inline">\hat\beta_0</span> in OLS for simple linear regression was <span class="math inline">\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}</span>. Rearranging this equation, we get:</p>
<p><span class="math display">
\begin{split}
\hat\beta_0  &amp; = \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat\beta_0 + \hat\beta_1 \bar x &amp; = \bar y \\
\bar y &amp; = \hat\beta_0 + \hat\beta_1 \bar x
\end{split}
</span></p>
<p>Thus, the OLS estimated best-fit line always passes though point <span class="math inline">(\bar x, \bar y)</span> (the means of our data). The same property applies to multiple linear regression, but for point <span class="math inline">(\bar x_1, \dots, \bar x_k, \bar y)</span>.</p>
</div>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="interpretation" class="level1">
<h1>Interpretation</h1>
<section id="interpretation-of-coefficients" class="level2">
<h2 class="anchored" data-anchor-id="interpretation-of-coefficients">Interpretation of Coefficients</h2>
<p>Once we have estimated our coefficients model, we will have the following fitted-values model.</p>
<p><span class="math display">
\widehat{y_i} = E(y_i|x_i)= \widehat{\beta_0} + \widehat{\beta_1}x_{1i} + \dots + \widehat{\beta_k} x_{ki}
</span></p>
<p>The coefficient <span class="math inline">\widehat{\beta_0}</span>, also called the intercept, is the expected value of <span class="math inline">y</span> given all explanatory variables <span class="math inline">x_1, \dots, x_k = 0</span>.</p>
<p><br></p>
<p>The coefficient <span class="math inline">\widehat{\beta_j} = \widehat{\beta_1}, \dots, \widehat{\beta_k}</span> has an interpretation that depends on the type of variables <span class="math inline">y</span> and <span class="math inline">x_j. =x_1, \dots, x_k</span> are:</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-28-contents" aria-controls="callout-28" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="math inline">\widehat{\beta_j}</span> for Continuous <span class="math inline">y</span>, Continuous <span class="math inline">x_j</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-28" class="callout-28-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For every one unit increase in <span class="math inline">x_j</span>, there is an expected <span class="math inline">\widehat{\beta_j}</span> unit change in <span class="math inline">y</span>.</p>
<p>We can also standardise our interpretations in respect to standard deviations: For a one standard deviation increase in <span class="math inline">x_j</span>, there is an expected <span class="math inline">\widehat{\beta_1}\sigma_x/\sigma_y</span> standard deviation change in <span class="math inline">y</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-29-contents" aria-controls="callout-29" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="math inline">\widehat{\beta_j}</span> for Continuous <span class="math inline">y</span>, Binary <span class="math inline">x_j</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-29" class="callout-29-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>There is an expected <span class="math inline">\widehat{\beta_j}</span> unit differnce in <span class="math inline">y</span> between categories <span class="math inline">x_j = 1</span> and <span class="math inline">x_j = 0</span>.</p>
<p>We can also calculate the expected <span class="math inline">y</span> for each category of <span class="math inline">x_j</span>:</p>
<ul>
<li>The expected <span class="math inline">y</span> of category <span class="math inline">x_j = 0</span> is <span class="math inline">\widehat{\beta_0}</span>.</li>
<li>The expected <span class="math inline">y</span> of category <span class="math inline">x_j = 1</span> is <span class="math inline">\widehat{\beta_0} + \widehat{\beta_1}</span>.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-30-contents" aria-controls="callout-30" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="math inline">\widehat{\beta_j}</span> for Continuous <span class="math inline">y</span>, Continuous <span class="math inline">x_j</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-30" class="callout-30-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For every one unit increase in <span class="math inline">x_j</span>, there is an expected <span class="math inline">\widehat{\beta_j} \times 100</span> percentage point change in the chance of a unit being in category <span class="math inline">y=1</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-31-contents" aria-controls="callout-31" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="math inline">\widehat{\beta_j}</span> for Continuous <span class="math inline">y</span>, Continuous <span class="math inline">x_j</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-31" class="callout-31-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>There is an expected <span class="math inline">\widehat{\beta_j}</span> percentage point difference in the chance of being in category <span class="math inline">y=1</span> between category <span class="math inline">x_j = 1</span> and category <span class="math inline">x_j = 0</span>.</p>
<p>We can also calculate the expected percentage of units in category <span class="math inline">y=1</span> for each category of <span class="math inline">x_j</span>:</p>
<ul>
<li>The percentage of units in category <span class="math inline">y=1</span> for category <span class="math inline">x_j = 0</span> is <span class="math inline">\widehat{\beta_0}</span>.</li>
<li>The percentage of units in category <span class="math inline">y=1</span> for category <span class="math inline">x_j = 1</span> is <span class="math inline">\widehat{\beta_0} + \widehat{\beta_1}</span>.</li>
</ul>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="model-summary-statistics" class="level2">
<h2 class="anchored" data-anchor-id="model-summary-statistics">Model Summary Statistics</h2>
<p>The <strong>Residual Standard Deviation</strong> <span class="math inline">\hat\sigma</span> is the standard deviation of the residuals (also, the standard deviation of the conditional expectation distribution).</p>
<p>The formula for Residual Variance is given below (square root for standard deviation):</p>
<p><span class="math display">
\hat\sigma^2 = \frac{\sum_{i=1}^n (y_i -\hat y_i)^2}{n-k-1}
</span></p>
<ul>
<li>Larger values mean the observed values of <span class="math inline">y</span> are more widely scattered around <span class="math inline">E(y|x)</span>.</li>
<li>Smaller values mean the observed values of <span class="math inline">y</span> are more tightly concentrated around <span class="math inline">E(y|x)</span>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-2778593669.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
<p><br></p>
<p>The <strong>R-squared</strong> Statistic, <span class="math inline">R^2</span>, is a measure of the fit of our best-fit line.</p>
<ul>
<li>Interpreted as: The proportion of the total variance in <span class="math inline">y_i</span> that is accounted for (or explained) by the explanatory variables in the model.</li>
<li><span class="math inline">R^2</span> is always between 0 and 1 (or 0% and 100%).</li>
</ul>
<p>The formula for <span class="math inline">R^2</span> is:</p>
<p><span class="math display">
R^2 = \frac{\sum_{i=1}^n(\widehat{y_i} - \bar y)^2}{\sum_{i=1}^n (y_i - \bar y)^2}
</span></p>
<ul>
<li>The numerator is also called the explained sum of squares (SSE), the variation in <span class="math inline">y</span> that is explained by our model.</li>
<li>The denominator is called the total sum of squares (SST), the total variation in <span class="math inline">y</span>.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-32-contents" aria-controls="callout-32" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Derivation of R-Squared
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-32" class="callout-32-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For each observation, we know that the actual <span class="math inline">y_i</span> value is the predicted <span class="math inline">\hat y_i</span> plus the residual term <span class="math inline">\hat u_i</span>. Thus:</p>
<p><span class="math display">
y_i = \hat y_i + \hat u_i
</span></p>
<p>Now, let us define these three concepts: the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR):</p>
<p><span class="math display">
\begin{split}
&amp; SST = \sum\limits_{i=1}^n (y_i - \bar y)^2 \\
&amp; SSE = \sum\limits_{i=1}^n (\hat y_i - \bar y)^2 \\
&amp; SSR = \sum\limits_{i=1}^n (\hat u_i)^2
\end{split}
</span></p>
<ul>
<li>The SST explains the total amount of variation in <span class="math inline">y</span></li>
<li>The SSE is the amount of variation in <span class="math inline">y</span> explained by our model</li>
<li>The SSR is the amount of variation in <span class="math inline">y</span> not explained by our model</li>
</ul>
<p>Let us look at the total sum of squares (SST). We can manipulate it as follows:</p>
<p><span class="math display">
\begin{split}
SST &amp; = \sum\limits_{i=1}^n (y_i - \bar y)^2 \\
&amp; = \sum\limits_{i=1}^n(y_i - \hat y_i+ \hat y_i - \bar y)^2 \\
&amp; = \sum\limits_{i=1}^n((y_i - \hat y_i)+ \hat y_i - \bar y)^2 \\
&amp; = \sum\limits_{i=1}^n[\hat u_i + \hat y_i - \bar y]^2 \\
&amp; = \sum\limits_{i=1}^n[\hat u_i^2 + \hat u_i \hat y_i - \hat u_i \bar y + \hat y_i \hat u_i + \hat y_i^2 - \hat y_i \bar y-\bar y \hat u_i -\bar y \hat  y_i+\hat y^2_i] \\
&amp; = \sum\limits_{i=1}^n[ \hat u_i^2 + 2 \hat u_i \hat y_i+ \hat y_i^2 - 2 \hat u_i \bar y - 2 \hat y_i \bar y + \bar y ^2]
\end{split}
</span></p>
<p>By a property of linear regression, <span class="math inline">\sum \hat y_i \hat u_i = 0</span>. Knowing this, we can further simplify to:</p>
<p><span class="math display">
\begin{split}
SST &amp; = \sum\limits_{i=1}^n[ \hat u_i^2 + \hat y_i^2 - 2 \hat u_i \bar y - 2 \hat y_i \bar y + \bar y ^2] \\
&amp; = \sum\limits_{i=1}^n[\hat u_i^2 + (\hat y_i - \bar y)^2]\\
&amp; = \sum\limits_{i=1}^n \hat u_i^2 + \sum\limits_{i=1}^n(\hat y_i - \bar y)^2 \\
&amp; = SSE + SSR
\end{split}
</span></p>
<p>This makes sense: After all, SSE is the squared errors explained by the model, and SSR is the residual (non-explained) parts of the model, so together, they should be equal to the total sum of squares.</p>
<p>Thus, SSE/SST should be a percentage between 0 and 100, and 1 - SSR/SST should also be equivalent to that.</p>
</div>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="assumptions" class="level1">
<h1>Assumptions</h1>
<section id="assumptions-for-unbiasedness" class="level2">
<h2 class="anchored" data-anchor-id="assumptions-for-unbiasedness">Assumptions for Unbiasedness</h2>
<p>Unbiasedness means the expected value of our estimate from our population is equal to the true population value. In other words, <span class="math inline">E(\hat\theta) = \theta</span>.</p>
<p>The OLS estimate of <span class="math inline">\widehat{\beta_j}</span> (the relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>) is only unbiased if these 4 conditions are met:</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-33-contents" aria-controls="callout-33" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.1 Linearity in Parameters
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-33" class="callout-33-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A model must be linear in parameters for unbiasedness. This means that the parameters of the model <span class="math inline">\beta_0, \dots, \beta_k</span> must not be multiplied/divided together.</p>
<p>Note: this does not mean the actual regression line must be linear - only the parameters/coefficients must not be multiplied.</p>
<p>For example, the following model is still linear in parameters:</p>
<p><span class="math display">
y = \beta_0 + \beta_1x^2
</span></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-34-contents" aria-controls="callout-34" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.2 Random Sampling
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-34" class="callout-34-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This assumption says that all observations in our sample are randomly sampled from the same population.</p>
<p>The error term <span class="math inline">u</span> in our regression model is some random variable (with its own probability distribution), that can be defined by its expectation <span class="math inline">E(u)</span>.</p>
<ul>
<li>If we randomly select one observation <span class="math inline">i</span> from the data, each observation <span class="math inline">i</span> has an equal chance of being selected.</li>
<li>The error term for that observation, <span class="math inline">u_i</span>, should also have the same expectation as the random variable <span class="math inline">u</span>, since each observation <span class="math inline">i</span> within <span class="math inline">u</span> has the same chance of being selected.</li>
</ul>
<p>Thus, random sampling allows us to say <span class="math inline">E(u) = E(u_i)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-35-contents" aria-controls="callout-35" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.3 No Perfect Multicollinearity/Variance in <span class="math inline">x</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-35" class="callout-35-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>No perfect multicollinearity (MLR.3) means that no two explanatory variables are perfectly correlated together. This also means that in the regression anatomy formula, <span class="math inline">\sum \widetilde{r_{ji}}^2 ≠ 0</span>.</p>
<p>If we have a simple linear regression, we instead must make sure that there is indeed variance in the explanatory variable <span class="math inline">x</span> (SLR.3). Or in other words: <span class="math inline">Var(x) ≠ 0</span>.</p>
<p><strong>These two assumptions are necessary to even calculate</strong> <span class="math inline">\widehat{\beta_0}</span>, since the formulas require dividing by these values, so if they equal 0, we cannot calculate OLS.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-36-contents" aria-controls="callout-36" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.4 Zero-Conditional Mean
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-36" class="callout-36-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In the population, the error term <span class="math inline">u</span> must have an expectation of 0, given all values of <span class="math inline">\overrightarrow x</span>.</p>
<p>Mathematically:</p>
<p><span class="math display">
E(u|x_1, \dots, x_k) = 0, \ \forall (x_1, \dots, x_k)
</span></p>
<p>When combined with MLR.2 Random Sampling from above, we can say that:</p>
<p><span class="math display">
E(u|x_1, \dots, x_k) = E(u_i|x_{1i}, \dots, x_{ki}) = 0
</span></p>
<p>This is the <u>key assumption</u> of OLS that is most frequently violated.</p>
</div>
</div>
</div>
<p><br></p>
<p>If all 4 conditions are met, The OLS estimate of <span class="math inline">\widehat{\beta_j}</span> (the relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>) is unbiased.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-41-contents" aria-controls="callout-41" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of Unbiasedness for Simple Linear Regression
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-41" class="callout-41-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We want to show <span class="math inline">E(\hat\beta_1) = \beta_1</span>. Let us start off with the OLS estimator:</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SLR.3 Variance in <span class="math inline">x</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>The existence of <span class="math inline">\hat{\beta}_1</span> is guaranteed by SLR.3 <span class="math inline">Var(x) ≠ 0</span>, since we cannot divide by 0.</p>
</div>
</div>
<p><br></p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-38-contents" aria-controls="callout-38" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-38" class="callout-38-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><strong>Property 1:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
</span></p>
<ul>
<li>This is because we can expand the left to <span class="math inline">\sum x_i - \sum \bar x</span>.</li>
<li>Then, we know <span class="math inline">\sum x_i = \sum \bar x</span> (by the formula for mean), so <span class="math inline">\sum x_i - \sum \bar x = 0</span>.</li>
</ul>
<p><strong>Property 2:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y})
</span></p>
<ul>
<li>This is because on the right side can expand to <span class="math inline">\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]</span>.</li>
<li>Then, split into <span class="math inline">\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)</span>.</li>
<li>We know that by property 1 (which applies to any variable), <span class="math inline">\sum (y_i - \bar y) = 0</span>. Thus, the right side disappears, and we are left with <span class="math inline">\sum x_i (y_i - \bar y)</span>.</li>
</ul>
<p><strong>Property 3:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
</span></p>
<ul>
<li>Start by expanding right side to <span class="math inline">\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]</span></li>
<li>Which splits into <span class="math inline">\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)</span></li>
<li>By the first property, we know <span class="math inline">\sum x_i - \bar x = 0</span>, so we are only left with <span class="math inline">\sum x_i (x_i - \bar x)</span></li>
</ul>
</div>
</div>
</div>
<p>Let us look at the numerator. Let us expand the numerator:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
= &amp; \sum\limits_{i=1}^n [(x_i - \bar x)y_i - (x_i - \bar x) \bar y] \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \sum\limits_{i=1}^n (x_i - \bar x) \bar y \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
\end{split}
</span></p>
<p>We know that <span class="math inline">\sum (x_i - \bar x) = 0</span> (from the properties above). Thus, we can further simplify to:</p>
<p><span class="math display">
\begin{split}
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y(0) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i
\end{split}
</span></p>
<p>Now, let us play with the numerator more (note the properties of summation introduced earlier):</p>
<p><span class="math display">
\begin{split}
&amp; = \sum\limits_{i=1}^n (x_i - \bar{x})y_i \\
&amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(\beta_0 + \beta_1 x_i + u_i) \\
&amp; = \sum\limits_{i=1}^n(x_i - \bar{x}) \beta_0 + \sum\limits_{i=1}^n(x_i - \bar{x}) \beta_1  x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\
&amp; = \beta_0 \sum\limits_{i=1}^n(x_i - \bar{x}) + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x}) x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\
&amp; = 0 + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i\end{split}
</span></p>
<p>Now, putting the numerator back into the equation, we simplify:</p>
<p><span class="math display">
\begin{split}
\hat{\beta}_1 &amp; = \frac{\beta_1 \sum_{i=1}^n(x_i - \bar{x})^2 + \sum_{i=1}^n(x_i - \bar{x})u_i}{\sum_{i=1}^n(x_i - \bar{x})^2} \\
&amp;  =  \beta_1 + \frac{\sum_{i=1}^n(x_i - \bar{x}) u_i}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&amp;  = \beta_1 + \sum\limits_{i=1}^n w_i u_i
\end{split}
</span></p>
<ul>
<li>Where <span class="math inline">w_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}</span>, which is a function of random variable <span class="math inline">x</span>. We could also write <span class="math inline">w_i</span> as <span class="math inline">\frac{x_i - \bar{x}}{SST_x}</span> (where <span class="math inline">SST_x</span> is total sum of squares for <span class="math inline">x</span>).</li>
</ul>
<p>Since <span class="math inline">w_i</span> is a function of <span class="math inline">x</span>, that means <span class="math inline">\hat\beta_1</span> is also a function of <span class="math inline">x</span> (depends on the value of <span class="math inline">x</span>).</p>
<p><br></p>
<p>Now we need to find the expectation <span class="math inline">E(\hat\beta_1)</span>. Thus, we have this equation:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1|x) &amp; = E \left( \beta_1 + \sum\limits_{i=1}^n w_i u_i \bigg| x \right) \\
&amp; = \beta_1 + \sum\limits_{i=1}^nE(w_iu_i|x)
\end{split}
</span></p>
<p>But what does <span class="math inline">\sum E(w_iu_i |x)</span> equal? This is where our other two Gauss-Markov Conditions come into play.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SLR.4 Zero Conditional Mean
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Zero-Conditional Mean assumption says <span class="math inline">E(u|x) = 0</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SLR.2 Random Sampling
</div>
</div>
<div class="callout-body-container callout-body">
<p>Random sampling, combined with zero-conditional mean, allows us to say:</p>
<p><span class="math display">
E(u|x) = E(u_i | x_i) = E(u_i|x) = 0
</span></p>
</div>
</div>
<p>This means that:</p>
<p><span class="math display">
E(w_i u_i|x) = w_i E(u_i|x) = 0
</span></p>
<p><br></p>
<p>Now knowing what <span class="math inline">E(w_iu_i|x)</span> is, let us plug it back into our equation:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1|x) &amp; = \beta_1 + \sum\limits_{i=1}^nE(w_iu_i|x) \\
&amp; = \beta_1 + \sum\limits_{i=1}^n0 \\
&amp; = \beta_1
\end{split}
</span></p>
<p>However, we have solved for <span class="math inline">E(\hat\beta_1 |x)</span>, and not <span class="math inline">E (\hat\beta_1)</span>. This is where the <strong>Law of Iterated Expectation</strong>. The law says the following:</p>
<p><span class="math display">
E(x) = E[E(x|y)]
</span></p>
<p>Thus, we can use this to conclude the proof:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1) &amp; = E[E(\hat\beta_1|x)] \\
&amp; = E(\beta_1) \\
&amp; = \beta_1
\end{split}
</span></p>
<ul>
<li>Since <span class="math inline">\beta_1</span> is the true value (a constant), its expectation is itself</li>
</ul>
<p>Thus, <span class="math inline">E(\hat\beta_1) = \beta_1</span>, proving the unbiasedness of OLS under the 4 conditions.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-46-contents" aria-controls="callout-46" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of Unbiasedness for Multiple Linear Regression
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-46" class="callout-46-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For simplicity, let us focus on <span class="math inline">\hat\beta_1</span>. However, this can be generalised to any <span class="math inline">\hat\beta_2, \dots, \hat\beta_k</span>.</p>
<p>Recall the regression anatomy solution of OLS for <span class="math inline">\hat\beta_1</span>:</p>
<p><span class="math display">
\hat\beta_1 = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ y_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
</span></p>
<ul>
<li>Where <span class="math inline">\widetilde{r_{1i}}</span> is the part of <span class="math inline">x_1</span> uncorrelated with <span class="math inline">x_2, \dots, x_k</span>.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.3 No Perfect Multicollinearity
</div>
</div>
<div class="callout-body-container callout-body">
<p>The existence of <span class="math inline">\hat\beta_1</span> is guaranteed by MLR.3 <span class="math inline">\sum\widetilde{r_{1i}}^2 ≠ 0</span>, since we cannot divide by 0.</p>
</div>
</div>
<p>Now, let us plug in <span class="math inline">y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i</span> into our regression anatomy formula:</p>
<p><span class="math display">
\begin{split}
\hat\beta_1 &amp; = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ y_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; = \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ (\beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i)}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
\end{split}
</span></p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-43-contents" aria-controls="callout-43" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summations
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-43" class="callout-43-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><strong>Property 1:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n \widetilde{r_{1i}} = 0
</span></p>
<ul>
<li>This is because <span class="math inline">\widetilde{r_{1i}}</span> is a residual term of a OLS regression of outcome <span class="math inline">x_1</span> and explanatory variables <span class="math inline">x_2, \dots, x_k</span>, and we know OLS residuals sum to 0.</li>
</ul>
<p><strong>Property 2:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n \widetilde{r_{1i}} x_{ji} = 0, \text{ given } j=2, \dots, k
</span></p>
<ul>
<li>Because for OLS, <span class="math inline">\sum x_i \hat u_i = 0</span>, and we know <span class="math inline">\widetilde{r_{1i}}</span> is the residual <span class="math inline">\hat u_i</span> in a regression with explanatory variables <span class="math inline">x_2, \dots, x_k</span> and outcome variable <span class="math inline">x_1</span>.</li>
</ul>
<p><strong>Property 3:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n \widetilde{r_{1i}} x_{1i} = \sum\limits_{i=1}^n \widetilde{r_{1i}}(\hat x_{1i} + \sum\limits_{i=1}^n \widetilde{r_{1i}}) = \sum\limits_{i=1}^n \widetilde{r_{1i}}^2
</span></p>
<ul>
<li>Because we have the regression fitted values <span class="math inline">\hat x_{1i} = \hat \gamma_0 + \hat\gamma_1 x_2 + \dots + \hat\gamma_{k-1} x_{k} + \widetilde{r_{1i}}</span> from regression anatomy.</li>
<li>And we know with regression, actual values are the predicted plus residual: <span class="math inline">y_i = \hat y_i + \hat u_i</span>. Thus, <span class="math inline">x_i = \hat x_i + \widetilde{r_{1i}}</span>.</li>
</ul>
</div>
</div>
</div>
<p>Now, focusing on the numerator, and using the summation properties above, let us simplify:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n \widetilde{r_{1i}} \ (\beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + u_i) \\
&amp; = \sum\limits_{i=1}^n (\widetilde{r_{1i}}\beta_0 + \widetilde{r_{1i}}\beta_1x_{1i} + \dots + \widetilde{r_{1i}}\beta_k x_{ki} + \widetilde{r_{1i}} u_i) \\
&amp; = \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_0 + \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_1 x_{1i} + \dots + \sum\limits_{i=1}^n \widetilde{r_{1i}}\beta_k x_{ki} + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
&amp; = \beta_0 \sum\limits_{i=1}^n \widetilde{r_{1i}} + \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}} x_{1i} + \dots + \beta_k \sum\limits_{i=1}^n \widetilde{r_{1i}} x_{ki} + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
&amp; = \beta_0 (0) + \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}}^2 + \beta_2 (0) + \dots + \beta_k (0) + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i \\
&amp; = \beta_1 \sum\limits_{i=1}^n \widetilde{r_{1i}}^2 + \sum\limits_{i=1}^n \widetilde{r_{1i}} u_i
\end{split}
</span></p>
<p>Now, putting the numerator back in, we can simplify:</p>
<p><span class="math display">
\begin{split}
\hat\beta_1 &amp; = \frac{\beta_1 \sum_{i=1}^n \widetilde{r_{1i}}^2 + \sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; \\
&amp; = \frac{\beta_1 \sum_{i=1}^n \widetilde{r_{1i}}^2}{\sum_{i=1}^n \widetilde{r_{1i}}^2} + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; \\
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
</span></p>
<p><br></p>
<p>Now, we want to find <span class="math inline">E(\hat\beta_1)</span>. Note that the second part of the equation is a function of <span class="math inline">u_i</span>, of which itself is a function of all explanatory variables <span class="math inline">x_{1i}, \dots, x_{ki}</span>. Thus, we know:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1 | x_{1i}, \dots x_{ki}) &amp; = \beta_1 + E\left( \frac{\sum_{i=1}^n \widetilde{r_{1i}} u_i}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \biggr|x_{1i},\dots, x_{ki} \right) \\
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ E(u_i|x_{1i}, \dots,x_{ki})}{\sum_{i=1}^n \widetilde{r_{1i}}^2}
\end{split}
</span></p>
<p>But what is <span class="math inline">E(u_i|x_{1i}, \dots , x_{ki})</span>? We can use two Gauss-Markov conditions to evaluate this.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.4 Zero Conditional Mean
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Zero-Conditional Mean assumption says <span class="math inline">E(u|x_1, \dots, x_k) = 0</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.2 Random Sampling
</div>
</div>
<div class="callout-body-container callout-body">
<p>Random sampling, combined with Zero-Conditional Mean, allows us to say:</p>
<p><span class="math display">
E(u|x_1, \dots, x_k)=E(u_1|x_{1i}, \dots, x_{ki}) = 0
</span></p>
</div>
</div>
<p>Thus, plugging that in to our formula, we get:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1 | x_{1i}, \dots x_{ki})
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ E(u_i|x_{1i}, \dots,x_{ki})}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; = \beta_1 + \frac{\sum_{i=1}^n \widetilde{r_{1i}} \ (0)}{\sum_{i=1}^n \widetilde{r_{1i}}^2} \\
&amp; = \beta_1 + 0 \\
&amp; = \beta_1
\end{split}
</span></p>
<p>Now, just like in simple linear regression, we use the <strong>law of iterated expectations</strong> to conclude this proof:</p>
<p><span class="math display">
\begin{split}
E(\hat\beta_1) &amp; = E[E(\hat\beta_1|x_{1i}, \dots, x_{ki})] \\
&amp; = E(\beta_1) \\
&amp; = \beta_1
\end{split}
</span></p>
<p>Thus, OLS is unbiased under these 4 conditions</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="assumptions-for-consistency" class="level2">
<h2 class="anchored" data-anchor-id="assumptions-for-consistency">Assumptions for Consistency</h2>
<p>Asymptotic consistency of an estimator is when we increase sample size <span class="math inline">n \rightarrow ∞</span>, the estimator should converge around the true population value.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-47-contents" aria-controls="callout-47" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notes on Asymptotic Consistency
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-47" class="callout-47-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An estimator is consistent, if as we increase sample size towards infinity, the estimate will become more and more concentrated around the true population value <span class="math inline">\theta</span>.</p>
<p>Or in other words, as sample size increases indefinitely, we will get closer and closer to the true population value <span class="math inline">\theta</span>, until at infinite sample size, all our estimates will be exactly <span class="math inline">\theta</span>.</p>
<p>Mathematically:</p>
<p><span class="math display">
Pr(|\hat\theta_n - \theta|&gt; \epsilon) \rightarrow 0, \text { as } n \rightarrow ∞
</span></p>
<ul>
<li>Or in other words, the proabability that the distance between an estimate <span class="math inline">\hat\theta_n</span> and the true population value <span class="math inline">\theta</span> will be higher than a small close-to-zero value <span class="math inline">\epsilon</span> will be 0, since our estimates <span class="math inline">\hat\theta_n</span> will converge at the <span class="math inline">\theta</span>.</li>
</ul>
<p><br></p>
<p>An estimator can be both biased, but consistent.</p>
<ul>
<li>i.e.&nbsp;in smaller sample sizes, the estimator might not be on average correct, but over a large enough sample size, it will become “unbiased”.</li>
</ul>
</div>
</div>
</div>
<p>The assumptions for the asymptotic consistency of the OLS estimate of <span class="math inline">\widehat{\beta_1}</span> are very similar to the assumptions for unbiasedness, but with assumption MLR.4 weakened to MLR.4’:</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-48-contents" aria-controls="callout-48" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.1 Linearity in Parameters
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-48" class="callout-48-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A model must be linear in parameters for unbiasedness. This means that the parameters of the model <span class="math inline">\beta_0, \dots, \beta_k</span> must not be multiplied/divided together.</p>
<p>Note: this does not mean the actual regression line must be linear - only the parameters/coefficients must not be multiplied.</p>
<p>For example, the following model is still linear in parameters:</p>
<p><span class="math display">
y = \beta_0 + \beta_1x^2
</span></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-49-contents" aria-controls="callout-49" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.2 Random Sampling
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-49" class="callout-49-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This assumption says that all observations in our sample are randomly sampled from the same population.</p>
<p>The error term <span class="math inline">u</span> in our regression model is some random variable (with its own probability distribution), that can be defined by its expectation <span class="math inline">E(u)</span>.</p>
<ul>
<li>If we randomly select one observation <span class="math inline">i</span> from the data, each observation <span class="math inline">i</span> has an equal chance of being selected.</li>
<li>The error term for that observation, <span class="math inline">u_i</span>, should also have the same expectation as the random variable <span class="math inline">u</span>, since each observation <span class="math inline">i</span> within <span class="math inline">u</span> has the same chance of being selected.</li>
</ul>
<p>Thus, random sampling allows us to say <span class="math inline">E(u) = E(u_i)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-50-contents" aria-controls="callout-50" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.3 No Perfect Multicollinearity/Variance in <span class="math inline">x</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-50" class="callout-50-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>No perfect multicollinearity (MLR.3) means that no two explanatory variables are perfectly correlated together. This also means that in the regression anatomy formula, <span class="math inline">\sum \widetilde{r_{ji}}^2 ≠ 0</span>.</p>
<p>If we have a simple linear regression, we instead must make sure that there is indeed variance in the explanatory variable <span class="math inline">x</span> (SLR.3). Or in other words: <span class="math inline">Var(x) ≠ 0</span>.</p>
<p><strong>These two assumptions are necessary to even calculate</strong> <span class="math inline">\widehat{\beta_0}</span>, since the formulas require dividing by these values, so if they equal 0, we cannot calculate OLS.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-51-contents" aria-controls="callout-51" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.4’ Zero-Mean and Exogeneity
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-51" class="callout-51-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We can weaken <em>MLR.4 Zero-Conditional Mean</em> to the <em>Zero-Mean and Exogeneity assumption</em>:</p>
<p><span class="math display">
\begin{split}
&amp; E(u) = 0 \\
&amp; Cov(x_j, u) = 0, \quad \text{for } j = 1,\dots k
\end{split}
</span></p>
<p>The second assumption can also be rewritten as:</p>
<p><span class="math display">
E(x_j u) = 0
</span></p>
<ul>
<li>The proof of this is almost identical to the proof of <span class="math inline">\sum x \hat u = 0</span> implying <span class="math inline">Cov(x, \hat u) = 0</span> we did in section 2.3.1.</li>
</ul>
<p>Note: if we meet the full MLR.4 Zero-Conditional Mean assumption, we will automatically meet this weakened assumption. However, the reverse is not true.</p>
<p><u>Under this new assumption (and the other 3 Gauss-Markov assumptions), OLS is biased in small sample sizes, but asymptotically consistent.</u></p>
</div>
</div>
</div>
<p><br></p>
<p>If these 4 conditions are met, OLS is Asymptotically consistent.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-55-contents" aria-controls="callout-55" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of OLS Asymptotic Consistency
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-55" class="callout-55-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-52-contents" aria-controls="callout-52" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-52" class="callout-52-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><strong>Property 1:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n (x_i - \bar{x}) = 0
</span></p>
<ul>
<li>This is because we can expand the left to <span class="math inline">\sum x_i - \sum \bar x</span>.</li>
<li>Then, we know <span class="math inline">\sum x_i = \sum \bar x</span> (by the formula for mean), so <span class="math inline">\sum x_i - \sum \bar x = 0</span>.</li>
</ul>
<p><strong>Property 2:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y})
</span></p>
<ul>
<li>This is because on the right side can expand to <span class="math inline">\sum [x_i(y_i - \bar y) - \bar x (y_i - \bar y)]</span>.</li>
<li>Then, split into <span class="math inline">\sum x_i (y_i - \bar y) - \bar x \sum (y_i - \bar y)</span>.</li>
<li>We know that by property 1 (which applies to any variable), <span class="math inline">\sum (y_i - \bar y) = 0</span>. Thus, the right side disappears, and we are left with <span class="math inline">\sum x_i (y_i - \bar y)</span>.</li>
</ul>
<p><strong>Property 3:</strong></p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2
</span></p>
<ul>
<li>Start by expanding right side to <span class="math inline">\sum [ x_i ( x_i - \bar x) - \bar x (x_i - \bar x)]</span></li>
<li>Which splits into <span class="math inline">\sum x_i (x_i - \bar x) - \bar x \sum (x_i - \bar x)</span></li>
<li>By the first property, we know <span class="math inline">\sum x_i - \bar x = 0</span>, so we are only left with <span class="math inline">\sum x_i (x_i - \bar x)</span></li>
</ul>
</div>
</div>
</div>
<p>Let us look at the numerator. Let us expand the numerator:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
= &amp; \sum\limits_{i=1}^n [(x_i - \bar x)y_i - (x_i - \bar x) \bar y] \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \sum\limits_{i=1}^n (x_i - \bar x) \bar y \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
\end{split}
</span></p>
<p>We know that <span class="math inline">\sum (x_i - \bar x) = 0</span> (from the properties above). Thus, we can further simplify to:</p>
<p><span class="math display">
\begin{split}
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y\sum\limits_{i=1}^n (x_i - \bar x) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i - \bar y(0) \\
= &amp; \sum\limits_{i=1}^n(x_i - \bar x)y_i
\end{split}
</span></p>
<p>Now, let us play with the numerator more (note the properties of summation introduced earlier):</p>
<p><span class="math display">
\begin{split}
&amp; = \sum\limits_{i=1}^n (x_i - \bar{x})y_i \\
&amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(\beta_0 + \beta_1 x_i + u_i) \\
&amp; = \sum\limits_{i=1}^n(x_i - \bar{x}) \beta_0 + \sum\limits_{i=1}^n(x_i - \bar{x}) \beta_1  x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\
&amp; = \beta_0 \sum\limits_{i=1}^n(x_i - \bar{x}) + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x}) x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\
&amp; = 0 + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i\end{split}
</span></p>
<p>Now, putting the numerator back into the equation, we simplify:</p>
<p><span class="math display">
\begin{split}
\hat{\beta}_1 &amp; = \frac{\beta_1 \sum_{i=1}^n(x_i - \bar{x})^2 + \sum_{i=1}^n(x_i - \bar{x})u_i}{\sum_{i=1}^n(x_i - \bar{x})^2} \\
&amp;  =  \beta_1 + \frac{\sum_{i=1}^n(x_i - \bar{x}) u_i}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{split}
</span></p>
<p>We can add <span class="math inline">\frac{1}{n}</span> (or <span class="math inline">n^{-1}</span>) to the top and bottom of the fraction (which cancel each other out, keeping the equation equivalent):</p>
<p><span class="math display">
\hat\beta_1=  \beta_1 + \frac{n^{-1} \sum_{i=1}^n(x_i - \bar{x}) u_i}{n^{-1} \sum_{i=1}^n (x_i - \bar{x})^2}
</span></p>
<p>Now, let us expand the numerator, and simplify, and we get:</p>
<p><span class="math display">
\begin{split}
\hat\beta_1 &amp; =  \beta_1 + \frac{n^{-1} \sum_{i=1}^n x_i u_i - n^{-1} \sum_{i=1}^n \bar x u_i}{n^{-1} \sum_{i=1}^n (x_i - \bar{x})^2} \\
&amp; =  \beta_1 + \frac{n^{-1} \sum_{i=1}^n x_i u_i - \bar x \ n^{-1} \sum_{i=1}^n u_i}{n^{-1} \sum_{i=1}^n (x_i - \bar{x})^2} \\
&amp; =  \beta_1 + \frac{ \overline{xu} - \bar x \bar u}{S.Var(x)}
\end{split}
</span></p>
<ul>
<li>Where <span class="math inline">S.Var(x)</span> is the sample variance of <span class="math inline">x</span>.</li>
</ul>
<p>We want to find <span class="math inline">\text{plim}(\hat\beta_1)</span>. We will need a few properties for this:</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-53-contents" aria-controls="callout-53" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of Probability Limits
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-53" class="callout-53-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We know these general rules about probability limits:</p>
<p><span class="math display">
\begin{split}
&amp; \text{plim}(\bar x_n) = \mu _x \\
&amp; \text{plim}(S.Var(x_i)) = Var(x_i) \\
&amp; \text{plim}(S.Cov(x_i, y_i)) = Cov (x_i, y_i)
\end{split}
</span></p>
<p>The other properties are about algebra with probability limits. Assume <span class="math inline">\text{plim} (u_n) = a</span>, and <span class="math inline">\text{plim}(v_n) = b</span>. Then, the following are true:</p>
<p><span class="math display">
\begin{split}
&amp; \text{plim} (u_n + v_n) = a + b \\
&amp; \text{plim} (u_n v_n) = ab \\
&amp; \text{plim} (u_n v_n) = a/b
\end{split}
</span></p>
</div>
</div>
</div>
<p>Knowing this, we then know that:</p>
<p><span class="math display">
\begin{split}
\text{plim}(\hat\beta_1) &amp; = \beta_1 + \frac{ \text{plim}(\overline{xu}) - \text{plim}( \bar x \bar u) }{ \text{plim}(S.Var(x))} \\
&amp; = \beta_1 + \frac{ \text{plim}(\overline{xu}) - \text{plim}(\bar x) \text{plim}(\bar u) }{ \text{plim}(S.Var(x))} \\
&amp;  = \beta_1 + \frac{ E(xu) - E(x) E(u) }{Var(x)} \\
\end{split}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLR.4’ Zero-Mean and Exogeneity Assumption
</div>
</div>
<div class="callout-body-container callout-body">
<p>The weakened Zero-Mean and Exogeneity assumption states:</p>
<p><span class="math display">
\begin{split}
&amp; E(u) = 0 \\
&amp; E(xu) = 0
\end{split}
</span></p>
</div>
</div>
<p>Using this assumption, we can conclude the proof:</p>
<p><span class="math display">
\begin{split}
\text{plim}(\hat\beta_1) &amp; = \beta_1 + \frac{ E(xu) - E(x) E(u) }{Var(x)} \\
&amp; = \beta_1 + \frac{ 0 - E(x)0}{Var(x)} \\
&amp; = \beta_1 + \frac{ 0 }{Var(x)} \\
&amp; = \beta_1+0 \\
\text{plim}(\hat\beta_1) &amp; = \beta_1
\end{split}
</span></p>
<p>Thus, OLS is asymptotically consistent under a weakened version of MLR.4 Zero-Conditional Mean - called the Zero-Mean and Exogeneity Assumption.</p>
<ul>
<li>Under this weakened assumption (without meeting the full MLR.4 assumption), OLS is <u>biased but consistent</u>.</li>
</ul>
</div>
</div>
</div>
<p>The key assumption is <strong>Exogeneity</strong> - that no explanatory variable is correlated with the error term. If exogeneity is violated, this is often caused by omitted variable bias.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-56-contents" aria-controls="callout-56" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Omitted Variable Bias
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-56" class="callout-56-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consider two regressions. The first regression, the “short” regression, is a simple linear regression with only explanatory variable <span class="math inline">x</span>. The second regression, the “long” regression, contains an extra variable <span class="math inline">z</span> that is omitted from the first regression:</p>
<p><span class="math display">
\begin{split}y_i &amp; = \beta_0^S + \beta_1^Sx_i + u_i^S \quad \text{short} \\y_i &amp; = \beta_0 + \beta_1x_i + \beta_2z_i + u_i \quad \text{long}\end{split}
</span></p>
<ul>
<li>Note: the <span class="math inline">S</span> in <span class="math inline">\beta_0^S</span> is a subscript representing short. It is not an exponent.</li>
</ul>
<p>Now consider an auxiliary regression, where the omitted variable <span class="math inline">z</span> is the outcome variable, and <span class="math inline">x</span> is the explanatory variable:</p>
<p><span class="math display">
z_i = \delta_0 + \delta_1 x_i + v_i
</span></p>
<ul>
<li>where <span class="math inline">\delta_0, \delta_1</span> are coefficients and <span class="math inline">v_i</span> is the error term</li>
</ul>
<p><br></p>
<p>Now we have <span class="math inline">z</span> in terms of <span class="math inline">x</span>, let us plug <span class="math inline">z</span> into our long regression to “recreate” the short regression:</p>
<p><span class="math display">
\begin{split}y_i &amp; = \beta_0 + \beta_1x_i + \beta_2z_i + u_i \\y_i &amp; = \beta_0 + \beta_1 x_i + \beta_2(\delta_0 + \delta_1x_i + v_i) + u_i \\y_i &amp; = \beta_0 + \beta_1 x_i + \beta_2 \delta_0 + \beta_2 \delta_1 x_i + \beta_2v_i + u_i \\y_i &amp; = \beta_0 + \beta_2 \delta_0 + (\beta_1 + \beta_2 \delta_1)x_i + \beta_2v_i + u_i\end{split}
</span></p>
<p>We have “recreated” the short regression with one variable <span class="math inline">x</span>. Let us see our recreation next to the original short regression:</p>
<p><span class="math display">
\begin{split}
y_i &amp; = \beta_0^S + \beta_1^Sx_i + u_i^S \\
y_i &amp; = \beta_0 + \beta_2 \delta_0 + (\beta_1 + \beta_2 \delta_1)x_i + \beta_2v_i + u_i
\end{split}
</span></p>
<ul>
<li>The short regression coefficient <span class="math inline">\beta_0^S</span> is analogous to the <span class="math inline">\beta_0 + \beta_2 \delta_0</span> in the recreation (both are the intercepts)</li>
<li>The short regression coefficient <span class="math inline">\beta_1^S x_i</span> is analogous to the <span class="math inline">(\beta_1 + \beta_2 \delta_1)x_i</span> in the recreation (both are the slope and variable of interest)</li>
<li>The short regression <span class="math inline">u_i^S</span> is analogous to the <span class="math inline">\beta_2 v_i + u_i</span> in the recreation (both are the error terms).</li>
</ul>
<p>Since the short regression <span class="math inline">\beta_1^S x_i</span> is analogous to the <span class="math inline">(\beta_1 + \beta_2 \delta_1)x_i</span> in the recreation, that means coefficient <span class="math inline">\beta_1^S = \beta_1 + \beta_2 \delta_1</span>.</p>
<p>Thus, the difference between the short regression (simple linear regression) coefficient <span class="math inline">\beta_1^S</span>, and the original long regression coefficient <span class="math inline">\beta_1</span>, is <span class="math inline">\beta_2 \delta_1</span>.</p>
<ul>
<li>If <span class="math inline">\beta_2 = 0</span> (meaning no relationship between omitted <span class="math inline">x</span> and <span class="math inline">y</span>), or <span class="math inline">\delta_1 = 0</span> (meaning no relationship between omitted <span class="math inline">x</span> and <span class="math inline">x</span>), then difference <span class="math inline">\beta_2 \delta_1 = 0</span>, thus there is no difference.</li>
<li>But if either of those facts are not true, then <span class="math inline">\beta_2 \delta_1 ≠ 0</span>, and there is a difference between <span class="math inline">\beta_1^S</span> and <span class="math inline">\beta_1</span>.</li>
</ul>
<p><br></p>
<p>More intuitively, if the omitted variable <span class="math inline">z</span> is both correlated with <span class="math inline">x</span> and <span class="math inline">y</span>, then the two coefficients are different by <span class="math inline">\beta_2 \delta_1</span>.</p>
<ul>
<li>Any variable <span class="math inline">z</span> correlated both with <span class="math inline">x</span> and <span class="math inline">y</span> is called a <strong>confounding variable</strong>.</li>
<li>This <span class="math inline">\beta_2 \delta_1</span> amount is called the <strong>omitted variable bias</strong>.</li>
</ul>
</div>
</div>
</div>
<p><br></p>
<hr>
</section>
</section>
<section id="statistical-inference" class="level1">
<h1>Statistical Inference</h1>
<p>The standard errors of OLS estimates quantify how precise our estimates are:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuition of Standard Errors and Uncertainty
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Homoscedasticity vs.&nbsp;Heteroscedasticity and Standard Errors
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p><br></p>
<p>We can mathematically derive standard errors as shown below:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deriving OLS Standard Errors
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deriving Robust Standard Errors
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p><br></p>
<p>Using standard errors, we can conduct hypothesis testing:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
T-Tests for Coefficients
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Confidence Intervals for Coefficients
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
F-Tests for Multiple Coefficients
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Confidence Intervals for Prediction
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p><br></p>
<p><br></p>
<hr>
</section>
<section id="extension-explanatory-variables" class="level1">
<h1>Extension: Explanatory Variables</h1>
<section id="categorical-explanatory-variables" class="level2">
<h2 class="anchored" data-anchor-id="categorical-explanatory-variables">Categorical Explanatory Variables</h2>
<p><br></p>
</section>
<section id="fixed-effects" class="level2">
<h2 class="anchored" data-anchor-id="fixed-effects">Fixed Effects</h2>
<p><br></p>
</section>
<section id="polynomial-transformations" class="level2">
<h2 class="anchored" data-anchor-id="polynomial-transformations">Polynomial Transformations</h2>
<p><br></p>
</section>
<section id="logarithmic-transformations" class="level2">
<h2 class="anchored" data-anchor-id="logarithmic-transformations">Logarithmic Transformations</h2>
<p><br></p>
</section>
<section id="interaction-effects" class="level2">
<h2 class="anchored" data-anchor-id="interaction-effects">Interaction Effects</h2>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="implementation-in-r" class="level1">
<h1><strong>Implementation in R</strong></h1>
<p><br></p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>